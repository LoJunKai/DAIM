{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Jun_Kai_Roboflow_Tutorial_Eval.ipynb","provenance":[],"collapsed_sections":["FIqnjbWYsuQw","2FKFq8RXs6bs","MFyCeiBb9BbS"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lsT4-_Eq45Ww","colab_type":"text"},"source":["NOTE: to obtain the most recent version of this notebook, please copy from \n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1U3fkRu6-hwjk7wWIpg-iylL2u5T9t7rr#scrollTo=lsT4-_Eq45Ww)\n"]},{"cell_type":"markdown","metadata":{"id":"uQCnYPVDrsgx","colab_type":"text"},"source":["## **Training Faster R-CNN Object Detection on a Custom Dataset**\n","\n","### **Overview**\n","\n","This notebook walks through how to train a Faster R-CNN object detection model using the TensorFlow Object Detection API.\n","\n","In this specific example, we'll training an object detection model to recognize cells types: white blood cells, red blood cells and platelets. **To adapt this example to train on your own dataset, you only need to change two lines of code in this notebook.**\n","\n","Everything in this notebook is also hosted on this [GitHub repo](https://github.com/roboflow-ai/tensorflow-object-detection-faster-rcnn).\n","\n","![Blood Cell Example](https://i.imgur.com/QwyX2aD.png)\n","\n","**Credit to [DLology](https://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/) and [Tony607](https://github.com/Tony607)**, whom wrote the first notebook on which much of this is example is based. \n","\n","### **Our Data**\n","\n","We'll be using an open source cell dataset called BCCD (Blood Cell Count and Detection). Our dataset contains 364 images (and 4888 annotations!) is hosted publicly on Roboflow [here](https://public.roboflow.ai/object-detection/bccd).\n","\n","When adapting this example to your own data, create two datasets in Roboflow: `train` and `test`. Use Roboflow to generate TFRecords for each, replace their URLs in this notebook, and you're able to train on your own custom dataset.\n","\n","### **Our Model**\n","\n","We'll be training a Faster R-CNN neural network. Faster R-CNN is a two-stage detector: first it identifies regions of interest, and then passes these regions to a convolutional neural network. The outputted features maps are passed to a support vector machine (SVM) for classification. Regression between predicted bounding boxes and ground truth bounding boxes are computed. (Consider [this](https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a) deep dive for more!)\n","\n","The model arechitecture is one of many available via TensorFlow's [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models).\n","\n","### **Training**\n","\n","Google Colab provides free GPU resources. Click \"Runtime\" → \"Change runtime type\" → Hardware Accelerator dropdown to \"GPU.\"\n","\n","Colab does have memory limitations, and notebooks must be open in your browser to run. Sessions automatically clear themselves after 12 hours.\n","\n","### **Inference**\n","\n","We'll run inference directly in this notebook, and on three test images contained in the \"test\" folder from our GitHub repo. \n","\n","When adapting to your own dataset, you'll need to add test images to the `test` folder located at `tensorflow-object-detection/test`.\n","\n","### **About**\n","\n","[Roboflow](https://roboflow.ai) makes managing, preprocessing, augmenting, and versioning datasets for computer vision seamless.\n","\n","Developers reduce 50% of their boilerplate code when using Roboflow's workflow, automate labelling quality assurance, save training time, and increase model reproducibility.\n","\n","#### ![Roboflow Workmark](https://i.imgur.com/WHFqYSJ.png)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"dW2W17o-p7SX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":962},"executionInfo":{"status":"ok","timestamp":1592144589985,"user_tz":-480,"elapsed":80763,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"cc0e4b62-9b9b-4025-d0c4-a65a7f782164"},"source":["!pip install tensorflow_gpu==1.15"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow_gpu==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n","\u001b[K     |████████████████████████████████| 411.5MB 41kB/s \n","\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 31.6MB/s \n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (3.2.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.1.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.12.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.18.5)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.0.8)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.12.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.34.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.2.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.9.0)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 34.5MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.29.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (47.1.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.2.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (1.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow_gpu==1.15) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (1.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.1.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=7ee349ed27f464a4dc0a56f750afea4b975c79c3afeae052c44656c95f1ba6fa\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorflow-estimator<2.3.0,>=2.2.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow-gpu\n","  Found existing installation: tensorboard 2.2.2\n","    Uninstalling tensorboard-2.2.2:\n","      Successfully uninstalled tensorboard-2.2.2\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow-estimator 2.2.0\n","    Uninstalling tensorflow-estimator-2.2.0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0\n","Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorboard","tensorflow"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"yhzxsJb3dpWq","colab_type":"text"},"source":["## Configs and Hyperparameters\n","\n","Support a variety of models, you can find more pretrained model from [Tensorflow detection model zoo: COCO-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models), as well as their pipline config files in [object_detection/samples/configs/](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)."]},{"cell_type":"code","metadata":{"id":"gnNXNQCjdniL","colab_type":"code","colab":{}},"source":["# If you forked the repo, you can replace the link.\n","repo_url = 'https://github.com/roboflow-ai/tensorflow-object-detection-faster-rcnn'\n","\n","# Number of training steps - 1000 will train very quickly, but more steps will increase accuracy.\n","num_steps = 200000  # 200000 to improve\n","\n","# Number of evaluation steps.\n","num_eval_steps = 50\n","\n","MODELS_CONFIG = {\n","    'ssd_mobilenet_v2': {\n","        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n","        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n","        'batch_size': 12\n","    },\n","    'faster_rcnn_inception_v2': {\n","        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n","        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n","        'batch_size': 12\n","    },\n","    'rfcn_resnet101': {\n","        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n","        'pipeline_file': 'rfcn_resnet101_pets.config',\n","        'batch_size': 8\n","    }\n","}\n","\n","# Pick the model you want to use\n","# Select a model in `MODELS_CONFIG`.\n","selected_model = 'faster_rcnn_inception_v2'\n","\n","# Name of the object detection model to use.\n","MODEL = MODELS_CONFIG[selected_model]['model_name']\n","\n","# Name of the pipline file in tensorflow object detection API.\n","pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n","\n","# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n","batch_size = MODELS_CONFIG[selected_model]['batch_size']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w4V-XE6kbkc1","colab_type":"text"},"source":["## Clone the `tensorflow-object-detection` repository or your fork."]},{"cell_type":"code","metadata":{"id":"dxc3DmvLQF3z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1592144643338,"user_tz":-480,"elapsed":8844,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"3d5a466c-ad2c-430e-e00f-3148e9c10498"},"source":["import os\n","\n","%cd /content\n","\n","repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n","\n","!git clone {repo_url}\n","%cd {repo_dir_path}\n","!git pull"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","fatal: destination path 'tensorflow-object-detection-faster-rcnn' already exists and is not an empty directory.\n","/content/tensorflow-object-detection-faster-rcnn\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bI8__uNS8-ns","colab_type":"text"},"source":["## Install required packages"]},{"cell_type":"code","metadata":{"id":"ecpHEnka8Kix","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592221182610,"user_tz":-480,"elapsed":59181,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"74c44418-b64b-4f63-e653-920cedb37a32"},"source":["%cd /content\n","!git clone --quiet https://github.com/tensorflow/models.git\n","\n","!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n","\n","!pip install -q Cython contextlib2 pillow lxml matplotlib\n","\n","!pip install -q pycocotools\n","\n","%cd /content/models/research\n","!protoc object_detection/protos/*.proto --python_out=.\n","\n","import os\n","os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n","\n","!pip install tf_slim\n","\n","!python object_detection/builders/model_builder_test.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","Selecting previously unselected package python-bs4.\n","(Reading database ... 144328 files and directories currently installed.)\n","Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n","Unpacking python-bs4 (4.6.0-1) ...\n","Selecting previously unselected package python-pkg-resources.\n","Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n","Unpacking python-pkg-resources (39.0.1-2) ...\n","Selecting previously unselected package python-chardet.\n","Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n","Unpacking python-chardet (3.0.4-1) ...\n","Selecting previously unselected package python-six.\n","Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n","Unpacking python-six (1.11.0-2) ...\n","Selecting previously unselected package python-webencodings.\n","Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n","Unpacking python-webencodings (0.5-2) ...\n","Selecting previously unselected package python-html5lib.\n","Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n","Unpacking python-html5lib (0.999999999-1) ...\n","Selecting previously unselected package python-lxml:amd64.\n","Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n","Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n","Selecting previously unselected package python-olefile.\n","Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n","Unpacking python-olefile (0.45.1-1) ...\n","Selecting previously unselected package python-pil:amd64.\n","Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.2_amd64.deb ...\n","Unpacking python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n","Setting up python-pkg-resources (39.0.1-2) ...\n","Setting up python-six (1.11.0-2) ...\n","Setting up python-bs4 (4.6.0-1) ...\n","Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n","Setting up python-olefile (0.45.1-1) ...\n","Setting up python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n","Setting up python-webencodings (0.5-2) ...\n","Setting up python-chardet (3.0.4-1) ...\n","Setting up python-html5lib (0.999999999-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","/content/models/research\n","object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n","Collecting tf_slim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n","\u001b[K     |████████████████████████████████| 358kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.9.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.12.0)\n","Installing collected packages: tf-slim\n","Successfully installed tf-slim-1.1.0\n","2020-06-15 11:39:40.127808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","Traceback (most recent call last):\n","  File \"object_detection/builders/model_builder_test.py\", line 21, in <module>\n","    from object_detection.builders import model_builder\n","  File \"/content/models/research/object_detection/builders/model_builder.py\", line 25, in <module>\n","    from object_detection.builders import matcher_builder\n","  File \"/content/models/research/object_detection/builders/matcher_builder.py\", line 19, in <module>\n","    from object_detection.matchers import bipartite_matcher\n","  File \"/content/models/research/object_detection/matchers/bipartite_matcher.py\", line 20, in <module>\n","    from tensorflow.contrib.image.python.ops import image_ops\n","ModuleNotFoundError: No module named 'tensorflow.contrib'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u-k7uGThXlny","colab_type":"text"},"source":["## Prepare `tfrecord` files\n","\n","Roboflow automatically creates our TFRecord and label_map files that we need!\n","\n","**Generating your own TFRecords the only step you need to change for your own custom dataset.**\n","\n","Because we need one TFRecord file for our training data, and one TFRecord file for our test data, we'll create two separate datasets in Roboflow and generate one set of TFRecords for each.\n","\n","To create a dataset in Roboflow and generate TFRecords, follow [this step-by-step guide](https://blog.roboflow.ai/getting-started-with-roboflow/)."]},{"cell_type":"code","metadata":{"id":"GNfIPc5yxDOv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1591971495328,"user_tz":-480,"elapsed":1018,"user":{"displayName":"SGPCLD","photoUrl":"","userId":"02007731753862560953"}},"outputId":"569953d4-41fa-41c3-b34f-e0f59974df17"},"source":["%cd /content/tensorflow-object-detection-faster-rcnn/data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/tensorflow-object-detection-faster-rcnn/data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yb_FMcfnSbRZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1591971759182,"user_tz":-480,"elapsed":3468,"user":{"displayName":"SGPCLD","photoUrl":"","userId":"02007731753862560953"}},"outputId":"bfc248f3-6510-46ad-dc34-0198e31662f3"},"source":["# UPDATE THIS LINK - get our data from Roboflow\n","!curl -L https://public.roboflow.ai/ds/E301mdvkk3?key=k4PF0rxMgl > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100   891  100   891    0     0   1280      0 --:--:-- --:--:-- --:--:--  1280\n","100 3731k  100 3731k    0     0  2604k      0  0:00:01  0:00:01 --:--:-- 7027k\n","Archive:  roboflow.zip\n"," extracting: test/People.tfrecord    \n"," extracting: train/People.tfrecord   \n"," extracting: valid/People.tfrecord   \n"," extracting: test/People_label_map.pbtxt  \n"," extracting: train/People_label_map.pbtxt  \n"," extracting: valid/People_label_map.pbtxt  \n"," extracting: README.roboflow.txt     \n"," extracting: README.dataset.txt      \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7T58u1YP9sUW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1591971831128,"user_tz":-480,"elapsed":4180,"user":{"displayName":"SGPCLD","photoUrl":"","userId":"02007731753862560953"}},"outputId":"d8893869-d819-4b33-97c8-3abaef6ba54f"},"source":["%ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FYI.txt  README.dataset.txt  README.roboflow.txt  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalid\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H5qhOGaTTFsq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1591971835712,"user_tz":-480,"elapsed":2821,"user":{"displayName":"SGPCLD","photoUrl":"","userId":"02007731753862560953"}},"outputId":"add7266e-7dca-4c84-b564-4e3e78cfd827"},"source":["# check out what we have in train\n","%ls train"],"execution_count":null,"outputs":[{"output_type":"stream","text":["People_label_map.pbtxt  People.tfrecord\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mKnnSSBu_XXF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1591971843536,"user_tz":-480,"elapsed":2830,"user":{"displayName":"SGPCLD","photoUrl":"","userId":"02007731753862560953"}},"outputId":"468c9a75-d215-438e-b032-ca768def1cf4"},"source":["# show what we have in test\n","%ls test"],"execution_count":null,"outputs":[{"output_type":"stream","text":["People_label_map.pbtxt  People.tfrecord\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ry_SlacON8Ke","colab_type":"text"},"source":["##Prepare tfrecord files"]},{"cell_type":"code","metadata":{"id":"N1FgvecjJzpc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1593807589633,"user_tz":-480,"elapsed":21460,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"6aaf70e5-dc1d-44bb-856c-8c4452602a1d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tgd-fzAIkZlV","colab_type":"code","colab":{}},"source":["# NOTE: Update these TFRecord names from \"cells\" and \"cells_label_map\" to your files!\n","test_record_fname = '/content/drive/My Drive/Colab Notebooks/TIL/CV/input/tfrecord/coco_val.record-00000-of-00001'\n","train_record_fname = '/content/drive/My Drive/Colab Notebooks/TIL/CV/input/tfrecord/coco_train.record-00000-of-00001'\n","label_map_pbtxt_fname = '/content/drive/My Drive/Colab Notebooks/TIL/CV/input/tfrecord/tfrecord_label_map.pbtxt'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCNYAaC7w6N8","colab_type":"text"},"source":["## Download base model"]},{"cell_type":"code","metadata":{"id":"orDCj6ihgUMR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592144722773,"user_tz":-480,"elapsed":3867,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"1905a2e4-4a17-4d93-b63d-2931a2c8f731"},"source":["%cd /content/models/research\n","\n","import os\n","import shutil\n","import glob\n","import urllib.request\n","import tarfile\n","MODEL_FILE = MODEL + '.tar.gz'\n","DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n","DEST_DIR = '/content/models/research/pretrained_model'\n","\n","if not (os.path.exists(MODEL_FILE)):\n","    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n","\n","tar = tarfile.open(MODEL_FILE)\n","tar.extractall()\n","tar.close()\n","\n","os.remove(MODEL_FILE)\n","if (os.path.exists(DEST_DIR)):\n","    shutil.rmtree(DEST_DIR)\n","os.rename(MODEL, DEST_DIR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/models/research\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pGhvAObeiIix","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1592144728359,"user_tz":-480,"elapsed":8082,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"85b03774-c11f-4268-c27e-26c4dee3b5f8"},"source":["!echo {DEST_DIR}\n","!ls -alh {DEST_DIR}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/models/research/pretrained_model\n","total 111M\n","drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 .\n","drwxr-xr-x 64 root   root 4.0K Jun 14 14:25 ..\n","-rw-r--r--  1 345018 5000   77 Feb  1  2018 checkpoint\n","-rw-r--r--  1 345018 5000  55M Feb  1  2018 frozen_inference_graph.pb\n","-rw-r--r--  1 345018 5000  51M Feb  1  2018 model.ckpt.data-00000-of-00001\n","-rw-r--r--  1 345018 5000  16K Feb  1  2018 model.ckpt.index\n","-rw-r--r--  1 345018 5000 5.5M Feb  1  2018 model.ckpt.meta\n","-rw-r--r--  1 345018 5000 3.2K Feb  1  2018 pipeline.config\n","drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 saved_model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UHnxlfRznPP3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592144728360,"user_tz":-480,"elapsed":6518,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"960f0a71-66a0-49f8-fc16-aecb49808abd"},"source":["fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n","fine_tune_checkpoint"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/models/research/pretrained_model/model.ckpt'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"MvwtHlLOeRJD","colab_type":"text"},"source":["## Configuring a Training Pipeline"]},{"cell_type":"code","metadata":{"id":"dIhw7IdpLuiU","colab_type":"code","colab":{}},"source":["import os\n","pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n","\n","assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fG1nCNpUXcRU","colab_type":"code","colab":{}},"source":["def get_num_classes(pbtxt_fname):\n","    from object_detection.utils import label_map_util\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)\n","    categories = label_map_util.convert_label_map_to_categories(\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)\n","    return len(category_index.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjtCbLF2i0wI","colab_type":"code","colab":{}},"source":["import re\n","\n","num_classes = get_num_classes(label_map_pbtxt_fname)  # Can actually just sub with no. of classes\n","\n","# Re-writing the pipeline_fname file to know which directory contains what\n","with open(pipeline_fname) as f:\n","    s = f.read()\n","with open(pipeline_fname, 'w') as f:\n","\n","    # fixed_shape_resizer - changed as aspect_ratio_resizer causes image shape errors\n","    # Refer to: https://github.com/tensorflow/tensorflow/issues/34544\n","    s = re.sub('''keep_aspect_ratio_resizer {\n","        min_dimension: 600\n","        max_dimension: 1024\n","      }''', \n","      '''fixed_shape_resizer {\n","        height: 600\n","        width: 800\n","      }''', s)\n","\n","    # # keep_aspect_ratio_resizer\n","    # s = re.sub('max_dimension: \\d*',\n","    #            'max_dimension: {}'.format(1024), s)\n","    \n","    # fine_tune_checkpoint\n","    s = re.sub('fine_tune_checkpoint: \".*?\"',\n","               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n","    \n","    # tfrecord files train and test.\n","    s = re.sub(\n","        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n","    s = re.sub(\n","        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n","\n","    # label_map_path\n","    s = re.sub(\n","        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n","\n","    # Set training batch_size.\n","    s = re.sub('batch_size: [0-9]+',\n","               'batch_size: {}'.format(batch_size), s)\n","\n","    # Set training steps, num_steps\n","    s = re.sub('num_steps: [0-9]+',\n","               'num_steps: {}'.format(num_steps), s)\n","    \n","    # Set number of classes num_classes.\n","    s = re.sub('num_classes: [0-9]+',\n","               'num_classes: {}'.format(num_classes), s)\n","    f.write(s)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GH0MEEanocn6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592147331345,"user_tz":-480,"elapsed":3701,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"852336ea-7170-4123-9f18-b9f6a185e8fd"},"source":["!cat {pipeline_fname}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# Faster R-CNN with Inception v2, configured for Oxford-IIIT Pets Dataset.\n","# Users should configure the fine_tune_checkpoint field in the train config as\n","# well as the label_map_path and input_path fields in the train_input_reader and\n","# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n","# should be configured.\n","\n","model {\n","  faster_rcnn {\n","    num_classes: 5\n","    image_resizer {\n","      fixed_shape_resizer {\n","        height: 600\n","        width: 800\n","      }\n","    }\n","    feature_extractor {\n","      type: 'faster_rcnn_inception_v2'\n","      first_stage_features_stride: 16\n","    }\n","    first_stage_anchor_generator {\n","      grid_anchor_generator {\n","        scales: [0.25, 0.5, 1.0, 2.0]\n","        aspect_ratios: [0.5, 1.0, 2.0]\n","        height_stride: 16\n","        width_stride: 16\n","      }\n","    }\n","    first_stage_box_predictor_conv_hyperparams {\n","      op: CONV\n","      regularizer {\n","        l2_regularizer {\n","          weight: 0.0\n","        }\n","      }\n","      initializer {\n","        truncated_normal_initializer {\n","          stddev: 0.01\n","        }\n","      }\n","    }\n","    first_stage_nms_score_threshold: 0.0\n","    first_stage_nms_iou_threshold: 0.7\n","    first_stage_max_proposals: 300\n","    first_stage_localization_loss_weight: 2.0\n","    first_stage_objectness_loss_weight: 1.0\n","    initial_crop_size: 14\n","    maxpool_kernel_size: 2\n","    maxpool_stride: 2\n","    second_stage_box_predictor {\n","      mask_rcnn_box_predictor {\n","        use_dropout: false\n","        dropout_keep_probability: 1.0\n","        fc_hyperparams {\n","          op: FC\n","          regularizer {\n","            l2_regularizer {\n","              weight: 0.0\n","            }\n","          }\n","          initializer {\n","            variance_scaling_initializer {\n","              factor: 1.0\n","              uniform: true\n","              mode: FAN_AVG\n","            }\n","          }\n","        }\n","      }\n","    }\n","    second_stage_post_processing {\n","      batch_non_max_suppression {\n","        score_threshold: 0.0\n","        iou_threshold: 0.6\n","        max_detections_per_class: 100\n","        max_total_detections: 300\n","      }\n","      score_converter: SOFTMAX\n","    }\n","    second_stage_localization_loss_weight: 2.0\n","    second_stage_classification_loss_weight: 1.0\n","  }\n","}\n","\n","train_config: {\n","  batch_size: 12\n","  optimizer {\n","    momentum_optimizer: {\n","      learning_rate: {\n","        manual_step_learning_rate {\n","          initial_learning_rate: 0.0002\n","          schedule {\n","            step: 900000\n","            learning_rate: .00002\n","          }\n","          schedule {\n","            step: 1200000\n","            learning_rate: .000002\n","          }\n","        }\n","      }\n","      momentum_optimizer_value: 0.9\n","    }\n","    use_moving_average: false\n","  }\n","  gradient_clipping_by_norm: 10.0\n","  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n","  from_detection_checkpoint: true\n","  load_all_detection_checkpoint_vars: true\n","  # Note: The below line limits the training process to 200K steps, which we\n","  # empirically found to be sufficient enough to train the pets dataset. This\n","  # effectively bypasses the learning rate schedule (the learning rate will\n","  # never decay). Remove the below line to train indefinitely.\n","  num_steps: 10000\n","  data_augmentation_options {\n","    random_horizontal_flip {\n","    }\n","  }\n","}\n","\n","\n","train_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"/content/drive/My Drive/Colab Notebooks/TIL/CV/input/tfrecord/coco_train.record-00000-of-00001\"\n","  }\n","  label_map_path: \"/content/drive/My Drive/Colab Notebooks/TIL/CV/input/tfrecord/tfrecord_label_map.pbtxt\"\n","}\n","\n","eval_config: {\n","  metrics_set: \"coco_detection_metrics\"\n","  num_examples: 1101\n","}\n","\n","eval_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"/content/drive/My Drive/Colab Notebooks/TIL/CV/input/tfrecord/coco_val.record-00000-of-00001\"\n","  }\n","  label_map_path: \"/content/drive/My Drive/Colab Notebooks/TIL/CV/input/tfrecord/tfrecord_label_map.pbtxt\"\n","  shuffle: false\n","  num_readers: 1\n","}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f11w0uO3jFCB","colab_type":"code","colab":{}},"source":["model_dir = '/content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/Roboflow_tf_save_folder/'\n","# Optionally remove content in output model directory to fresh start.\n","# !rm -rf {model_dir}\n","os.makedirs(model_dir, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23TECXvNezIF","colab_type":"text"},"source":["## Run Tensorboard(Optional)"]},{"cell_type":"code","metadata":{"id":"0H2PZs-mSCmO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1592549197474,"user_tz":-480,"elapsed":5191,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"f51de035-5ba8-44be-f3fb-ebc4e6be529b"},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip -o ngrok-stable-linux-amd64.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-06-19 06:46:33--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.20.175.105, 54.208.57.0, 34.225.3.211, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.20.175.105|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13773305 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n","\n","ngrok-stable-linux- 100%[===================>]  13.13M  17.7MB/s    in 0.7s    \n","\n","2020-06-19 06:46:34 (17.7 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G8o6r1o5SC5M","colab_type":"code","colab":{}},"source":["LOG_DIR = model_dir\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ge1OX7gcSC7S","colab_type":"code","colab":{}},"source":["get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m5GSGxZNh8rp","colab_type":"text"},"source":["### Get Tensorboard link"]},{"cell_type":"code","metadata":{"id":"rjhPT9iPSJ6T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592549200243,"user_tz":-480,"elapsed":5748,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"353f6c7d-3f71-4874-bbf8-2a8a5173f6f9"},"source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["https://57f8a2d1e88e.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JDddx2rPfex9","colab_type":"text"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"nC7_syR1SJ9F","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CjDHjhKQofT5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592148083177,"user_tz":-480,"elapsed":744972,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"23e906c6-0477-4e22-b37f-6bde985dc372"},"source":["!python /content/models/research/object_detection/model_main.py \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --model_dir={model_dir} \\\n","    --alsologtostderr \\\n","    --num_train_steps={num_steps} \\\n","    --num_eval_steps={num_eval_steps}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n","W0614 15:09:03.489730 139873361508224 model_lib.py:717] Forced number of epochs for all eval validations to be 1.\n","INFO:tensorflow:Maybe overwriting train_steps: 10000\n","I0614 15:09:03.489978 139873361508224 config_util.py:523] Maybe overwriting train_steps: 10000\n","INFO:tensorflow:Maybe overwriting use_bfloat16: False\n","I0614 15:09:03.490128 139873361508224 config_util.py:523] Maybe overwriting use_bfloat16: False\n","INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n","I0614 15:09:03.490294 139873361508224 config_util.py:523] Maybe overwriting sample_1_of_n_eval_examples: 1\n","INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n","I0614 15:09:03.490489 139873361508224 config_util.py:523] Maybe overwriting eval_num_epochs: 1\n","INFO:tensorflow:Maybe overwriting load_pretrained: True\n","I0614 15:09:03.490665 139873361508224 config_util.py:523] Maybe overwriting load_pretrained: True\n","INFO:tensorflow:Ignoring config override key: load_pretrained\n","I0614 15:09:03.490848 139873361508224 config_util.py:533] Ignoring config override key: load_pretrained\n","WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n","W0614 15:09:03.491832 139873361508224 model_lib.py:733] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n","INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False\n","I0614 15:09:03.491988 139873361508224 model_lib.py:768] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n","INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f366f88c198>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","I0614 15:09:03.492682 139873361508224 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f366f88c198>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f3655b370d0>) includes params argument, but params are not passed to Estimator.\n","W0614 15:09:03.492969 139873361508224 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f3655b370d0>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Not using Distribute Coordinator.\n","I0614 15:09:03.493767 139873361508224 estimator_training.py:186] Not using Distribute Coordinator.\n","INFO:tensorflow:Running training and evaluation locally (non-distributed).\n","I0614 15:09:03.494005 139873361508224 training.py:612] Running training and evaluation locally (non-distributed).\n","INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n","I0614 15:09:03.494292 139873361508224 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","W0614 15:09:03.504547 139873361508224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n","W0614 15:09:03.557690 139873361508224 dataset_builder.py:84] num_readers has been reduced to 1 to match input file shards.\n","WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.experimental.parallel_interleave(...)`.\n","W0614 15:09:03.564243 139873361508224 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:101: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.experimental.parallel_interleave(...)`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","W0614 15:09:03.564463 139873361508224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","2020-06-14 15:09:05.411570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-14 15:09:05.426882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:09:05.427968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-06-14 15:09:05.428263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n","2020-06-14 15:09:05.430260: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n","2020-06-14 15:09:05.431826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n","2020-06-14 15:09:05.432389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n","2020-06-14 15:09:05.435863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n","2020-06-14 15:09:05.438113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n","2020-06-14 15:09:05.443855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-14 15:09:05.443999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:09:05.444902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:09:05.445696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n","WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:77: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","W0614 15:09:16.006734 139873361508224 deprecation.py:323] From /content/models/research/object_detection/inputs.py:77: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0614 15:09:16.125802 139873361508224 deprecation.py:323] From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:259: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0614 15:09:21.008464 139873361508224 deprecation.py:323] From /content/models/research/object_detection/inputs.py:259: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:174: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n","W0614 15:09:25.552515 139873361508224 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:174: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n","INFO:tensorflow:Calling model_fn.\n","I0614 15:09:25.569264 139873361508224 estimator.py:1148] Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","W0614 15:09:25.812973 139873361508224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:09:27.718729 139873361508224 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:09:27.735281 139873361508224 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0614 15:09:27.735737 139873361508224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","WARNING:tensorflow:From /content/models/research/object_detection/utils/spatial_transform_ops.py:428: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","W0614 15:09:36.121392 139873361508224 deprecation.py:506] From /content/models/research/object_detection/utils/spatial_transform_ops.py:428: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","W0614 15:09:36.955143 139873361508224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:09:36.957990 139873361508224 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:09:36.978923 139873361508224 regularizers.py:99] Scale of 0 disables regularizer.\n","W0614 15:09:37.071536 139873361508224 variables_helper.py:161] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[360]], model variable shape: [[20]]. This variable will not be initialized from the checkpoint.\n","W0614 15:09:37.071681 139873361508224 variables_helper.py:161] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1024, 360]], model variable shape: [[1024, 20]]. This variable will not be initialized from the checkpoint.\n","W0614 15:09:37.071831 139873361508224 variables_helper.py:161] Variable [SecondStageBoxPredictor/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[91]], model variable shape: [[6]]. This variable will not be initialized from the checkpoint.\n","W0614 15:09:37.071956 139873361508224 variables_helper.py:161] Variable [SecondStageBoxPredictor/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1024, 91]], model variable shape: [[1024, 6]]. This variable will not be initialized from the checkpoint.\n","W0614 15:09:37.072804 139873361508224 variables_helper.py:164] Variable [global_step] is not available in checkpoint\n","WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:347: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","W0614 15:09:41.501946 139873361508224 deprecation.py:323] From /content/models/research/object_detection/core/losses.py:347: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","hello\n","INFO:tensorflow:Done calling model_fn.\n","I0614 15:09:51.378172 139873361508224 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","I0614 15:09:51.379950 139873361508224 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","I0614 15:09:56.368273 139873361508224 monitored_session.py:240] Graph was finalized.\n","2020-06-14 15:09:56.368813: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2020-06-14 15:09:56.373890: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n","2020-06-14 15:09:56.374284: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c999480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-14 15:09:56.374319: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-14 15:09:56.472277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:09:56.473148: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c9992c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-14 15:09:56.473181: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-14 15:09:56.473410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:09:56.474201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-06-14 15:09:56.474303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n","2020-06-14 15:09:56.474348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n","2020-06-14 15:09:56.474384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n","2020-06-14 15:09:56.474430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n","2020-06-14 15:09:56.474466: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n","2020-06-14 15:09:56.474500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n","2020-06-14 15:09:56.474535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-14 15:09:56.474646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:09:56.475471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:09:56.476272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n","2020-06-14 15:09:56.476330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n","2020-06-14 15:09:56.478085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-14 15:09:56.478120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n","2020-06-14 15:09:56.478138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n","2020-06-14 15:09:56.478311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:09:56.479296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:09:56.480213: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-14 15:09:56.480284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-0\n","I0614 15:09:56.482632 139873361508224 saver.py:1284] Restoring parameters from training/model.ckpt-0\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","W0614 15:09:58.979305 139873361508224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","INFO:tensorflow:Running local_init_op.\n","I0614 15:09:59.975832 139873361508224 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0614 15:10:00.496597 139873361508224 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.\n","I0614 15:10:14.648790 139873361508224 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.\n","2020-06-14 15:10:27.235201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n","2020-06-14 15:10:32.090580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","INFO:tensorflow:loss = 3.058229, step = 0\n","I0614 15:10:41.726946 139873361508224 basic_session_run_hooks.py:262] loss = 3.058229, step = 0\n","INFO:tensorflow:global_step/sec: 0.803769\n","I0614 15:12:46.139895 139873361508224 basic_session_run_hooks.py:692] global_step/sec: 0.803769\n","INFO:tensorflow:loss = 1.0384508, step = 100 (124.425 sec)\n","I0614 15:12:46.151778 139873361508224 basic_session_run_hooks.py:260] loss = 1.0384508, step = 100 (124.425 sec)\n","INFO:tensorflow:global_step/sec: 0.780656\n","I0614 15:14:54.237189 139873361508224 basic_session_run_hooks.py:692] global_step/sec: 0.780656\n","INFO:tensorflow:loss = 1.3094667, step = 200 (128.087 sec)\n","I0614 15:14:54.238435 139873361508224 basic_session_run_hooks.py:260] loss = 1.3094667, step = 200 (128.087 sec)\n","2020-06-14 15:15:17.123626: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 371614530 exceeds 10% of system memory.\n","2020-06-14 15:15:17.852280: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1486458120 exceeds 10% of system memory.\n","tcmalloc: large alloc 1486462976 bytes == 0x128b78000 @  0x7f36cdc1ab6b 0x7f36cdc3a379 0x7f3683942ee7 0x7f368373051f 0x7f36835fb05b 0x7f36835c0a36 0x7f36835c18c3 0x7f36835c1a93 0x7f368889fc7b 0x7f3683874e0c 0x7f3683867575 0x7f3683925021 0x7f3683922718 0x7f36cc51a6df 0x7f36cd5fc6db 0x7f36cd93588f\n","2020-06-14 15:15:18.915185: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1486458120 exceeds 10% of system memory.\n","tcmalloc: large alloc 1486462976 bytes == 0x1842a2000 @  0x7f36cdc1ab6b 0x7f36cdc3a379 0x7f3683942ee7 0x7f368373051f 0x7f36835fb05b 0x7f36835c0a36 0x7f36835c18c3 0x7f36835c1a93 0x7f3687e22866 0x7f3683874e0c 0x7f3683867575 0x7f3683925021 0x7f3683922718 0x7f36cc51a6df 0x7f36cd5fc6db 0x7f36cd93588f\n","INFO:tensorflow:global_step/sec: 0.728552\n","I0614 15:17:11.503387 139873361508224 basic_session_run_hooks.py:692] global_step/sec: 0.728552\n","INFO:tensorflow:loss = 0.7805402, step = 300 (137.266 sec)\n","I0614 15:17:11.504487 139873361508224 basic_session_run_hooks.py:260] loss = 0.7805402, step = 300 (137.266 sec)\n","2020-06-14 15:18:50.483840: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 380300508 exceeds 10% of system memory.\n","2020-06-14 15:18:50.636335: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 380300508 exceeds 10% of system memory.\n","INFO:tensorflow:global_step/sec: 0.783642\n","I0614 15:19:19.105094 139873361508224 basic_session_run_hooks.py:692] global_step/sec: 0.783642\n","INFO:tensorflow:loss = 0.9726857, step = 400 (127.602 sec)\n","I0614 15:19:19.106219 139873361508224 basic_session_run_hooks.py:260] loss = 0.9726857, step = 400 (127.602 sec)\n","INFO:tensorflow:Saving checkpoints for 446 into training/model.ckpt.\n","I0614 15:20:18.810466 139873361508224 basic_session_run_hooks.py:606] Saving checkpoints for 446 into training/model.ckpt.\n","INFO:tensorflow:Calling model_fn.\n","I0614 15:20:22.304383 139873361508224 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:20:24.124252 139873361508224 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:20:24.141675 139873361508224 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0614 15:20:24.142078 139873361508224 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:20:25.668987 139873361508224 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:20:25.689665 139873361508224 regularizers.py:99] Scale of 0 disables regularizer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n","Instructions for updating:\n","`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n","W0614 15:20:26.354264 139873361508224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n","Instructions for updating:\n","`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/eval_util.py:828: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0614 15:20:27.064114 139873361508224 deprecation.py:323] From /content/models/research/object_detection/eval_util.py:828: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","tf.py_func is deprecated in TF V2. Instead, there are two\n","    options available in V2.\n","    - tf.py_function takes a python function which manipulates tf eager\n","    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n","    an ndarray (just call tensor.numpy()) but having access to eager tensors\n","    means `tf.py_function`s can use accelerators such as GPUs as well as\n","    being differentiable using a gradient tape.\n","    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n","    (it is not differentiable, and manipulates numpy arrays). It drops the\n","    stateful argument making all functions stateful.\n","    \n","W0614 15:20:27.317102 139873361508224 deprecation.py:323] From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","tf.py_func is deprecated in TF V2. Instead, there are two\n","    options available in V2.\n","    - tf.py_function takes a python function which manipulates tf eager\n","    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n","    an ndarray (just call tensor.numpy()) but having access to eager tensors\n","    means `tf.py_function`s can use accelerators such as GPUs as well as\n","    being differentiable using a gradient tape.\n","    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n","    (it is not differentiable, and manipulates numpy arrays). It drops the\n","    stateful argument making all functions stateful.\n","    \n","INFO:tensorflow:Done calling model_fn.\n","I0614 15:20:28.016025 139873361508224 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-06-14T15:20:28Z\n","I0614 15:20:28.034092 139873361508224 evaluation.py:255] Starting evaluation at 2020-06-14T15:20:28Z\n","INFO:tensorflow:Graph was finalized.\n","I0614 15:20:28.564782 139873361508224 monitored_session.py:240] Graph was finalized.\n","2020-06-14 15:20:28.566096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:20:28.566644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-06-14 15:20:28.694823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n","2020-06-14 15:20:28.694925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n","2020-06-14 15:20:28.694983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n","2020-06-14 15:20:28.695036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n","2020-06-14 15:20:28.695082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n","2020-06-14 15:20:28.695122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n","2020-06-14 15:20:28.695165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-14 15:20:28.695364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:20:28.696200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:20:28.696736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n","2020-06-14 15:20:28.696807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-14 15:20:28.696826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n","2020-06-14 15:20:28.696840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n","2020-06-14 15:20:28.696979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:20:28.697505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:20:28.697932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-446\n","I0614 15:20:28.699371 139873361508224 saver.py:1284] Restoring parameters from training/model.ckpt-446\n","INFO:tensorflow:Running local_init_op.\n","I0614 15:20:29.701767 139873361508224 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0614 15:20:29.841992 139873361508224 session_manager.py:502] Done running local_init_op.\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KP-tUdtnRybs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1592148088047,"user_tz":-480,"elapsed":3198,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"0ff89740-9d69-4733-8342-e260e21c04d6"},"source":["# /content/models/research/training\n","!ls {model_dir}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["checkpoint\t\t\t\t     model.ckpt-0.index\n","events.out.tfevents.1592144802.e43594651d21  model.ckpt-0.meta\n","events.out.tfevents.1592145810.e43594651d21  model.ckpt-446.data-00000-of-00001\n","events.out.tfevents.1592147393.e43594651d21  model.ckpt-446.index\n","graph.pbtxt\t\t\t\t     model.ckpt-446.meta\n","model.ckpt-0.data-00000-of-00001\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OmSESMetj1sa","colab_type":"text"},"source":["## Exporting a Trained Inference Graph\n","Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"]},{"cell_type":"code","metadata":{"id":"DHoP90pUyKSq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592148129073,"user_tz":-480,"elapsed":44209,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"3523ffdd-7ffc-47dc-cdfa-5afd243e0aba"},"source":["import re\n","import numpy as np\n","\n","output_directory = './fine_tuned_model'\n","\n","lst = os.listdir(model_dir)\n","lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n","steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n","last_model = lst[steps.argmax()].replace('.meta', '')\n","\n","last_model_path = os.path.join(model_dir, last_model)\n","print(last_model_path)\n","!python /content/models/research/object_detection/export_inference_graph.py \\\n","    --input_type=image_tensor \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --output_directory={output_directory} \\\n","    --trained_checkpoint_prefix={last_model_path}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["training/model.ckpt-446\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","W0614 15:21:44.195484 140602950838144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:21:45.963340 140602950838144 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:21:45.980528 140602950838144 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0614 15:21:45.980917 140602950838144 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","WARNING:tensorflow:From /content/models/research/object_detection/core/box_list_ops.py:166: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0614 15:21:46.044064 140602950838144 deprecation.py:323] From /content/models/research/object_detection/core/box_list_ops.py:166: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/models/research/object_detection/utils/spatial_transform_ops.py:428: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","W0614 15:21:46.706394 140602950838144 deprecation.py:506] From /content/models/research/object_detection/utils/spatial_transform_ops.py:428: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","W0614 15:21:47.449874 140602950838144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:21:47.458723 140602950838144 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0614 15:21:47.485853 140602950838144 regularizers.py:99] Scale of 0 disables regularizer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n","Instructions for updating:\n","`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n","W0614 15:21:48.386873 140602950838144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n","Instructions for updating:\n","`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:400: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.get_or_create_global_step\n","W0614 15:21:48.499673 140602950838144 deprecation.py:323] From /content/models/research/object_detection/exporter.py:400: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.get_or_create_global_step\n","WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:555: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n","Instructions for updating:\n","Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n","W0614 15:21:48.503055 140602950838144 deprecation.py:323] From /content/models/research/object_detection/exporter.py:555: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n","Instructions for updating:\n","Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n","W0614 15:21:48.504433 140602950838144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n","215 ops no flops stats due to incomplete shapes.\n","Parsing Inputs...\n","Incomplete shape.\n","\n","=========================Options=============================\n","-max_depth                  10000\n","-min_bytes                  0\n","-min_peak_bytes             0\n","-min_residual_bytes         0\n","-min_output_bytes           0\n","-min_micros                 0\n","-min_accelerator_micros     0\n","-min_cpu_micros             0\n","-min_params                 0\n","-min_float_ops              0\n","-min_occurrence             0\n","-step                       -1\n","-order_by                   name\n","-account_type_regexes       _trainable_variables\n","-start_name_regexes         .*\n","-trim_name_regexes          .*BatchNorm.*\n","-show_name_regexes          .*\n","-hide_name_regexes          \n","-account_displayed_op_only  true\n","-select                     params\n","-output                     stdout:\n","\n","==================Model Analysis Report======================\n","Incomplete shape.\n","\n","Doc:\n","scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n","param: Number of parameters (in the Variable).\n","\n","Profile:\n","node name | # parameters\n","_TFProfRoot (--/12.86m params)\n","  Conv (--/2.65m params)\n","    Conv/biases (512, 512/512 params)\n","    Conv/weights (3x3x576x512, 2.65m/2.65m params)\n","  FirstStageBoxPredictor (--/36.94k params)\n","    FirstStageBoxPredictor/BoxEncodingPredictor (--/24.62k params)\n","      FirstStageBoxPredictor/BoxEncodingPredictor/biases (48, 48/48 params)\n","      FirstStageBoxPredictor/BoxEncodingPredictor/weights (1x1x512x48, 24.58k/24.58k params)\n","    FirstStageBoxPredictor/ClassPredictor (--/12.31k params)\n","      FirstStageBoxPredictor/ClassPredictor/biases (24, 24/24 params)\n","      FirstStageBoxPredictor/ClassPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n","  FirstStageFeatureExtractor (--/4.25m params)\n","    FirstStageFeatureExtractor/InceptionV2 (--/4.25m params)\n","      FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7 (--/2.71k params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm (--/0 params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights (7x7x3x8, 1.18k/1.18k params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights (1x1x24x64, 1.54k/1.54k params)\n","      FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1 (--/4.10k params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm (--/0 params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights (1x1x64x64, 4.10k/4.10k params)\n","      FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3 (--/110.59k params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm (--/0 params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights (3x3x64x192, 110.59k/110.59k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_3b (--/218.11k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0 (--/12.29k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1 (--/12.29k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1 (--/49.15k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1 (--/12.29k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3 (--/36.86k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x64, 36.86k/36.86k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2 (--/150.53k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1 (--/12.29k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3 (--/6.14k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1 (--/6.14k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights (1x1x192x32, 6.14k/6.14k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_3c (--/259.07k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0 (--/16.38k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1 (--/16.38k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1 (--/71.68k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1 (--/16.38k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2 (--/154.62k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1 (--/16.38k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3 (--/16.38k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1 (--/16.38k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4a (--/384.00k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0 (--/225.28k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1 (--/40.96k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights (1x1x320x128, 40.96k/40.96k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3 (--/184.32k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1 (--/158.72k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1 (--/20.48k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights (1x1x320x64, 20.48k/20.48k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3 (--/82.94k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4b (--/608.26k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0 (--/129.02k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1 (--/129.02k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights (1x1x576x224, 129.02k/129.02k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1 (--/92.16k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1 (--/36.86k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights (1x1x576x64, 36.86k/36.86k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2 (--/313.34k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3 (--/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4c (--/663.55k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0 (--/110.59k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1 (--/110.59k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1 (--/165.89k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3 (--/110.59k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2 (--/313.34k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3 (--/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4d (--/893.95k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0 (--/92.16k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1 (--/92.16k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1 (--/258.05k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3 (--/184.32k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2 (--/488.45k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3 (--/184.32k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3 (--/230.40k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights (3x3x160x160, 230.40k/230.40k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3 (--/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4e (--/1.11m params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0 (--/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1 (--/294.91k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3 (--/221.18k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2 (--/700.42k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1 (--/92.16k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3 (--/276.48k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights (3x3x160x192, 276.48k/276.48k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3 (--/331.78k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights (3x3x192x192, 331.78k/331.78k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3 (--/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","  SecondStageBoxPredictor (--/26.65k params)\n","    SecondStageBoxPredictor/BoxEncodingPredictor (--/20.50k params)\n","      SecondStageBoxPredictor/BoxEncodingPredictor/biases (20, 20/20 params)\n","      SecondStageBoxPredictor/BoxEncodingPredictor/weights (1024x20, 20.48k/20.48k params)\n","    SecondStageBoxPredictor/ClassPredictor (--/6.15k params)\n","      SecondStageBoxPredictor/ClassPredictor/biases (6, 6/6 params)\n","      SecondStageBoxPredictor/ClassPredictor/weights (1024x6, 6.14k/6.14k params)\n","  SecondStageFeatureExtractor (--/5.89m params)\n","    SecondStageFeatureExtractor/InceptionV2 (--/5.89m params)\n","      SecondStageFeatureExtractor/InceptionV2/Mixed_5a (--/1.44m params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0 (--/294.91k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1 (--/73.73k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3 (--/221.18k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1 (--/1.14m params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1 (--/110.59k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3 (--/442.37k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights (3x3x192x256, 442.37k/442.37k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3 (--/589.82k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights (3x3x256x256, 589.82k/589.82k params)\n","      SecondStageFeatureExtractor/InceptionV2/Mixed_5b (--/2.18m params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0 (--/360.45k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1 (--/749.57k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2 (--/937.98k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1 (--/163.84k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x160, 163.84k/163.84k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3 (--/322.56k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights (3x3x160x224, 322.56k/322.56k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3 (--/131.07k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n","      SecondStageFeatureExtractor/InceptionV2/Mixed_5c (--/2.28m params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0 (--/360.45k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1 (--/749.57k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2 (--/1.04m params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1 (--/196.61k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3 (--/387.07k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights (3x3x192x224, 387.07k/387.07k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3 (--/131.07k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n","\n","======================End of Report==========================\n","215 ops no flops stats due to incomplete shapes.\n","Parsing Inputs...\n","Incomplete shape.\n","\n","=========================Options=============================\n","-max_depth                  10000\n","-min_bytes                  0\n","-min_peak_bytes             0\n","-min_residual_bytes         0\n","-min_output_bytes           0\n","-min_micros                 0\n","-min_accelerator_micros     0\n","-min_cpu_micros             0\n","-min_params                 0\n","-min_float_ops              1\n","-min_occurrence             0\n","-step                       -1\n","-order_by                   float_ops\n","-account_type_regexes       .*\n","-start_name_regexes         .*\n","-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n","-show_name_regexes          .*\n","-hide_name_regexes          \n","-account_displayed_op_only  true\n","-select                     float_ops\n","-output                     stdout:\n","\n","==================Model Analysis Report======================\n","Incomplete shape.\n","\n","Doc:\n","scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n","flops: Number of float operations. Note: Please read the implementation for the math behind it.\n","\n","Profile:\n","node name | # float_ops\n","_TFProfRoot (--/6.17k flops)\n","  map/while/ToNormalizedCoordinates/Scale/mul (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_2 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_3 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Minimum (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_1 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_2 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_3 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_1 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Maximum (300/300 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul (300/300 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_1 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_2 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_3 (300/300 flops)\n","  map/while/ToNormalizedCoordinates/Scale/mul_1 (300/300 flops)\n","  map/while/ToNormalizedCoordinates/Scale/mul_2 (300/300 flops)\n","  map/while/ToNormalizedCoordinates/Scale/mul_3 (300/300 flops)\n","  map_2/while/mul_3 (300/300 flops)\n","  map_2/while/mul_2 (300/300 flops)\n","  map_2/while/mul_1 (300/300 flops)\n","  map_2/while/mul (300/300 flops)\n","  GridAnchorGenerator/mul (12/12 flops)\n","  GridAnchorGenerator/mul_1 (12/12 flops)\n","  GridAnchorGenerator/mul_2 (12/12 flops)\n","  GridAnchorGenerator/truediv (12/12 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_9 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_8 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_7 (1/1 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n","  mul (1/1 flops)\n","  map_2/while/Less_1 (1/1 flops)\n","  map_2/while/Less (1/1 flops)\n","  map_1/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n","  map_1/while/ToNormalizedCoordinates/truediv (1/1 flops)\n","  map_1/while/Less_1 (1/1 flops)\n","  map_1/while/Less (1/1 flops)\n","  map/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n","  map/while/ToNormalizedCoordinates/truediv (1/1 flops)\n","  map/while/Less_1 (1/1 flops)\n","  map/while/Less (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/truediv (1/1 flops)\n","  SecondStagePostprocessor/map/while/Less_1 (1/1 flops)\n","  SecondStagePostprocessor/map/while/Less (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_4 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_3 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_4 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_5 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_2 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_3 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_5 (1/1 flops)\n","  Preprocessor/map/while/Less (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n","  FirstStageFeatureExtractor/GreaterEqual (1/1 flops)\n","  FirstStageFeatureExtractor/GreaterEqual_1 (1/1 flops)\n","  GridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n","  GridAnchorGenerator/mul_7 (1/1 flops)\n","  GridAnchorGenerator/mul_8 (1/1 flops)\n","  GridAnchorGenerator/zeros/Less (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_6 (1/1 flops)\n","  Preprocessor/map/while/Less_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchGather/mul (1/1 flops)\n","  SecondStagePostprocessor/BatchGather/mul_2 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n","\n","======================End of Report==========================\n","2020-06-14 15:21:51.058547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-14 15:21:51.113671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:21:51.114531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-06-14 15:21:51.128389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n","2020-06-14 15:21:51.352334: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n","2020-06-14 15:21:51.442166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n","2020-06-14 15:21:51.470719: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n","2020-06-14 15:21:51.749465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n","2020-06-14 15:21:51.883927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n","2020-06-14 15:21:52.438413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-14 15:21:52.438785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:21:52.439845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:21:52.440910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n","2020-06-14 15:21:52.441447: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2020-06-14 15:21:52.455204: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n","2020-06-14 15:21:52.455633: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3292d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-14 15:21:52.455673: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-14 15:21:52.591470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:21:52.592389: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3292a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-14 15:21:52.592433: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-14 15:21:52.592972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:21:52.593673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-06-14 15:21:52.593766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n","2020-06-14 15:21:52.593815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n","2020-06-14 15:21:52.593872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n","2020-06-14 15:21:52.593919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n","2020-06-14 15:21:52.593957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n","2020-06-14 15:21:52.594009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n","2020-06-14 15:21:52.594053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-14 15:21:52.594181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:21:52.595097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:21:52.595950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n","2020-06-14 15:21:52.600563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n","2020-06-14 15:21:52.602325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-14 15:21:52.602361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n","2020-06-14 15:21:52.602378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n","2020-06-14 15:21:52.603760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:21:52.604766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:21:52.605588: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-14 15:21:52.605645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-446\n","I0614 15:21:52.607723 140602950838144 saver.py:1284] Restoring parameters from training/model.ckpt-446\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","W0614 15:22:03.812234 140602950838144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","2020-06-14 15:22:04.639474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:04.640396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-06-14 15:22:04.640490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n","2020-06-14 15:22:04.640554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n","2020-06-14 15:22:04.640594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n","2020-06-14 15:22:04.640638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n","2020-06-14 15:22:04.640677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n","2020-06-14 15:22:04.640729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n","2020-06-14 15:22:04.640767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-14 15:22:04.640883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:04.641630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:04.642310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n","2020-06-14 15:22:04.642384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-14 15:22:04.642403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n","2020-06-14 15:22:04.642416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n","2020-06-14 15:22:04.642562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:04.643421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:04.644097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-446\n","I0614 15:22:04.645348 140602950838144 saver.py:1284] Restoring parameters from training/model.ckpt-446\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n","W0614 15:22:05.393538 140602950838144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.extract_sub_graph`\n","W0614 15:22:05.393886 140602950838144 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.extract_sub_graph`\n","INFO:tensorflow:Froze 356 variables.\n","I0614 15:22:05.837992 140602950838144 graph_util_impl.py:334] Froze 356 variables.\n","INFO:tensorflow:Converted 356 variables to const ops.\n","I0614 15:22:05.970102 140602950838144 graph_util_impl.py:394] Converted 356 variables to const ops.\n","2020-06-14 15:22:06.192632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:06.193425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2020-06-14 15:22:06.193527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n","2020-06-14 15:22:06.193568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n","2020-06-14 15:22:06.193609: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n","2020-06-14 15:22:06.193655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n","2020-06-14 15:22:06.193693: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n","2020-06-14 15:22:06.193765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n","2020-06-14 15:22:06.193805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-14 15:22:06.193940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:06.194739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:06.195403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n","2020-06-14 15:22:06.195451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-14 15:22:06.195472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n","2020-06-14 15:22:06.195485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n","2020-06-14 15:22:06.195610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:06.196506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-14 15:22:06.197184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:326: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n","W0614 15:22:06.671609 140602950838144 deprecation.py:323] From /content/models/research/object_detection/exporter.py:326: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n","INFO:tensorflow:No assets to save.\n","I0614 15:22:06.672534 140602950838144 builder_impl.py:640] No assets to save.\n","INFO:tensorflow:No assets to write.\n","I0614 15:22:06.672747 140602950838144 builder_impl.py:460] No assets to write.\n","INFO:tensorflow:SavedModel written to: ./fine_tuned_model/saved_model/saved_model.pb\n","I0614 15:22:07.115248 140602950838144 builder_impl.py:425] SavedModel written to: ./fine_tuned_model/saved_model/saved_model.pb\n","INFO:tensorflow:Writing pipeline config file to ./fine_tuned_model/pipeline.config\n","I0614 15:22:07.172580 140602950838144 config_util.py:225] Writing pipeline config file to ./fine_tuned_model/pipeline.config\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"usgBZvkz0nqD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1592148131996,"user_tz":-480,"elapsed":47122,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"23c50a0b-5d3c-47ab-aaf0-93502ad9b1b0"},"source":["!ls {output_directory}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["checkpoint\t\t\tmodel.ckpt.index  saved_model\n","frozen_inference_graph.pb\tmodel.ckpt.meta\n","model.ckpt.data-00000-of-00001\tpipeline.config\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p09AOThWkaQv","colab_type":"text"},"source":["## Download the model `.pb` file"]},{"cell_type":"code","metadata":{"id":"CnDo1lonKgFr","colab_type":"code","colab":{}},"source":["import os\n","\n","pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")\n","assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lHqWkLBINYoI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592148134617,"user_tz":-480,"elapsed":49729,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"b212d827-36df-481b-8aa8-c6a5a6197384"},"source":["!ls -alh {pb_fname}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-rw-r--r-- 1 root root 50M Jun 14 15:22 /content/models/research/fine_tuned_model/frozen_inference_graph.pb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KNP64varaH-M","colab_type":"code","colab":{}},"source":["!cp /content/models/research/fine_tuned_model/frozen_inference_graph.pb \"/content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/roboflow_faster_rcnn_inception_v2\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FIqnjbWYsuQw","colab_type":"text"},"source":["### Option1 : upload the `.pb` file to your Google Drive\n","Then download it from your Google Drive to local file system.\n","\n","During this step, you will be prompted to enter the token."]},{"cell_type":"code","metadata":{"id":"hAqyASIJqjae","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592150069482,"user_tz":-480,"elapsed":24991,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"6de818a8-3915-490b-fde0-9196c2021b73"},"source":["# Install the PyDrive wrapper & import libraries.\n","# This only needs to be done once in a notebook.\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once in a notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","fname = os.path.basename(pb_fname)\n","# Create & upload a text file.\n","uploaded = drive.CreateFile({'title': fname})\n","uploaded.SetContentFile(pb_fname)\n","uploaded.Upload()\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uploaded file with ID 1xxOWOG26NV1tKxH8d7Vg4wOwXd8sWNLW\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2FKFq8RXs6bs","colab_type":"text"},"source":["### Option2 :  Download the `.pb` file directly to your local file system\n","This method may not be stable when downloading large files like the model `.pb` file. Try **option 1** instead if not working."]},{"cell_type":"code","metadata":{"id":"-bP0iMMnnr77","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.download(pb_fname)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFyCeiBb9BbS","colab_type":"text"},"source":["### OPTIONAL: Download the `label_map.pbtxt` file"]},{"cell_type":"code","metadata":{"id":"K1TbL6Ox8q6Z","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.download(label_map_pbtxt_fname)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUmAo9foa1xq","colab_type":"text"},"source":["### OPTIONAL: Download the modified pipline file\n","If you plan to use OpenVINO toolkit to convert the `.pb` file to inference faster on Intel's hardware (CPU/GPU, Movidius, etc.)"]},{"cell_type":"code","metadata":{"id":"pql2QpemazE1","colab_type":"code","colab":{}},"source":["files.download(pipeline_fname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1AgBj1l0v_W","colab_type":"code","colab":{}},"source":["# !tar cfz fine_tuned_model.tar.gz fine_tuned_model\n","# from google.colab import files\n","# files.download('fine_tuned_model.tar.gz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lnx57Mbe72yY","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mz1gX19GlVW7","colab_type":"text"},"source":["## Run inference test\n","\n","To test on your own images, you need to upload raw test images to the `test` folder located inside `/data`.\n","\n","Right now, this folder contains TFRecord files from Roboflow. We need the raw images.\n"]},{"cell_type":"code","metadata":{"id":"45ENiVg_74Lf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"3fbac66d-344d-4127-9ff5-5114d4bea248"},"source":["# optionally, remove the TFRecord and cells_label_map.pbtxt from\n","# the test directory so it is only raw images\n","%cd {repo_dir_path}\n","%cd data/test\n","%rm cells.tfrecord\n","%rm cells_label_map.pbtxt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/tensorflow-object-detection-faster-rcnn\n","/content/tensorflow-object-detection-faster-rcnn/data/test\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pzj9A4e5mj5l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593807659569,"user_tz":-480,"elapsed":4934,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"a6161083-b558-44ca-e8e6-9cb3b3baca45"},"source":["import os\n","import glob\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","# PATH_TO_CKPT = pb_fname\n","\n","# List of the strings that is used to add correct label for each box.\n","# PATH_TO_LABELS = label_map_pbtxt_fname\n","\n","# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n","PATH_TO_TEST_IMAGES_DIR = \"/content/drive/My Drive/Colab Notebooks/TIL/CV/DeepFashion2/train/test/\"  # os.path.join(repo_dir_path, \"data/test\")\n","PATH_TO_TEST_IMAGES_DIR = \"/content/drive/My Drive/Colab Notebooks/TIL/Search Rescue/Barbie/test/images\"\n","# PATH_TO_TEST_IMAGES_DIR = \"/content/drive/My Drive/Colab Notebooks/TIL/Search Rescue/Barbie/test/test_doll_video/dresses\"\n","# PATH_TO_TEST_IMAGES_DIR = \"/content/drive/My Drive/Colab Notebooks/TIL/Search Rescue/Barbie/doll\"\n","\n","# assert os.path.isfile(pb_fname)\n","assert os.path.isfile(PATH_TO_LABELS)\n","TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n","assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n","print(TEST_IMAGE_PATHS[:2])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['/content/drive/My Drive/Colab Notebooks/TIL/Search Rescue/Barbie/test/images/dresses_0.png', '/content/drive/My Drive/Colab Notebooks/TIL/Search Rescue/Barbie/test/images/dresses_1.png']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dNFc5CM3Duav","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593808158038,"user_tz":-480,"elapsed":2099,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"80126e6a-5585-4e31-d98b-0669ff213e4d"},"source":["%cd /content/models/research/object_detection\n","\n","import numpy as np\n","import os\n","import six.moves.urllib as urllib\n","import sys\n","import tarfile\n","import tensorflow as tf\n","import zipfile\n","\n","from collections import defaultdict\n","from io import StringIO\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","\n","# This is needed since the notebook is stored in the object_detection folder.\n","sys.path.append(\"..\")\n","from object_detection.utils import ops as utils_ops\n","\n","\n","# This is needed to display the images.\n","%matplotlib inline\n","\n","\n","from object_detection.utils import label_map_util\n","\n","from object_detection.utils import visualization_utils as vis_util"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/models/research/object_detection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CG5YUMdg1Po7","colab_type":"code","colab":{}},"source":["detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","    od_graph_def = tf.GraphDef()\n","    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n","        serialized_graph = fid.read()\n","        od_graph_def.ParseFromString(serialized_graph)\n","        tf.import_graph_def(od_graph_def, name='')\n","\n","\n","label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n","categories = label_map_util.convert_label_map_to_categories(\n","    label_map, max_num_classes=num_classes, use_display_name=True)\n","category_index = label_map_util.create_category_index(categories)\n","\n","\n","def load_image_into_numpy_array(image):\n","    (im_width, im_height) = image.size\n","    return np.array(image.getdata()).reshape(\n","        (im_height, im_width, 3)).astype(np.uint8)\n","\n","# Size, in inches, of the output images.\n","IMAGE_SIZE = (12, 8)\n","\n","\n","def run_inference_for_single_image(image, graph):\n","    with graph.as_default():\n","        with tf.Session() as sess:\n","            # Get handles to input and output tensors\n","            ops = tf.get_default_graph().get_operations()\n","            all_tensor_names = {\n","                output.name for op in ops for output in op.outputs}\n","            tensor_dict = {}\n","            for key in [\n","                'num_detections', 'detection_boxes', 'detection_scores',\n","                'detection_classes', 'detection_masks'\n","            ]:\n","                tensor_name = key + ':0'\n","                if tensor_name in all_tensor_names:\n","                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n","                        tensor_name)\n","            if 'detection_masks' in tensor_dict:\n","                # The following processing is only for single image\n","                detection_boxes = tf.squeeze(\n","                    tensor_dict['detection_boxes'], [0])\n","                detection_masks = tf.squeeze(\n","                    tensor_dict['detection_masks'], [0])\n","                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n","                real_num_detection = tf.cast(\n","                    tensor_dict['num_detections'][0], tf.int32)\n","                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n","                                           real_num_detection, -1])\n","                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n","                                           real_num_detection, -1, -1])\n","                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n","                detection_masks_reframed = tf.cast(\n","                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n","                # Follow the convention by adding back the batch dimension\n","                tensor_dict['detection_masks'] = tf.expand_dims(\n","                    detection_masks_reframed, 0)\n","            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n","\n","            # Run inference\n","            output_dict = sess.run(tensor_dict,\n","                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n","\n","            # all outputs are float32 numpy arrays, so convert types as appropriate\n","            output_dict['num_detections'] = int(\n","                output_dict['num_detections'][0])\n","            output_dict['detection_classes'] = output_dict[\n","                'detection_classes'][0].astype(np.uint8)\n","            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n","            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n","            if 'detection_masks' in output_dict:\n","                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n","    return output_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-IbKIjbY_MRk","colab_type":"code","colab":{}},"source":["# Output images not showing? Run this cell again, and try the cell above\n","# This is needed to display the images.\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdoCWBLtFG_R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"status":"ok","timestamp":1593802040388,"user_tz":-480,"elapsed":10587,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"f454808e-7fe6-461c-b501-81f8af165c15"},"source":["!fc-list | grep \"\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf: Liberation Sans Narrow:style=Italic\n","/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf: Liberation Sans:style=Regular\n","/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf: Liberation Mono:style=Bold Italic\n","/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf: Liberation Serif:style=Italic\n","/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf: Liberation Mono:style=Bold\n","/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf: Liberation Sans Narrow:style=Regular\n","/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf: Liberation Serif:style=Bold\n","/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf: Liberation Mono:style=Regular\n","/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf: Liberation Sans:style=Italic\n","/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf: Liberation Serif:style=Bold Italic\n","/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf: Liberation Sans Narrow:style=Bold Italic\n","/usr/share/fonts/truetype/liberation/LiberationMono-Italic.ttf: Liberation Mono:style=Italic\n","/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf: Liberation Sans:style=Bold Italic\n","/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf: Liberation Serif:style=Regular\n","/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Bold.ttf: Liberation Sans Narrow:style=Bold\n","/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf: Liberation Sans:style=Bold\n","/usr/share/fonts/truetype/humor-sans/Humor-Sans.ttf: Humor Sans:style=Regular\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RxwsMsiQlVBi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"li19wos_MVqK","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ABTakWkVpfHg2piufDtRFGZRJQXdrx6T"},"executionInfo":{"status":"error","timestamp":1593802146820,"user_tz":-480,"elapsed":116655,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"2671c59f-3522-4afb-8d9f-59cfcf90a9d7"},"source":["from PIL import ImageFont, ImageDraw\n","\n","# Double check if this is still valid\n","cat_list = ['tops', 'trousers', 'outerwear', 'dresses', 'skirts']\n","rank_colors = ['cyan', 'magenta', 'DarkOrange', 'DimGray', 'DarkTurquoise']\n","\n","for image_path in TEST_IMAGE_PATHS[:20]:\n","    image = Image.open(image_path)\n","    # the array based representation of the image will be used later in order to prepare the\n","    # result image with boxes and labels on it.\n","    try:\n","      if np.array(image).shape[2] == 4:\n","        png = image.convert('RGBA')\n","        background = Image.new('RGBA', png.size, (255,255,255))\n","        image = Image.alpha_composite(background, png).convert('RGB')\n","      image_np = load_image_into_numpy_array(image)\n","    except ValueError:\n","      print(\"image of shape:\", np.array(image).shape)\n","      continue\n","\n","    # Actual detection.\n","    output_dict = run_inference_for_single_image(image_np, detection_graph)\n","    \n","    score_list = output_dict['detection_scores']\n","    cat_id_list = output_dict['detection_classes']\n","    bbox_list = output_dict['detection_boxes']\n","\n","    for i_in, i_val in enumerate(score_list>0.85):\n","      if i_val:\n","        score = score_list[i_in]\n","        cat_id = cat_id_list[i_in]\n","        y0,x0,y1,x1 = bbox_list[i_in]\n","        W,H = image.size\n","        wh = W*H\n","\n","        x0 = int(W*x0)\n","        y0 = int(H*y0)\n","        x1 = int(W*x1)\n","        y1 = int(H*y1)\n","        text = cat_list[cat_id-1]\n","        score = str(round(score, 5))\n","\n","        if wh>2000000:\n","          fn_size = wh//80000\n","          rec_width = wh//500000\n","        else:\n","          fn_size = wh//30000\n","          rec_width = wh//80000\n","        print(W, H)\n","        print(fn_size, rec_width)\n","\n","        font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', int(fn_size))\n","        draw = ImageDraw.Draw(image)\n","        draw.rectangle([x0, y0, x1, y1], outline = rank_colors[cat_id-1], width=rec_width)\n","        draw.text([x0, y0], text, fill = (255,255,255), font=font)\n","        draw.text([x0, y0-100], score, fill = (255,255,255), font=font)\n"," \n","    # display(image.resize((W//5,H//5)))\n","    display(image)\n","\n","# Always confused with skirts/trousers-top pair and dresses"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"FqMIrjBZuQyb","colab_type":"text"},"source":["##Evaluation"]},{"cell_type":"code","metadata":{"id":"1FaRuFME-8qH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1593801662989,"user_tz":-480,"elapsed":55145,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"0d10eda9-5003-4524-d86c-d5052a1e8eaa"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zdvs6l0Hx97S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":877},"executionInfo":{"status":"ok","timestamp":1593801743855,"user_tz":-480,"elapsed":85604,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"7c9403cc-45e2-4fcd-86f1-1b846d45ba08"},"source":["!pip install tensorflow_gpu==1.15"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow_gpu==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n","\u001b[K     |████████████████████████████████| 411.5MB 38kB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.34.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.18.5)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.12.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.12.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.8.1)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 21.7MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.9.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.1.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (3.10.0)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 36.5MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.1.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.2.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.30.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (3.2.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow_gpu==1.15) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow_gpu==1.15) (47.3.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.2.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (1.0.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (1.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.1.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=cf141f7ce49dc98ad9f7a45ff9649db4a2547829ff5472971941915a15ca6828\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorflow-estimator<2.3.0,>=2.2.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow-gpu\n","  Found existing installation: tensorflow-estimator 2.2.0\n","    Uninstalling tensorflow-estimator-2.2.0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0\n","  Found existing installation: tensorboard 2.2.2\n","    Uninstalling tensorboard-2.2.2:\n","      Successfully uninstalled tensorboard-2.2.2\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P9vYXRrBzngk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":860},"executionInfo":{"status":"ok","timestamp":1593807743447,"user_tz":-480,"elapsed":50025,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"e002e5c1-a02e-4839-a18d-89bc6c00ab06"},"source":["%cd /content\n","!git clone --quiet https://github.com/tensorflow/models.git\n","\n","!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n","\n","!pip install -q Cython contextlib2 pillow lxml matplotlib\n","\n","!pip install -q pycocotools\n","\n","%cd /content/models/research\n","!protoc object_detection/protos/*.proto --python_out=.\n","\n","import os\n","os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n","\n","!pip install tf_slim\n","\n","!python object_detection/builders/model_builder_test.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","Selecting previously unselected package python-bs4.\n","(Reading database ... 144379 files and directories currently installed.)\n","Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n","Unpacking python-bs4 (4.6.0-1) ...\n","Selecting previously unselected package python-pkg-resources.\n","Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n","Unpacking python-pkg-resources (39.0.1-2) ...\n","Selecting previously unselected package python-chardet.\n","Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n","Unpacking python-chardet (3.0.4-1) ...\n","Selecting previously unselected package python-six.\n","Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n","Unpacking python-six (1.11.0-2) ...\n","Selecting previously unselected package python-webencodings.\n","Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n","Unpacking python-webencodings (0.5-2) ...\n","Selecting previously unselected package python-html5lib.\n","Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n","Unpacking python-html5lib (0.999999999-1) ...\n","Selecting previously unselected package python-lxml:amd64.\n","Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n","Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n","Selecting previously unselected package python-olefile.\n","Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n","Unpacking python-olefile (0.45.1-1) ...\n","Selecting previously unselected package python-pil:amd64.\n","Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.2_amd64.deb ...\n","Unpacking python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n","Setting up python-pkg-resources (39.0.1-2) ...\n","Setting up python-six (1.11.0-2) ...\n","Setting up python-bs4 (4.6.0-1) ...\n","Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n","Setting up python-olefile (0.45.1-1) ...\n","Setting up python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n","Setting up python-webencodings (0.5-2) ...\n","Setting up python-chardet (3.0.4-1) ...\n","Setting up python-html5lib (0.999999999-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","/content/models/research\n","object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n","Collecting tf_slim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n","\u001b[K     |████████████████████████████████| 358kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.9.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.12.0)\n","Installing collected packages: tf-slim\n","Successfully installed tf-slim-1.1.0\n","2020-07-03 20:22:19.470040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ck4th2qrwB6B","colab_type":"code","colab":{}},"source":["import os\n","import PIL\n","import json\n","import numpy as np\n","import tensorflow as tf\n","from PIL import Image\n","from tqdm import tqdm\n","from multiprocessing import Pool\n","from object_detection.utils import label_map_util\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.python.keras.utils.data_utils import Sequence\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmLNjdfisQDs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1593807745067,"user_tz":-480,"elapsed":47786,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"05d0366f-f7d7-4bdb-dc22-b91918cd8869"},"source":["# PATH_TO_CKPT = '/content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/roboflow_faster_rcnn_inception_v2_12595.pb'\n","# PATH_TO_LABELS = '/content/drive/My Drive/Colab Notebooks/TIL/CV/input/tfrecord/tfrecord_label_map.pbtxt'\n","# _eval_folder = \"interim_1\"\n","# base_folder = \"/content/drive/My Drive/Colab Notebooks/TIL/CV\"\n","# eval_folder = os.path.join(base_folder, _eval_folder)\n","# model_name = \"roboflow\"\n","# num_classes = 5\n","\n","# # Load dataset path\n","# eval_annotations = os.path.join(eval_folder, \"CV_interim_evaluation.json\")\n","# eval_imgs_folder = os.path.join(eval_folder, \"CV_interim_images\")\n","\n","# # Submission file name\n","# submit_annotations = os.path.join( base_folder, _eval_folder, \"interim_2\", \"Jun_Kai_\" + model_name + \"_12595_submission.json\")\n","\n","# print(\"{:<20}{}\".format(\"eval_folder:\", eval_folder))\n","# print(\"{:<20}{}\".format(\"model_name:\", model_name))\n","# print(\"{:<20}{}\".format(\"eval_annotations:\", eval_annotations))\n","# print(\"{:<20}{}\".format(\"eval_imgs_folder:\", eval_imgs_folder))\n","# print(\"{:<20}{}\".format(\"PATH_TO_CKPT:\", PATH_TO_CKPT))\n","# print(\"{:<20}{}\".format(\"submit_annotations:\", submit_annotations))\n","\n","\n","PATH_TO_CKPT = '/content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/improved_frcnn_inception_v2_36000.pb'\n","PATH_TO_LABELS = '/content/drive/My Drive/Colab Notebooks/TIL/CV/input/tfrecord/tfrecord_label_map.pbtxt'\n","_eval_folder = \"input\"\n","base_folder = \"/content/drive/My Drive/Colab Notebooks/TIL/CV\"\n","eval_folder = os.path.join(base_folder, _eval_folder)\n","model_name = \"frcnn\"\n","num_classes = 5\n","\n","# Load dataset path\n","eval_annotations = os.path.join(eval_folder, \"val.json\")\n","eval_imgs_folder = os.path.join(eval_folder, \"val\", \"val\")\n","\n","# Submission file name\n","submit_annotations = os.path.join( base_folder, \"junkai\", \"Jun_Kai_\" + _eval_folder + \"_\" + model_name + \"_36000.json\")\n","\n","print(\"{:<20}{}\".format(\"eval_folder:\", eval_folder))\n","print(\"{:<20}{}\".format(\"model_name:\", model_name))\n","print(\"{:<20}{}\".format(\"eval_annotations:\", eval_annotations))\n","print(\"{:<20}{}\".format(\"eval_imgs_folder:\", eval_imgs_folder))\n","print(\"{:<20}{}\".format(\"PATH_TO_CKPT:\", PATH_TO_CKPT))\n","print(\"{:<20}{}\".format(\"submit_annotations:\", submit_annotations))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["eval_folder:        /content/drive/My Drive/Colab Notebooks/TIL/CV/input\n","model_name:         frcnn\n","eval_annotations:   /content/drive/My Drive/Colab Notebooks/TIL/CV/input/val.json\n","eval_imgs_folder:   /content/drive/My Drive/Colab Notebooks/TIL/CV/input/val/val\n","PATH_TO_CKPT:       /content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/improved_frcnn_inception_v2_36000.pb\n","submit_annotations: /content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/Jun_Kai_input_frcnn_36000.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hmdTi6Z0sPz1","colab_type":"code","colab":{}},"source":["detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","    od_graph_def = tf.GraphDef()\n","    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n","        serialized_graph = fid.read()\n","        od_graph_def.ParseFromString(serialized_graph)\n","        tf.import_graph_def(od_graph_def, name='')\n","\n","label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n","categories = label_map_util.convert_label_map_to_categories(\n","    label_map, max_num_classes=num_classes, use_display_name=True)\n","category_index = label_map_util.create_category_index(categories)\n","\n","def load_image_into_numpy_array(image):\n","    (im_width, im_height) = image.size\n","    return np.array(image.getdata()).reshape(\n","        (im_height, im_width, 3)).astype(np.uint8)\n","\n","def run_inference_for_single_image(image, graph):\n","    with graph.as_default():\n","        with tf.Session() as sess:\n","            # Get handles to input and output tensors\n","            ops = tf.get_default_graph().get_operations()\n","            all_tensor_names = {\n","                output.name for op in ops for output in op.outputs}\n","            tensor_dict = {}\n","            for key in [\n","                'num_detections', 'detection_boxes', 'detection_scores',\n","                'detection_classes', 'detection_masks'\n","            ]:\n","                tensor_name = key + ':0'\n","                if tensor_name in all_tensor_names:\n","                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n","                        tensor_name)\n","            if 'detection_masks' in tensor_dict:\n","                # The following processing is only for single image\n","                detection_boxes = tf.squeeze(\n","                    tensor_dict['detection_boxes'], [0])\n","                detection_masks = tf.squeeze(\n","                    tensor_dict['detection_masks'], [0])\n","                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n","                real_num_detection = tf.cast(\n","                    tensor_dict['num_detections'][0], tf.int32)\n","                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n","                                           real_num_detection, -1])\n","                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n","                                           real_num_detection, -1, -1])\n","                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n","                detection_masks_reframed = tf.cast(\n","                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n","                # Follow the convention by adding back the batch dimension\n","                tensor_dict['detection_masks'] = tf.expand_dims(\n","                    detection_masks_reframed, 0)\n","            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n","\n","            # Run inference\n","            output_dict = sess.run(tensor_dict,\n","                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n","\n","            # all outputs are float32 numpy arrays, so convert types as appropriate\n","            output_dict['num_detections'] = int(\n","                output_dict['num_detections'][0])\n","            output_dict['detection_classes'] = output_dict[\n","                'detection_classes'][0].astype(np.uint8)\n","            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n","            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n","            if 'detection_masks' in output_dict:\n","                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n","    return output_dict\n","\n","# Custom function to replace decode_tensor()\n","def unpack_preds(pred_dict):\n","  results = []\n","  det_num = pred_dict['num_detections']\n","  det_score = pred_dict['detection_scores']\n","  category_id = pred_dict['detection_classes']\n","  for i in range(det_num):\n","    predx, predy, predw, predh = pred_dict['detection_boxes'][i]\n","    results.append((det_score[i], category_id[i], predx, predy, predw, predh))\n","  return results\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WztyY_tUqihT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":403},"executionInfo":{"status":"ok","timestamp":1592944016659,"user_tz":-480,"elapsed":5630148,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"d8335da3-1eec-4c04-e34d-22c2588ab467"},"source":["# Custom sequence to run the evaluation annotation file\n","\n","print(\"Loading eval images from:     \", eval_imgs_folder)\n","imgs_dict = {im.split('.')[0]:im for im in os.listdir(eval_imgs_folder) if im.endswith('.jpg')}\n","\n","print(\"Loading eval annotations from:\", eval_annotations)\n","test_sequence = []\n","with open(eval_annotations, 'r') as f:\n","  annotations_dict = json.load(f)\n","annotations_list = annotations_dict['images']\n","for annotation in annotations_list:\n","  img_id = str(annotation['id'])\n","  if img_id in imgs_dict:\n","    img_fp = os.path.join(eval_imgs_folder, imgs_dict[img_id])\n","    test_sequence.append( (int(img_id), img_fp) )  # img_id, (w,h), input_arr\n","\n","print('Running detections:')\n","\n","# Generating detections on the folder of validation images\n","detections = []\n","det_threshold=0.\n","for i in tqdm(range(len(test_sequence))):\n","  img_id, img_fp = test_sequence[i]\n","  image_pil = PIL.Image.open(img_fp)\n","  W,H = image_pil.size\n","  image_np = np.array(image_pil)\n","\n","  try:\n","    pred_dict = run_inference_for_single_image(image_np, detection_graph)\n","  except ValueError:\n","    print(\"Image shape is not (?, ?, ?, 3): \", end='')\n","    print(img_id, image_np.shape)\n","    continue\n","\n","  # Visualise output\n","  # vis_util.visualize_boxes_and_labels_on_image_array(\n","  #     image_np,\n","  #     pred_dict['detection_boxes'],\n","  #     pred_dict['detection_classes'],\n","  #     pred_dict['detection_scores'],\n","  #     category_index,\n","  #     instance_masks=pred_dict.get('detection_masks'),\n","  #     use_normalized_coordinates=True,\n","  #     line_thickness=8)\n","  # plt.figure(figsize=(10,10))\n","  # plt.imshow(image_np)\n","\n","  preds = unpack_preds(pred_dict)\n","\n","  # Post-processing\n","  preds = [pred for pred in preds if pred[0] >= det_threshold]\n","  preds.sort( key=lambda x:x[0], reverse=True )\n","  preds = preds[:100] # we only evaluate you on 100 detections per image\n","  \n","  for i, pred in enumerate(preds):\n","    conf,cat_id,y1,x1,y2,x2 = pred  # y1, x1, y2, x2\n","    width = W*(x2-x1)\n","    height = H*(y2-y1)\n","    x1 = W*x1\n","    y1 = H*y1\n","\n","    width = round(width,1)\n","    height = round(height,1)\n","    x1 = round(x1,1)\n","    y1 = round(y1,1)\n","    conf = float(conf)\n","    cat_id = int(cat_id)\n","    detections.append( {'image_id':img_id, 'category_id':cat_id, 'bbox':[x1, y1, width, height], 'score':conf} )\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading eval images from:      /content/drive/My Drive/Colab Notebooks/TIL/CV/input/val/val\n","Loading eval annotations from: /content/drive/My Drive/Colab Notebooks/TIL/CV/input/val.json\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/1474 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Running detections:\n"],"name":"stdout"},{"output_type":"stream","text":["  4%|▍         | 61/1474 [03:51<1:13:09,  3.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 10221 (2592, 1944, 4)\n"],"name":"stdout"},{"output_type":"stream","text":["  5%|▌         | 76/1474 [04:45<1:07:02,  2.88s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 10266 (3264, 1675, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 12%|█▏        | 178/1474 [11:14<1:02:09,  2.88s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 10609 (3264, 1836)\n"],"name":"stdout"},{"output_type":"stream","text":[" 22%|██▏       | 327/1474 [20:57<59:54,  3.13s/it]  "],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 1512 (2668, 1480, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 26%|██▋       | 387/1474 [24:36<48:37,  2.68s/it]  "],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 1746 (851, 555, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 33%|███▎      | 480/1474 [30:26<45:42,  2.76s/it]  "],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 95 (1127, 805, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 41%|████      | 597/1474 [37:44<40:13,  2.75s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 785 (554, 800, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 49%|████▉     | 726/1474 [46:20<33:49,  2.71s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 343 (525, 632, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 742/1474 [47:18<34:15,  2.81s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 401 (619, 619, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 53%|█████▎    | 780/1474 [49:45<32:53,  2.84s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 8276 (2048, 1150, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 54%|█████▍    | 801/1474 [51:01<31:57,  2.85s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 6977 (4608, 2844)\n"],"name":"stdout"},{"output_type":"stream","text":[" 59%|█████▉    | 876/1474 [55:42<27:30,  2.76s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 12497 (636, 497, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 67%|██████▋   | 994/1474 [1:03:36<22:30,  2.81s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 5010 (594, 608, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 69%|██████▉   | 1014/1474 [1:04:49<21:08,  2.76s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 14737 (960, 640, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 71%|███████   | 1045/1474 [1:06:45<20:29,  2.87s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 8876 (1080, 893, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 72%|███████▏  | 1063/1474 [1:07:50<20:29,  2.99s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 7556 (2352, 1262, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 77%|███████▋  | 1134/1474 [1:12:44<15:37,  2.76s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 7306 (3264, 1669)\n"],"name":"stdout"},{"output_type":"stream","text":[" 83%|████████▎ | 1228/1474 [1:18:42<11:31,  2.81s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 17677 (1077, 566, 4)\n"],"name":"stdout"},{"output_type":"stream","text":[" 84%|████████▍ | 1235/1474 [1:19:05<10:50,  2.72s/it]"],"name":"stderr"},{"output_type":"stream","text":["Image shape is not (?, ?, ?, 3): 4875 (2048, 1366)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1474/1474 [1:34:58<00:00,  3.87s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"yZIuClmZBRyS","colab_type":"code","colab":{}},"source":["with open(submit_annotations, 'w') as f:\n","  json.dump(detections, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQBwiK8mdoDt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592944019069,"user_tz":-480,"elapsed":20,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"56fb31e1-cd2f-4927-ffde-2cb9ae72535b"},"source":["submit_annotations"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/Jun_Kai_input_frcnn_36000.json'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"VKYc8AMDGeVr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1592944034118,"user_tz":-480,"elapsed":15055,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"e34d7583-2ca0-4b02-9a95-af9fdc15b076"},"source":["!nvidia-smi\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oaxJ_yb5U4z8","colab_type":"code","colab":{}},"source":["# Auto reload after installing new modules\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HjpEKIWRU5KI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"status":"ok","timestamp":1592964074410,"user_tz":-480,"elapsed":16003,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"fb0ed48f-210b-48fa-d98c-5eac700d936d"},"source":["# First, we need to install cocoapi to evaluate our detections\n","# This installation is a modified version of the original to suit this competition\n","! pip install git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI --upgrade"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI\n","  Cloning https://github.com/jinmingteo/cocoapi.git to /tmp/pip-req-build-4wvnjk6r\n","  Running command git clone -q https://github.com/jinmingteo/cocoapi.git /tmp/pip-req-build-4wvnjk6r\n","Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (47.3.1)\n","Requirement already satisfied, skipping upgrade: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.20)\n","Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)\n","Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.5)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n","Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n","Building wheels for collected packages: pycocotools\n","  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=267047 sha256=54456ab404a78bf6c79c27df7820099f81784ff5999d4bf67801811af88a9c85\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2r0thblt/wheels/27/81/92/3a512329d1b1ae7fc278285a1f114ef08082568bf32eee0002\n","Successfully built pycocotools\n","Installing collected packages: pycocotools\n","  Found existing installation: pycocotools 2.0.1\n","    Uninstalling pycocotools-2.0.1:\n","      Successfully uninstalled pycocotools-2.0.1\n","Successfully installed pycocotools-2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zdp_vR-9U7v-","colab_type":"code","colab":{}},"source":["from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WawByIccyT0X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1592998933251,"user_tz":-480,"elapsed":895,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"224d74c6-f4e0-4243-9678-419b0a795df4"},"source":["val_annotations = \"/content/drive/My Drive/Colab Notebooks/TIL/CV/input/val.json\"\n","print(val_annotations)\n","submit_annotations = '/content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/Jun_Kai_input_frcnn_36000.json'\n","print(submit_annotations)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/TIL/CV/input/val.json\n","/content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/Jun_Kai_input_frcnn_36000.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B4ZOtz5LU-Sq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":319},"executionInfo":{"status":"ok","timestamp":1592964157388,"user_tz":-480,"elapsed":10405,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"0f870e16-d1eb-4699-f305-41ad4bba806c"},"source":["# Get evaluation score against validation set\n","coco_gt = COCO(val_annotations)\n","coco_dt = coco_gt.loadRes(submit_annotations)\n","cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n","cocoEval.evaluate()\n","cocoEval.accumulate()\n","cocoEval.summarize()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=1.98s)\n","creating index...\n","index created!\n","Loading and preparing results...\n","DONE (t=1.78s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=4.06s).\n","Accumulating evaluation results...\n","DONE (t=1.53s).\n"," Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.587\n"," Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.628\n"," Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.605\n"," Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.576\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.532\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3EvGvFVefZGz","colab_type":"text"},"source":["## Initial Run:\n","#### 12595 FIXED\n","```\n","Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.629\n","Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.643\n","Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.635\n","Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.625\n","Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.607\n","```\n","\n","#### 22182 (44.49%)\n","```\n"," Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.645\n"," Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.658\n"," Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.652\n"," Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.642\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.626\n","```\n","#### 28182 (56.94%)\n","```\n","Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.644\n","Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.657\n","Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.651\n","Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.641\n","Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.626\n","```\n","\n","\n","\n","## Improved\n","#### 20000\n","```\n","Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.565\n","Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.609\n","Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.581\n","Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.555\n","Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.508\n","```\n","\n","#### 24000 (52.06%)\n","```\n","Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.574\n","Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.616\n","Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.592\n","Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.562\n","Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.520\n","```\n","#### 36000\n","```\n","Average Precision  (AP) @[ IoU=0.20:0.50 | area=   all | maxDets=100 ] = 0.587\n","Average Precision  (AP) @[ IoU=0.20      | area=   all | maxDets=100 ] = 0.628\n","Average Precision  (AP) @[ IoU=0.30      | area=   all | maxDets=100 ] = 0.605\n","Average Precision  (AP) @[ IoU=0.40      | area=   all | maxDets=100 ] = 0.576\n","Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.532\n","```"]},{"cell_type":"code","metadata":{"id":"MjxF2glG3ITi","colab_type":"code","colab":{}},"source":["with open(\"/content/drive/My Drive/Colab Notebooks/TIL/CV/working/submission-model-7x7-14x14-3aspect-modyoloposneg-wd0.0005.json\") as f:\n","  valannos = json.load(f)\n","with open(submit_annotations) as f:\n","  subannos = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UXvoNpYh30jg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1592486959964,"user_tz":-480,"elapsed":2908,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"d83d3818-e86e-47d5-ed0a-83c80c0dae74"},"source":["valannos[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'bbox': [571.5, 194.6, 815.2, 821.1],\n"," 'category_id': 4,\n"," 'image_id': 1,\n"," 'score': 0.6992745399475098}"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"TFsWbDHA3ot8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1592486959965,"user_tz":-480,"elapsed":2440,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"cd125025-6cc9-4ea1-8fb1-342385a41361"},"source":["subannos[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'bbox': [526.3, 127.6, -1021701.6, -246942.5],\n"," 'category_id': 4,\n"," 'image_id': 1,\n"," 'score': 0.8680194616317749}"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"k-4GeYuc385m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592476388250,"user_tz":-480,"elapsed":2004,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"f38d533f-5b71-4870-e8a8-30efd305556e"},"source":["a=0\n","for i in valannos:\n","  if i['image_id'] == 1:  # 2084, 1472, 3030\n","    a+=1\n","    print (i)\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'image_id': 1, 'category_id': 4, 'bbox': [571.5, 194.6, 815.2, 821.1], 'score': 0.6992745399475098}\n","{'image_id': 1, 'category_id': 4, 'bbox': [731.3, 128.9, 530.9, 857.4], 'score': 0.11952656507492065}\n","{'image_id': 1, 'category_id': 4, 'bbox': [668.4, 226.5, 613.6, 971.3], 'score': 0.03665538132190704}\n","{'image_id': 1, 'category_id': 3, 'bbox': [879.9, 223.0, 901.2, 669.5], 'score': 0.014186333864927292}\n","{'image_id': 1, 'category_id': 4, 'bbox': [582.3, 108.9, 709.3, 917.1], 'score': 0.01228678971529007}\n","{'image_id': 1, 'category_id': 4, 'bbox': [632.7, 57.0, 704.6, 505.9], 'score': 0.011506722308695316}\n","{'image_id': 1, 'category_id': 3, 'bbox': [1292.6, 144.2, 382.8, 665.6], 'score': 0.004970135632902384}\n","{'image_id': 1, 'category_id': 4, 'bbox': [891.0, 32.2, 655.1, 426.8], 'score': 0.00437589455395937}\n","{'image_id': 1, 'category_id': 5, 'bbox': [759.9, 550.0, 495.9, 805.6], 'score': 0.0040878006257116795}\n","{'image_id': 1, 'category_id': 2, 'bbox': [769.7, 842.0, 491.4, 654.4], 'score': 0.003458512481302023}\n","{'image_id': 1, 'category_id': 4, 'bbox': [240.1, 116.0, 999.3, 1029.8], 'score': 0.0032126344740390778}\n","{'image_id': 1, 'category_id': 4, 'bbox': [1015.3, 108.2, 408.5, 731.6], 'score': 0.0021131697576493025}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1287.0, 529.4, 460.0, 782.3], 'score': 0.0020606054458767176}\n","{'image_id': 1, 'category_id': 4, 'bbox': [810.7, 222.9, 729.1, 598.3], 'score': 0.001816713367588818}\n","{'image_id': 1, 'category_id': 4, 'bbox': [469.0, 202.3, 924.5, 737.0], 'score': 0.0018118319567292929}\n","{'image_id': 1, 'category_id': 4, 'bbox': [506.5, 221.6, 870.6, 1004.6], 'score': 0.001490538357757032}\n","{'image_id': 1, 'category_id': 2, 'bbox': [632.2, 1003.7, 638.9, 470.0], 'score': 0.0012443060986697674}\n","{'image_id': 1, 'category_id': 5, 'bbox': [688.0, 771.1, 640.4, 467.9], 'score': 0.001170646632090211}\n","{'image_id': 1, 'category_id': 4, 'bbox': [861.2, 315.5, 210.0, 208.8], 'score': 0.0010894681327044964}\n","{'image_id': 1, 'category_id': 4, 'bbox': [538.0, 480.3, 711.3, 898.4], 'score': 0.0010651021730154753}\n","{'image_id': 1, 'category_id': 4, 'bbox': [899.0, 103.4, 120.2, 114.6], 'score': 0.0008967900648713112}\n","{'image_id': 1, 'category_id': 4, 'bbox': [777.8, -19.0, 384.1, 593.5], 'score': 0.000890600320417434}\n","{'image_id': 1, 'category_id': 4, 'bbox': [275.5, 190.8, 926.5, 1007.7], 'score': 0.0008433505427092314}\n","{'image_id': 1, 'category_id': 4, 'bbox': [1187.5, 114.4, 104.0, 99.2], 'score': 0.0008013100596144795}\n","{'image_id': 1, 'category_id': 5, 'bbox': [579.2, 395.8, 759.2, 638.6], 'score': 0.0007664082804694772}\n","{'image_id': 1, 'category_id': 3, 'bbox': [1021.7, 15.5, 752.6, 501.5], 'score': 0.0007094203610904515}\n","{'image_id': 1, 'category_id': 3, 'bbox': [454.4, 65.5, 720.9, 489.3], 'score': 0.0006809812039136887}\n","{'image_id': 1, 'category_id': 4, 'bbox': [1450.4, 369.3, 138.3, 136.3], 'score': 0.0006402127328328788}\n","{'image_id': 1, 'category_id': 4, 'bbox': [1058.8, -56.1, 341.7, 562.3], 'score': 0.0005093298968859017}\n","{'image_id': 1, 'category_id': 4, 'bbox': [638.3, 123.7, 87.0, 83.1], 'score': 0.0004210661572869867}\n","{'image_id': 1, 'category_id': 4, 'bbox': [1182.5, 374.5, 115.6, 112.9], 'score': 0.0004144138947594911}\n","{'image_id': 1, 'category_id': 3, 'bbox': [1254.6, 293.9, 480.6, 760.1], 'score': 0.0003284916456323117}\n","{'image_id': 1, 'category_id': 4, 'bbox': [368.1, 137.2, 63.5, 60.5], 'score': 0.00026680019800551236}\n","{'image_id': 1, 'category_id': 5, 'bbox': [830.6, 441.8, 987.3, 680.2], 'score': 0.00026448050630278885}\n","{'image_id': 1, 'category_id': 2, 'bbox': [531.8, 745.3, 658.9, 781.1], 'score': 0.00026244187029078603}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1660.2, 903.4, 163.6, 286.0], 'score': 0.0002609154034871608}\n","{'image_id': 1, 'category_id': 4, 'bbox': [1336.7, -31.6, 336.5, 507.3], 'score': 0.0002606028283480555}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1521.1, 903.4, 164.2, 286.3], 'score': 0.00022775609977543354}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1243.1, 764.6, 164.9, 286.9], 'score': 0.00019966783293057233}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1598.9, 963.2, 273.1, 158.0], 'score': 0.00019666890148073435}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1799.0, 903.4, 164.0, 285.7], 'score': 0.00019273739599157125}\n","{'image_id': 1, 'category_id': 4, 'bbox': [619.3, 355.9, 128.6, 129.6], 'score': 0.0001759090955602005}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1103.5, 1598.1, 167.2, 286.8], 'score': 0.00017224242037627846}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1460.0, 963.0, 273.0, 158.4], 'score': 0.00017050348105840385}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1242.4, 1598.1, 167.2, 286.8], 'score': 0.00016980187501758337}\n","{'image_id': 1, 'category_id': 4, 'bbox': [243.5, 417.9, 811.9, 991.8], 'score': 0.00016880713519640267}\n","{'image_id': 1, 'category_id': 2, 'bbox': [244.7, 716.4, 794.1, 1018.5], 'score': 0.00016819559095893055}\n","{'image_id': 1, 'category_id': 4, 'bbox': [35.5, 41.5, 65.8, 66.7], 'score': 0.00016760204744059592}\n","{'image_id': 1, 'category_id': 2, 'bbox': [964.7, 1598.1, 167.2, 286.8], 'score': 0.00016716666868887842}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1381.7, 903.6, 166.0, 286.5], 'score': 0.0001607362210052088}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1381.3, 1598.1, 167.2, 286.8], 'score': 0.00016047230747062713}\n","{'image_id': 1, 'category_id': 4, 'bbox': [35.8, 319.6, 64.7, 66.0], 'score': 0.0001603881682967767}\n","{'image_id': 1, 'category_id': 2, 'bbox': [965.0, 764.8, 166.5, 286.4], 'score': 0.00015646766405552626}\n","{'image_id': 1, 'category_id': 2, 'bbox': [825.8, 1598.1, 167.2, 286.7], 'score': 0.0001556693750899285}\n","{'image_id': 1, 'category_id': 2, 'bbox': [826.1, 764.8, 166.5, 286.4], 'score': 0.00015527753566857427}\n","{'image_id': 1, 'category_id': 2, 'bbox': [687.3, 764.8, 166.5, 286.4], 'score': 0.00015431977226398885}\n","{'image_id': 1, 'category_id': 2, 'bbox': [964.6, 1042.6, 167.3, 286.8], 'score': 0.000153386004967615}\n","{'image_id': 1, 'category_id': 2, 'bbox': [548.4, 764.8, 166.5, 286.4], 'score': 0.0001530010486021638}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1520.2, 1042.6, 167.0, 286.6], 'score': 0.00015281846572179347}\n","{'image_id': 1, 'category_id': 2, 'bbox': [409.5, 764.8, 166.5, 286.4], 'score': 0.00015250772412400693}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1103.5, 1042.6, 167.3, 286.8], 'score': 0.0001524738036096096}\n","{'image_id': 1, 'category_id': 2, 'bbox': [524.6, 915.8, 579.7, 841.3], 'score': 0.0001521778031019494}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1381.3, 1042.6, 167.2, 286.7], 'score': 0.00015148744569160044}\n","{'image_id': 1, 'category_id': 2, 'bbox': [270.7, 764.8, 166.4, 286.4], 'score': 0.00015143952623475343}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1242.4, 1042.6, 167.3, 286.8], 'score': 0.00015142624033614993}\n","{'image_id': 1, 'category_id': 2, 'bbox': [270.4, 1598.1, 167.2, 286.7], 'score': 0.00015136963338591158}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1103.9, 764.8, 166.3, 286.5], 'score': 0.00015120745229069144}\n","{'image_id': 1, 'category_id': 2, 'bbox': [131.8, 764.8, 166.5, 286.4], 'score': 0.00015045572945382446}\n","{'image_id': 1, 'category_id': 2, 'bbox': [825.8, 1042.6, 167.3, 286.7], 'score': 0.00015044822066556662}\n","{'image_id': 1, 'category_id': 2, 'bbox': [964.6, 1181.5, 167.3, 286.8], 'score': 0.00015039338904898614}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1103.5, 1181.5, 167.4, 286.8], 'score': 0.0001498143101343885}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1242.3, 1181.5, 167.4, 286.8], 'score': 0.00014959955296944827}\n","{'image_id': 1, 'category_id': 2, 'bbox': [825.8, 1181.5, 167.3, 286.7], 'score': 0.00014893156185280532}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1520.1, 1598.1, 167.2, 286.7], 'score': 0.0001487113331677392}\n","{'image_id': 1, 'category_id': 2, 'bbox': [687.0, 1598.1, 167.2, 286.7], 'score': 0.00014818472845945507}\n","{'image_id': 1, 'category_id': 2, 'bbox': [409.3, 1598.1, 167.2, 286.7], 'score': 0.00014809114509262145}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1381.2, 1181.5, 167.4, 286.8], 'score': 0.0001479686179663986}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1737.6, 963.2, 273.4, 158.0], 'score': 0.00014781294157728553}\n","{'image_id': 1, 'category_id': 2, 'bbox': [964.6, 1320.4, 167.3, 286.7], 'score': 0.00014770419511478394}\n","{'image_id': 1, 'category_id': 1, 'bbox': [-2.8, 208.9, 152.2, 289.4], 'score': 0.00014769834524486214}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1659.1, 1042.6, 166.9, 286.6], 'score': 0.00014763472427148372}\n","{'image_id': 1, 'category_id': 2, 'bbox': [825.8, 1320.4, 167.3, 286.7], 'score': 0.0001475871540606022}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1182.3, 823.9, 273.2, 159.0], 'score': 0.00014731015835423023}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1103.6, 903.8, 167.1, 286.6], 'score': 0.00014727568486705422}\n","{'image_id': 1, 'category_id': 2, 'bbox': [687.0, 1042.6, 167.2, 286.7], 'score': 0.00014717818703502417}\n","{'image_id': 1, 'category_id': 2, 'bbox': [686.9, 1320.4, 167.3, 286.7], 'score': 0.00014713435666635633}\n","{'image_id': 1, 'category_id': 2, 'bbox': [686.9, 1181.5, 167.3, 286.7], 'score': 0.00014710283721797168}\n","{'image_id': 1, 'category_id': 2, 'bbox': [964.7, 903.8, 167.2, 286.7], 'score': 0.00014698866289108992}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1242.4, 1320.4, 167.3, 286.7], 'score': 0.00014698351151309907}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1103.5, 1320.4, 167.3, 286.7], 'score': 0.0001468090631533414}\n","{'image_id': 1, 'category_id': 2, 'bbox': [548.1, 1598.1, 167.2, 286.7], 'score': 0.000146365724503994}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1381.2, 1320.4, 167.3, 286.7], 'score': 0.0001463067310396582}\n","{'image_id': 1, 'category_id': 2, 'bbox': [131.6, 1598.1, 167.1, 286.6], 'score': 0.00014628194912802428}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1520.1, 1181.5, 167.3, 286.7], 'score': 0.00014627737982664257}\n","{'image_id': 1, 'category_id': 1, 'bbox': [-2.6, -68.5, 152.0, 287.9], 'score': 0.00014600130089093}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1242.5, 903.7, 166.8, 286.5], 'score': 0.00014591745275538415}\n","{'image_id': 1, 'category_id': 1, 'bbox': [-75.3, 276.3, 288.1, 153.5], 'score': 0.00014557968825101852}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1520.1, 1320.4, 167.3, 286.7], 'score': 0.00014535333320964128}\n","{'image_id': 1, 'category_id': 1, 'bbox': [-75.0, -1.6, 287.0, 153.0], 'score': 0.00014516316878143698}\n","{'image_id': 1, 'category_id': 2, 'bbox': [548.1, 1320.4, 167.3, 286.7], 'score': 0.0001446560345357284}\n","100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QYu1eJ3l5bDH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592476388252,"user_tz":-480,"elapsed":1157,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"c0a35a20-f484-4b38-f0cb-646cb07feae9"},"source":["a=0\n","for i in subannos:\n","  if i['image_id'] == 1:  # 2037, 4601, 3921\n","    a+=1\n","    print (i)\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'image_id': 1, 'category_id': 4, 'area': 893729.7, 'bbox': [935.0, 659.4, 819.6, 1090.4], 'score': 0.8415889739990234}\n","{'image_id': 1, 'category_id': 3, 'area': 570104.6, 'bbox': [959.2, 402.6, 929.4, 613.4], 'score': 0.6006994247436523}\n","{'image_id': 1, 'category_id': 2, 'area': 113575.7, 'bbox': [1011.7, 1485.6, 190.4, 596.5], 'score': 0.13425743579864502}\n","{'image_id': 1, 'category_id': 3, 'area': 944046.1, 'bbox': [967.3, 584.3, 1019.4, 926.1], 'score': 0.12345073372125626}\n","{'image_id': 1, 'category_id': 2, 'area': 194867.6, 'bbox': [901.0, 1454.5, 387.8, 502.6], 'score': 0.11707131564617157}\n","{'image_id': 1, 'category_id': 5, 'area': 667710.1, 'bbox': [906.4, 781.2, 759.0, 879.7], 'score': 0.07988986372947693}\n","{'image_id': 1, 'category_id': 2, 'area': 346890.4, 'bbox': [917.8, 1463.1, 549.9, 630.8], 'score': 0.06630423665046692}\n","{'image_id': 1, 'category_id': 2, 'area': 132761.8, 'bbox': [826.1, 1482.7, 220.0, 603.4], 'score': 0.05301739275455475}\n","{'image_id': 1, 'category_id': 1, 'area': 432656.9, 'bbox': [983.5, 313.1, 903.7, 478.8], 'score': 0.03916747868061066}\n","{'image_id': 1, 'category_id': 1, 'area': 739060.4, 'bbox': [921.1, 457.0, 990.0, 746.5], 'score': 0.03728179633617401}\n","{'image_id': 1, 'category_id': 3, 'area': 325459.8, 'bbox': [966.3, 315.6, 726.5, 448.0], 'score': 0.031403981149196625}\n","{'image_id': 1, 'category_id': 5, 'area': 1166127.0, 'bbox': [992.7, 809.3, 1013.5, 1150.5], 'score': 0.026136226952075958}\n","{'image_id': 1, 'category_id': 3, 'area': 673538.9, 'bbox': [822.0, 644.5, 661.6, 1018.0], 'score': 0.018608085811138153}\n","{'image_id': 1, 'category_id': 2, 'area': 73202.2, 'bbox': [821.5, 1475.3, 140.5, 521.1], 'score': 0.013777441345155239}\n","{'image_id': 1, 'category_id': 1, 'area': 250654.0, 'bbox': [953.6, 290.0, 646.1, 387.9], 'score': 0.013378710485994816}\n","{'image_id': 1, 'category_id': 2, 'area': 1022226.1, 'bbox': [916.2, 731.3, 913.3, 1119.3], 'score': 0.01241259090602398}\n","{'image_id': 1, 'category_id': 2, 'area': 251845.7, 'bbox': [988.7, 1466.8, 353.1, 713.1], 'score': 0.011168341152369976}\n","{'image_id': 1, 'category_id': 2, 'area': 555896.6, 'bbox': [933.7, 844.4, 754.3, 737.0], 'score': 0.011137024499475956}\n","{'image_id': 1, 'category_id': 2, 'area': 49380.1, 'bbox': [1017.2, 1483.7, 119.9, 411.9], 'score': 0.010573280975222588}\n","{'image_id': 1, 'category_id': 1, 'area': 969069.8, 'bbox': [912.7, 690.0, 914.5, 1059.7], 'score': 0.009601874276995659}\n","{'image_id': 1, 'category_id': 3, 'area': 231278.7, 'bbox': [913.7, 204.0, 990.5, 233.5], 'score': 0.009305201470851898}\n","{'image_id': 1, 'category_id': 2, 'area': 930816.7, 'bbox': [929.3, 1152.8, 849.5, 1095.7], 'score': 0.008759144693613052}\n","{'image_id': 1, 'category_id': 4, 'area': 648004.3, 'bbox': [730.6, 717.9, 568.7, 1139.4], 'score': 0.00764768011868}\n","{'image_id': 1, 'category_id': 2, 'area': 748224.4, 'bbox': [911.9, 1418.1, 792.9, 943.7], 'score': 0.007374855689704418}\n","{'image_id': 1, 'category_id': 1, 'area': 242758.7, 'bbox': [937.7, 182.2, 966.0, 251.3], 'score': 0.006090442184358835}\n","{'image_id': 1, 'category_id': 4, 'area': 1134672.9, 'bbox': [960.0, 518.7, 1223.5, 927.4], 'score': 0.0051016476936638355}\n","{'image_id': 1, 'category_id': 4, 'area': 1411294.8, 'bbox': [1078.8, 792.6, 1127.8, 1251.4], 'score': 0.004801481030881405}\n","{'image_id': 1, 'category_id': 5, 'area': 989579.7, 'bbox': [870.1, 640.2, 1084.5, 912.5], 'score': 0.004768789280205965}\n","{'image_id': 1, 'category_id': 5, 'area': 571259.6, 'bbox': [1042.1, 845.0, 651.4, 876.9], 'score': 0.004088353831321001}\n","{'image_id': 1, 'category_id': 4, 'area': 636100.4, 'bbox': [1042.5, 821.2, 649.0, 980.2], 'score': 0.004076266195625067}\n","{'image_id': 1, 'category_id': 4, 'area': 597877.5, 'bbox': [929.6, 903.2, 785.5, 761.2], 'score': 0.004031406249850988}\n","{'image_id': 1, 'category_id': 2, 'area': 607816.3, 'bbox': [1047.4, 749.1, 658.7, 922.8], 'score': 0.0038817364256829023}\n","{'image_id': 1, 'category_id': 2, 'area': 125190.3, 'bbox': [1165.9, 821.5, 208.3, 601.0], 'score': 0.0036678528413176537}\n","{'image_id': 1, 'category_id': 2, 'area': 51804.4, 'bbox': [988.8, 1459.8, 109.6, 472.5], 'score': 0.0036223391070961952}\n","{'image_id': 1, 'category_id': 3, 'area': 190472.9, 'bbox': [871.8, 213.5, 669.5, 284.5], 'score': 0.003247144166380167}\n","{'image_id': 1, 'category_id': 2, 'area': 920300.4, 'bbox': [1092.7, 597.4, 849.3, 1083.6], 'score': 0.003244285238906741}\n","{'image_id': 1, 'category_id': 2, 'area': 401360.8, 'bbox': [911.5, 1001.7, 726.6, 552.4], 'score': 0.0029876597691327333}\n","{'image_id': 1, 'category_id': 2, 'area': 1312105.8, 'bbox': [1115.1, 972.5, 888.2, 1477.2], 'score': 0.0025606669951230288}\n","{'image_id': 1, 'category_id': 2, 'area': 33487.2, 'bbox': [853.3, 1508.0, 95.8, 349.5], 'score': 0.0022496546152979136}\n","{'image_id': 1, 'category_id': 2, 'area': 90277.0, 'bbox': [634.0, 798.5, 161.1, 560.2], 'score': 0.0021035505924373865}\n","{'image_id': 1, 'category_id': 3, 'area': 1397103.3, 'bbox': [865.8, 688.0, 1015.3, 1376.1], 'score': 0.0019961914513260126}\n","{'image_id': 1, 'category_id': 3, 'area': 715484.6, 'bbox': [974.1, 717.2, 679.3, 1053.3], 'score': 0.001927111530676484}\n","{'image_id': 1, 'category_id': 2, 'area': 185304.1, 'bbox': [898.0, 1035.0, 513.7, 360.7], 'score': 0.0018398963147774339}\n","{'image_id': 1, 'category_id': 2, 'area': 441541.4, 'bbox': [775.4, 1463.5, 549.9, 802.9], 'score': 0.001618573209270835}\n","{'image_id': 1, 'category_id': 2, 'area': 452762.2, 'bbox': [1056.3, 928.4, 551.8, 820.5], 'score': 0.0014932409394532442}\n","{'image_id': 1, 'category_id': 2, 'area': 1264135.5, 'bbox': [952.6, 1019.2, 1195.6, 1057.3], 'score': 0.00149213382974267}\n","{'image_id': 1, 'category_id': 2, 'area': 99887.6, 'bbox': [922.8, 1186.4, 469.0, 213.0], 'score': 0.0013939616037532687}\n","{'image_id': 1, 'category_id': 1, 'area': 605592.6, 'bbox': [890.7, 736.0, 589.7, 1026.9], 'score': 0.0012995187425985932}\n","{'image_id': 1, 'category_id': 4, 'area': 1760644.6, 'bbox': [880.3, 893.6, 1087.7, 1618.7], 'score': 0.001260266755707562}\n","{'image_id': 1, 'category_id': 2, 'area': 1165938.5, 'bbox': [1221.1, 1198.9, 936.3, 1245.2], 'score': 0.0012491339584812522}\n","{'image_id': 1, 'category_id': 1, 'area': 161513.0, 'bbox': [916.0, 194.6, 585.7, 275.8], 'score': 0.0011244119377806783}\n","{'image_id': 1, 'category_id': 2, 'area': 386195.3, 'bbox': [929.7, 1630.5, 739.4, 522.3], 'score': 0.0011226693168282509}\n","{'image_id': 1, 'category_id': 5, 'area': 306816.6, 'bbox': [939.5, 955.5, 595.9, 514.9], 'score': 0.0010555454064160585}\n","{'image_id': 1, 'category_id': 2, 'area': 545825.6, 'bbox': [740.9, 731.1, 544.7, 1002.1], 'score': 0.0008786162943579257}\n","{'image_id': 1, 'category_id': 2, 'area': 977969.4, 'bbox': [977.1, 552.0, 1224.2, 798.9], 'score': 0.0008238124428316951}\n","{'image_id': 1, 'category_id': 4, 'area': 360293.2, 'bbox': [929.3, 852.9, 512.2, 703.5], 'score': 0.0007768217474222183}\n","{'image_id': 1, 'category_id': 2, 'area': 121308.8, 'bbox': [917.6, 1245.8, 452.3, 268.2], 'score': 0.0007354053668677807}\n","{'image_id': 1, 'category_id': 2, 'area': 1327400.9, 'bbox': [998.9, 1385.5, 1199.5, 1106.6], 'score': 0.0007301527075469494}\n","{'image_id': 1, 'category_id': 4, 'area': 590886.1, 'bbox': [933.7, 413.6, 853.7, 692.2], 'score': 0.0006658241618424654}\n","{'image_id': 1, 'category_id': 4, 'area': 1081310.3, 'bbox': [716.1, 793.6, 922.9, 1171.6], 'score': 0.000660413526929915}\n","{'image_id': 1, 'category_id': 5, 'area': 582856.3, 'bbox': [755.1, 763.0, 496.6, 1173.8], 'score': 0.00062222417909652}\n","{'image_id': 1, 'category_id': 5, 'area': 921401.6, 'bbox': [870.7, 483.1, 1339.9, 687.7], 'score': 0.0005776517791673541}\n","{'image_id': 1, 'category_id': 5, 'area': 848618.0, 'bbox': [936.9, 1180.4, 903.1, 939.7], 'score': 0.0005329478299245238}\n","{'image_id': 1, 'category_id': 2, 'area': 117563.4, 'bbox': [939.3, 1730.0, 478.5, 245.7], 'score': 0.0005323695368133485}\n","{'image_id': 1, 'category_id': 5, 'area': 200460.0, 'bbox': [883.4, 1496.9, 432.1, 463.9], 'score': 0.0005002914112992585}\n","{'image_id': 1, 'category_id': 2, 'area': 24204.5, 'bbox': [630.9, 697.7, 73.3, 330.1], 'score': 0.0004419492615852505}\n","{'image_id': 1, 'category_id': 1, 'area': 513521.6, 'bbox': [740.5, 675.3, 460.0, 1116.3], 'score': 0.0004356785211712122}\n","{'image_id': 1, 'category_id': 4, 'area': 129901.6, 'bbox': [1008.2, 1520.3, 235.8, 550.9], 'score': 0.00043183215893805027}\n","{'image_id': 1, 'category_id': 2, 'area': 817488.3, 'bbox': [1056.8, 399.2, 1261.0, 648.3], 'score': 0.0004100537043996155}\n","{'image_id': 1, 'category_id': 2, 'area': 788339.5, 'bbox': [1220.8, 1533.6, 964.3, 817.6], 'score': 0.0004075947217643261}\n","{'image_id': 1, 'category_id': 4, 'area': 1012048.3, 'bbox': [916.9, 1165.3, 896.2, 1129.2], 'score': 0.0004047358816023916}\n","{'image_id': 1, 'category_id': 5, 'area': 1542516.7, 'bbox': [1204.5, 999.5, 1169.1, 1319.5], 'score': 0.000401025841711089}\n","{'image_id': 1, 'category_id': 1, 'area': 578882.4, 'bbox': [1008.6, 755.1, 679.1, 852.4], 'score': 0.00036416901275515556}\n","{'image_id': 1, 'category_id': 5, 'area': 1007219.3, 'bbox': [1134.9, 562.3, 1100.7, 915.0], 'score': 0.00033876465749926865}\n","{'image_id': 1, 'category_id': 2, 'area': 358815.5, 'bbox': [677.0, 874.1, 393.2, 912.5], 'score': 0.0003353961219545454}\n","{'image_id': 1, 'category_id': 4, 'area': 266181.9, 'bbox': [948.1, 324.3, 602.4, 441.9], 'score': 0.0003286750870756805}\n","{'image_id': 1, 'category_id': 4, 'area': 122017.4, 'bbox': [627.2, 803.2, 178.1, 685.2], 'score': 0.0003251392627134919}\n","{'image_id': 1, 'category_id': 3, 'area': 294832.8, 'bbox': [1125.4, 174.0, 1196.7, 246.4], 'score': 0.0003077136934734881}\n","{'image_id': 1, 'category_id': 3, 'area': 730283.0, 'bbox': [890.4, 905.4, 879.2, 830.6], 'score': 0.00029549465398304164}\n","{'image_id': 1, 'category_id': 4, 'area': 51814.5, 'bbox': [1017.2, 1482.5, 134.0, 386.7], 'score': 0.0002847623254638165}\n","{'image_id': 1, 'category_id': 3, 'area': 122022.9, 'bbox': [1129.3, 837.6, 205.6, 593.6], 'score': 0.00025224671117030084}\n","{'image_id': 1, 'category_id': 1, 'area': 333073.1, 'bbox': [1173.6, 177.1, 1221.7, 272.6], 'score': 0.0002384844410698861}\n","{'image_id': 1, 'category_id': 2, 'area': 19751.7, 'bbox': [654.3, 693.4, 68.5, 288.6], 'score': 0.00023371844145003706}\n","{'image_id': 1, 'category_id': 3, 'area': 588415.6, 'bbox': [1101.2, 637.8, 604.3, 973.7], 'score': 0.00022793913376517594}\n","{'image_id': 1, 'category_id': 2, 'area': 580532.4, 'bbox': [715.0, 881.2, 687.3, 844.6], 'score': 0.00022757099941372871}\n","{'image_id': 1, 'category_id': 4, 'area': 143168.5, 'bbox': [919.1, 1145.8, 611.4, 234.2], 'score': 0.00019422789046075195}\n","{'image_id': 1, 'category_id': 5, 'area': 112791.9, 'bbox': [1006.2, 1515.3, 206.4, 546.4], 'score': 0.00019117568444926292}\n","{'image_id': 1, 'category_id': 5, 'area': 59012.3, 'bbox': [1012.7, 1470.4, 147.9, 399.0], 'score': 0.00019077588513027877}\n","{'image_id': 1, 'category_id': 2, 'area': 1018136.6, 'bbox': [596.6, 1380.4, 903.3, 1127.1], 'score': 0.00018889484636019915}\n","{'image_id': 1, 'category_id': 2, 'area': 708066.4, 'bbox': [984.2, 1568.8, 1080.2, 655.5], 'score': 0.00018252751033287495}\n","{'image_id': 1, 'category_id': 5, 'area': 168985.4, 'bbox': [930.8, 1111.1, 528.4, 319.8], 'score': 0.0001767284848028794}\n","{'image_id': 1, 'category_id': 4, 'area': 203098.0, 'bbox': [917.2, 1054.3, 589.0, 344.8], 'score': 0.0001710239885142073}\n","{'image_id': 1, 'category_id': 1, 'area': 1695971.3, 'bbox': [905.1, 735.7, 1405.3, 1206.9], 'score': 0.00015889227506704628}\n","{'image_id': 1, 'category_id': 5, 'area': 398658.7, 'bbox': [894.7, 1494.0, 702.5, 567.5], 'score': 0.000156627589603886}\n","{'image_id': 1, 'category_id': 2, 'area': 9928.9, 'bbox': [627.3, 607.4, 58.9, 168.7], 'score': 0.00015512312529608607}\n","{'image_id': 1, 'category_id': 4, 'area': 245373.1, 'bbox': [911.0, 186.4, 954.3, 257.1], 'score': 0.00014272895350586623}\n","{'image_id': 1, 'category_id': 3, 'area': 479868.3, 'bbox': [535.4, 355.2, 923.1, 519.8], 'score': 0.0001388167729601264}\n","{'image_id': 1, 'category_id': 4, 'area': 123876.0, 'bbox': [946.5, 1196.7, 600.3, 206.4], 'score': 0.00013683753786608577}\n","{'image_id': 1, 'category_id': 5, 'area': 538867.0, 'bbox': [1184.4, 851.4, 567.0, 950.4], 'score': 0.00013559675426222384}\n","{'image_id': 1, 'category_id': 1, 'area': 65629.7, 'bbox': [914.2, 91.6, 733.2, 89.5], 'score': 0.00013503107766155154}\n","100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gMFeF1qI58BP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592254251414,"user_tz":-480,"elapsed":28,"user":{"displayName":"JK Lo","photoUrl":"","userId":"02007731753862560953"}},"outputId":"113e0463-57ef-4288-fc2d-e67d8d9e9583"},"source":["a\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"zHbp9ppi_3Jw","colab_type":"code","colab":{}},"source":["with open(submit_annotations) as f:\n","  subannos = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BCgwSnrpADsP","colab_type":"code","colab":{}},"source":["for i in subannos:\n","  x, y, width, height = i['bbox']\n","\n","  x1 = x - width/2\n","  y1 = y - height/2\n","\n","  x1 = round(x1, 1)\n","  y1 = round(y1, 1)\n","\n","  i['bbox'] = [x1, y1, width, height]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Piyg25AiBctE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592477017074,"user_tz":-480,"elapsed":1645,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"a7dff215-4e13-43ba-bb3d-b9f49b769b47"},"source":["a=0\n","for i in subannos:\n","  if i['image_id'] == 1:  # 2037, 4601, 3921\n","    a+=1\n","    print (i)\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'image_id': 1, 'category_id': 4, 'area': 893729.7, 'bbox': [525.2, 114.2, 819.6, 1090.4], 'score': 0.8415889739990234}\n","{'image_id': 1, 'category_id': 3, 'area': 570104.6, 'bbox': [494.5, 95.9, 929.4, 613.4], 'score': 0.6006994247436523}\n","{'image_id': 1, 'category_id': 2, 'area': 113575.7, 'bbox': [916.5, 1187.3, 190.4, 596.5], 'score': 0.13425743579864502}\n","{'image_id': 1, 'category_id': 3, 'area': 944046.1, 'bbox': [457.6, 121.2, 1019.4, 926.1], 'score': 0.12345073372125626}\n","{'image_id': 1, 'category_id': 2, 'area': 194867.6, 'bbox': [707.1, 1203.2, 387.8, 502.6], 'score': 0.11707131564617157}\n","{'image_id': 1, 'category_id': 5, 'area': 667710.1, 'bbox': [526.9, 341.4, 759.0, 879.7], 'score': 0.07988986372947693}\n","{'image_id': 1, 'category_id': 2, 'area': 346890.4, 'bbox': [642.8, 1147.7, 549.9, 630.8], 'score': 0.06630423665046692}\n","{'image_id': 1, 'category_id': 2, 'area': 132761.8, 'bbox': [716.1, 1181.0, 220.0, 603.4], 'score': 0.05301739275455475}\n","{'image_id': 1, 'category_id': 1, 'area': 432656.9, 'bbox': [531.6, 73.7, 903.7, 478.8], 'score': 0.03916747868061066}\n","{'image_id': 1, 'category_id': 1, 'area': 739060.4, 'bbox': [426.1, 83.8, 990.0, 746.5], 'score': 0.03728179633617401}\n","{'image_id': 1, 'category_id': 3, 'area': 325459.8, 'bbox': [603.0, 91.6, 726.5, 448.0], 'score': 0.031403981149196625}\n","{'image_id': 1, 'category_id': 5, 'area': 1166127.0, 'bbox': [486.0, 234.0, 1013.5, 1150.5], 'score': 0.026136226952075958}\n","{'image_id': 1, 'category_id': 3, 'area': 673538.9, 'bbox': [491.2, 135.5, 661.6, 1018.0], 'score': 0.018608085811138153}\n","{'image_id': 1, 'category_id': 2, 'area': 73202.2, 'bbox': [751.2, 1214.8, 140.5, 521.1], 'score': 0.013777441345155239}\n","{'image_id': 1, 'category_id': 1, 'area': 250654.0, 'bbox': [630.5, 96.1, 646.1, 387.9], 'score': 0.013378710485994816}\n","{'image_id': 1, 'category_id': 2, 'area': 1022226.1, 'bbox': [459.6, 171.6, 913.3, 1119.3], 'score': 0.01241259090602398}\n","{'image_id': 1, 'category_id': 2, 'area': 251845.7, 'bbox': [812.2, 1110.2, 353.1, 713.1], 'score': 0.011168341152369976}\n","{'image_id': 1, 'category_id': 2, 'area': 555896.6, 'bbox': [556.6, 475.9, 754.3, 737.0], 'score': 0.011137024499475956}\n","{'image_id': 1, 'category_id': 2, 'area': 49380.1, 'bbox': [957.2, 1277.8, 119.9, 411.9], 'score': 0.010573280975222588}\n","{'image_id': 1, 'category_id': 1, 'area': 969069.8, 'bbox': [455.5, 160.1, 914.5, 1059.7], 'score': 0.009601874276995659}\n","{'image_id': 1, 'category_id': 3, 'area': 231278.7, 'bbox': [418.5, 87.2, 990.5, 233.5], 'score': 0.009305201470851898}\n","{'image_id': 1, 'category_id': 2, 'area': 930816.7, 'bbox': [504.5, 604.9, 849.5, 1095.7], 'score': 0.008759144693613052}\n","{'image_id': 1, 'category_id': 4, 'area': 648004.3, 'bbox': [446.2, 148.2, 568.7, 1139.4], 'score': 0.00764768011868}\n","{'image_id': 1, 'category_id': 2, 'area': 748224.4, 'bbox': [515.5, 946.2, 792.9, 943.7], 'score': 0.007374855689704418}\n","{'image_id': 1, 'category_id': 1, 'area': 242758.7, 'bbox': [454.7, 56.5, 966.0, 251.3], 'score': 0.006090442184358835}\n","{'image_id': 1, 'category_id': 4, 'area': 1134672.9, 'bbox': [348.2, 55.0, 1223.5, 927.4], 'score': 0.0051016476936638355}\n","{'image_id': 1, 'category_id': 4, 'area': 1411294.8, 'bbox': [514.9, 166.9, 1127.8, 1251.4], 'score': 0.004801481030881405}\n","{'image_id': 1, 'category_id': 5, 'area': 989579.7, 'bbox': [327.9, 184.0, 1084.5, 912.5], 'score': 0.004768789280205965}\n","{'image_id': 1, 'category_id': 5, 'area': 571259.6, 'bbox': [716.4, 406.6, 651.4, 876.9], 'score': 0.004088353831321001}\n","{'image_id': 1, 'category_id': 4, 'area': 636100.4, 'bbox': [718.0, 331.1, 649.0, 980.2], 'score': 0.004076266195625067}\n","{'image_id': 1, 'category_id': 4, 'area': 597877.5, 'bbox': [536.9, 522.6, 785.5, 761.2], 'score': 0.004031406249850988}\n","{'image_id': 1, 'category_id': 2, 'area': 607816.3, 'bbox': [718.1, 287.7, 658.7, 922.8], 'score': 0.0038817364256829023}\n","{'image_id': 1, 'category_id': 2, 'area': 125190.3, 'bbox': [1061.8, 521.0, 208.3, 601.0], 'score': 0.0036678528413176537}\n","{'image_id': 1, 'category_id': 2, 'area': 51804.4, 'bbox': [934.0, 1223.5, 109.6, 472.5], 'score': 0.0036223391070961952}\n","{'image_id': 1, 'category_id': 3, 'area': 190472.9, 'bbox': [537.0, 71.2, 669.5, 284.5], 'score': 0.003247144166380167}\n","{'image_id': 1, 'category_id': 2, 'area': 920300.4, 'bbox': [668.1, 55.6, 849.3, 1083.6], 'score': 0.003244285238906741}\n","{'image_id': 1, 'category_id': 2, 'area': 401360.8, 'bbox': [548.2, 725.5, 726.6, 552.4], 'score': 0.0029876597691327333}\n","{'image_id': 1, 'category_id': 2, 'area': 1312105.8, 'bbox': [671.0, 233.9, 888.2, 1477.2], 'score': 0.0025606669951230288}\n","{'image_id': 1, 'category_id': 2, 'area': 33487.2, 'bbox': [805.4, 1333.2, 95.8, 349.5], 'score': 0.0022496546152979136}\n","{'image_id': 1, 'category_id': 2, 'area': 90277.0, 'bbox': [553.5, 518.4, 161.1, 560.2], 'score': 0.0021035505924373865}\n","{'image_id': 1, 'category_id': 3, 'area': 1397103.3, 'bbox': [358.1, -0.0, 1015.3, 1376.1], 'score': 0.0019961914513260126}\n","{'image_id': 1, 'category_id': 3, 'area': 715484.6, 'bbox': [634.5, 190.6, 679.3, 1053.3], 'score': 0.001927111530676484}\n","{'image_id': 1, 'category_id': 2, 'area': 185304.1, 'bbox': [641.1, 854.6, 513.7, 360.7], 'score': 0.0018398963147774339}\n","{'image_id': 1, 'category_id': 2, 'area': 441541.4, 'bbox': [500.4, 1062.0, 549.9, 802.9], 'score': 0.001618573209270835}\n","{'image_id': 1, 'category_id': 2, 'area': 452762.2, 'bbox': [780.4, 518.1, 551.8, 820.5], 'score': 0.0014932409394532442}\n","{'image_id': 1, 'category_id': 2, 'area': 1264135.5, 'bbox': [354.8, 490.6, 1195.6, 1057.3], 'score': 0.00149213382974267}\n","{'image_id': 1, 'category_id': 2, 'area': 99887.6, 'bbox': [688.3, 1079.9, 469.0, 213.0], 'score': 0.0013939616037532687}\n","{'image_id': 1, 'category_id': 1, 'area': 605592.6, 'bbox': [595.9, 222.5, 589.7, 1026.9], 'score': 0.0012995187425985932}\n","{'image_id': 1, 'category_id': 4, 'area': 1760644.6, 'bbox': [336.4, 84.2, 1087.7, 1618.7], 'score': 0.001260266755707562}\n","{'image_id': 1, 'category_id': 2, 'area': 1165938.5, 'bbox': [752.9, 576.3, 936.3, 1245.2], 'score': 0.0012491339584812522}\n","{'image_id': 1, 'category_id': 1, 'area': 161513.0, 'bbox': [623.1, 56.7, 585.7, 275.8], 'score': 0.0011244119377806783}\n","{'image_id': 1, 'category_id': 2, 'area': 386195.3, 'bbox': [560.0, 1369.3, 739.4, 522.3], 'score': 0.0011226693168282509}\n","{'image_id': 1, 'category_id': 5, 'area': 306816.6, 'bbox': [641.5, 698.0, 595.9, 514.9], 'score': 0.0010555454064160585}\n","{'image_id': 1, 'category_id': 2, 'area': 545825.6, 'bbox': [468.5, 230.1, 544.7, 1002.1], 'score': 0.0008786162943579257}\n","{'image_id': 1, 'category_id': 2, 'area': 977969.4, 'bbox': [365.0, 152.6, 1224.2, 798.9], 'score': 0.0008238124428316951}\n","{'image_id': 1, 'category_id': 4, 'area': 360293.2, 'bbox': [673.2, 501.1, 512.2, 703.5], 'score': 0.0007768217474222183}\n","{'image_id': 1, 'category_id': 2, 'area': 121308.8, 'bbox': [691.5, 1111.7, 452.3, 268.2], 'score': 0.0007354053668677807}\n","{'image_id': 1, 'category_id': 2, 'area': 1327400.9, 'bbox': [399.1, 832.2, 1199.5, 1106.6], 'score': 0.0007301527075469494}\n","{'image_id': 1, 'category_id': 4, 'area': 590886.1, 'bbox': [506.9, 67.5, 853.7, 692.2], 'score': 0.0006658241618424654}\n","{'image_id': 1, 'category_id': 4, 'area': 1081310.3, 'bbox': [254.7, 207.8, 922.9, 1171.6], 'score': 0.000660413526929915}\n","{'image_id': 1, 'category_id': 5, 'area': 582856.3, 'bbox': [506.8, 176.1, 496.6, 1173.8], 'score': 0.00062222417909652}\n","{'image_id': 1, 'category_id': 5, 'area': 921401.6, 'bbox': [200.8, 139.2, 1339.9, 687.7], 'score': 0.0005776517791673541}\n","{'image_id': 1, 'category_id': 5, 'area': 848618.0, 'bbox': [485.3, 710.6, 903.1, 939.7], 'score': 0.0005329478299245238}\n","{'image_id': 1, 'category_id': 2, 'area': 117563.4, 'bbox': [700.0, 1607.2, 478.5, 245.7], 'score': 0.0005323695368133485}\n","{'image_id': 1, 'category_id': 5, 'area': 200460.0, 'bbox': [667.3, 1265.0, 432.1, 463.9], 'score': 0.0005002914112992585}\n","{'image_id': 1, 'category_id': 2, 'area': 24204.5, 'bbox': [594.2, 532.7, 73.3, 330.1], 'score': 0.0004419492615852505}\n","{'image_id': 1, 'category_id': 1, 'area': 513521.6, 'bbox': [510.5, 117.1, 460.0, 1116.3], 'score': 0.0004356785211712122}\n","{'image_id': 1, 'category_id': 4, 'area': 129901.6, 'bbox': [890.3, 1244.8, 235.8, 550.9], 'score': 0.00043183215893805027}\n","{'image_id': 1, 'category_id': 2, 'area': 817488.3, 'bbox': [426.3, 75.1, 1261.0, 648.3], 'score': 0.0004100537043996155}\n","{'image_id': 1, 'category_id': 2, 'area': 788339.5, 'bbox': [738.6, 1124.8, 964.3, 817.6], 'score': 0.0004075947217643261}\n","{'image_id': 1, 'category_id': 4, 'area': 1012048.3, 'bbox': [468.8, 600.7, 896.2, 1129.2], 'score': 0.0004047358816023916}\n","{'image_id': 1, 'category_id': 5, 'area': 1542516.7, 'bbox': [620.0, 339.8, 1169.1, 1319.5], 'score': 0.000401025841711089}\n","{'image_id': 1, 'category_id': 1, 'area': 578882.4, 'bbox': [669.0, 328.9, 679.1, 852.4], 'score': 0.00036416901275515556}\n","{'image_id': 1, 'category_id': 5, 'area': 1007219.3, 'bbox': [584.6, 104.8, 1100.7, 915.0], 'score': 0.00033876465749926865}\n","{'image_id': 1, 'category_id': 2, 'area': 358815.5, 'bbox': [480.4, 417.9, 393.2, 912.5], 'score': 0.0003353961219545454}\n","{'image_id': 1, 'category_id': 4, 'area': 266181.9, 'bbox': [646.9, 103.4, 602.4, 441.9], 'score': 0.0003286750870756805}\n","{'image_id': 1, 'category_id': 4, 'area': 122017.4, 'bbox': [538.2, 460.6, 178.1, 685.2], 'score': 0.0003251392627134919}\n","{'image_id': 1, 'category_id': 3, 'area': 294832.8, 'bbox': [527.1, 50.8, 1196.7, 246.4], 'score': 0.0003077136934734881}\n","{'image_id': 1, 'category_id': 3, 'area': 730283.0, 'bbox': [450.8, 490.1, 879.2, 830.6], 'score': 0.00029549465398304164}\n","{'image_id': 1, 'category_id': 4, 'area': 51814.5, 'bbox': [950.2, 1289.2, 134.0, 386.7], 'score': 0.0002847623254638165}\n","{'image_id': 1, 'category_id': 3, 'area': 122022.9, 'bbox': [1026.5, 540.8, 205.6, 593.6], 'score': 0.00025224671117030084}\n","{'image_id': 1, 'category_id': 1, 'area': 333073.1, 'bbox': [562.7, 40.8, 1221.7, 272.6], 'score': 0.0002384844410698861}\n","{'image_id': 1, 'category_id': 2, 'area': 19751.7, 'bbox': [620.0, 549.1, 68.5, 288.6], 'score': 0.00023371844145003706}\n","{'image_id': 1, 'category_id': 3, 'area': 588415.6, 'bbox': [799.1, 150.9, 604.3, 973.7], 'score': 0.00022793913376517594}\n","{'image_id': 1, 'category_id': 2, 'area': 580532.4, 'bbox': [371.4, 458.9, 687.3, 844.6], 'score': 0.00022757099941372871}\n","{'image_id': 1, 'category_id': 4, 'area': 143168.5, 'bbox': [613.4, 1028.7, 611.4, 234.2], 'score': 0.00019422789046075195}\n","{'image_id': 1, 'category_id': 5, 'area': 112791.9, 'bbox': [903.0, 1242.1, 206.4, 546.4], 'score': 0.00019117568444926292}\n","{'image_id': 1, 'category_id': 5, 'area': 59012.3, 'bbox': [938.8, 1270.9, 147.9, 399.0], 'score': 0.00019077588513027877}\n","{'image_id': 1, 'category_id': 2, 'area': 1018136.6, 'bbox': [145.0, 816.9, 903.3, 1127.1], 'score': 0.00018889484636019915}\n","{'image_id': 1, 'category_id': 2, 'area': 708066.4, 'bbox': [444.1, 1241.0, 1080.2, 655.5], 'score': 0.00018252751033287495}\n","{'image_id': 1, 'category_id': 5, 'area': 168985.4, 'bbox': [666.6, 951.2, 528.4, 319.8], 'score': 0.0001767284848028794}\n","{'image_id': 1, 'category_id': 4, 'area': 203098.0, 'bbox': [622.7, 881.9, 589.0, 344.8], 'score': 0.0001710239885142073}\n","{'image_id': 1, 'category_id': 1, 'area': 1695971.3, 'bbox': [202.5, 132.2, 1405.3, 1206.9], 'score': 0.00015889227506704628}\n","{'image_id': 1, 'category_id': 5, 'area': 398658.7, 'bbox': [543.5, 1210.2, 702.5, 567.5], 'score': 0.000156627589603886}\n","{'image_id': 1, 'category_id': 2, 'area': 9928.9, 'bbox': [597.8, 523.0, 58.9, 168.7], 'score': 0.00015512312529608607}\n","{'image_id': 1, 'category_id': 4, 'area': 245373.1, 'bbox': [433.9, 57.8, 954.3, 257.1], 'score': 0.00014272895350586623}\n","{'image_id': 1, 'category_id': 3, 'area': 479868.3, 'bbox': [73.8, 95.3, 923.1, 519.8], 'score': 0.0001388167729601264}\n","{'image_id': 1, 'category_id': 4, 'area': 123876.0, 'bbox': [646.4, 1093.5, 600.3, 206.4], 'score': 0.00013683753786608577}\n","{'image_id': 1, 'category_id': 5, 'area': 538867.0, 'bbox': [900.9, 376.2, 567.0, 950.4], 'score': 0.00013559675426222384}\n","{'image_id': 1, 'category_id': 1, 'area': 65629.7, 'bbox': [547.6, 46.8, 733.2, 89.5], 'score': 0.00013503107766155154}\n","100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HGIBYEkHBrFC","colab_type":"code","colab":{}},"source":["{'image_id': 1, 'category_id': 4, 'bbox': [571.5, 194.6, 815.2, 821.1], 'score': 0.6992745399475098}\n","{'image_id': 1, 'category_id': 4, 'bbox': [731.3, 128.9, 530.9, 857.4], 'score': 0.11952656507492065}\n","{'image_id': 1, 'category_id': 4, 'bbox': [668.4, 226.5, 613.6, 971.3], 'score': 0.03665538132190704}\n","{'image_id': 1, 'category_id': 3, 'bbox': [879.9, 223.0, 901.2, 669.5], 'score': 0.014186333864927292}\n","{'image_id': 1, 'category_id': 4, 'bbox': [582.3, 108.9, 709.3, 917.1], 'score': 0.01228678971529007}\n","{'image_id': 1, 'category_id': 4, 'bbox': [632.7, 57.0, 704.6, 505.9], 'score': 0.011506722308695316}\n","{'image_id': 1, 'category_id': 3, 'bbox': [1292.6, 144.2, 382.8, 665.6], 'score': 0.004970135632902384}\n","{'image_id': 1, 'category_id': 4, 'bbox': [891.0, 32.2, 655.1, 426.8], 'score': 0.00437589455395937}\n","{'image_id': 1, 'category_id': 5, 'bbox': [759.9, 550.0, 495.9, 805.6], 'score': 0.0040878006257116795}\n","{'image_id': 1, 'category_id': 2, 'bbox': [769.7, 842.0, 491.4, 654.4], 'score': 0.003458512481302023}\n","{'image_id': 1, 'category_id': 4, 'bbox': [240.1, 116.0, 999.3, 1029.8], 'score': 0.0032126344740390778}\n","{'image_id': 1, 'category_id': 4, 'bbox': [1015.3, 108.2, 408.5, 731.6], 'score': 0.0021131697576493025}\n","{'image_id': 1, 'category_id': 2, 'bbox': [1287.0, 529.4, 460.0, 782.3], 'score': 0.0020606054458767176}\n","{'image_id': 1, 'category_id': 4, 'bbox': [810.7, 222.9, 729.1, 598.3], 'score': 0.001816713367588818}\n","{'image_id': 1, 'category_id': 4, 'bbox': [469.0, 202.3, 924.5, 737.0], 'score': 0.0018118319567292929}\n","{'image_id': 1, 'category_id': 4, 'bbox': [506.5, 221.6, 870.6, 1004.6], 'score': 0.001490538357757032}\n","{'image_id': 1, 'category_id': 2, 'bbox': [632.2, 1003.7, 638.9, 470.0], 'score': 0.0012443060986697674}\n","{'image_id': 1, 'category_id': 5, 'bbox': [688.0, 771.1, 640.4, 467.9], 'score': 0.001170646632090211}\n","{'image_id': 1, 'category_id': 4, 'bbox': [861.2, 315.5, 210.0, 208.8], 'score': 0.0010894681327044964}\n","{'image_id': 1, 'category_id': 4, 'bbox': [538.0, 480.3, 711.3, 898.4], 'score': 0.0010651021730154753}\n","\n","{'image_id': 1, 'category_id': 4, 'area': 893729.7, 'bbox': [525.2, 114.2, 819.6, 1090.4], 'score': 0.8415889739990234}\n","{'image_id': 1, 'category_id': 3, 'area': 570104.6, 'bbox': [494.5, 95.9, 929.4, 613.4], 'score': 0.6006994247436523}\n","{'image_id': 1, 'category_id': 2, 'area': 113575.7, 'bbox': [916.5, 1187.3, 190.4, 596.5], 'score': 0.13425743579864502}\n","{'image_id': 1, 'category_id': 3, 'area': 944046.1, 'bbox': [457.6, 121.2, 1019.4, 926.1], 'score': 0.12345073372125626}\n","{'image_id': 1, 'category_id': 2, 'area': 194867.6, 'bbox': [707.1, 1203.2, 387.8, 502.6], 'score': 0.11707131564617157}\n","{'image_id': 1, 'category_id': 5, 'area': 667710.1, 'bbox': [526.9, 341.4, 759.0, 879.7], 'score': 0.07988986372947693}\n","{'image_id': 1, 'category_id': 2, 'area': 346890.4, 'bbox': [642.8, 1147.7, 549.9, 630.8], 'score': 0.06630423665046692}\n","{'image_id': 1, 'category_id': 2, 'area': 132761.8, 'bbox': [716.1, 1181.0, 220.0, 603.4], 'score': 0.05301739275455475}\n","{'image_id': 1, 'category_id': 1, 'area': 432656.9, 'bbox': [531.6, 73.7, 903.7, 478.8], 'score': 0.03916747868061066}\n","{'image_id': 1, 'category_id': 1, 'area': 739060.4, 'bbox': [426.1, 83.8, 990.0, 746.5], 'score': 0.03728179633617401}\n","{'image_id': 1, 'category_id': 3, 'area': 325459.8, 'bbox': [603.0, 91.6, 726.5, 448.0], 'score': 0.031403981149196625}\n","{'image_id': 1, 'category_id': 5, 'area': 1166127.0, 'bbox': [486.0, 234.0, 1013.5, 1150.5], 'score': 0.026136226952075958}\n","{'image_id': 1, 'category_id': 3, 'area': 673538.9, 'bbox': [491.2, 135.5, 661.6, 1018.0], 'score': 0.018608085811138153}\n","{'image_id': 1, 'category_id': 2, 'area': 73202.2, 'bbox': [751.2, 1214.8, 140.5, 521.1], 'score': 0.013777441345155239}\n","{'image_id': 1, 'category_id': 1, 'area': 250654.0, 'bbox': [630.5, 96.1, 646.1, 387.9], 'score': 0.013378710485994816}\n","{'image_id': 1, 'category_id': 2, 'area': 1022226.1, 'bbox': [459.6, 171.6, 913.3, 1119.3], 'score': 0.01241259090602398}\n","{'image_id': 1, 'category_id': 2, 'area': 251845.7, 'bbox': [812.2, 1110.2, 353.1, 713.1], 'score': 0.011168341152369976}\n","{'image_id': 1, 'category_id': 2, 'area': 555896.6, 'bbox': [556.6, 475.9, 754.3, 737.0], 'score': 0.011137024499475956}\n","{'image_id': 1, 'category_id': 2, 'area': 49380.1, 'bbox': [957.2, 1277.8, 119.9, 411.9], 'score': 0.010573280975222588}\n","{'image_id': 1, 'category_id': 1, 'area': 969069.8, 'bbox': [455.5, 160.1, 914.5, 1059.7], 'score': 0.009601874276995659}\n","{'image_id': 1, 'category_id': 3, 'area': 231278.7, 'bbox': [418.5, 87.2, 990.5, 233.5], 'score': 0.009305201470851898}\n","{'image_id': 1, 'category_id': 2, 'area': 930816.7, 'bbox': [504.5, 604.9, 849.5, 1095.7], 'score': 0.008759144693613052}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sW0TuGQYHVl6","colab_type":"code","colab":{}},"source":["# To fix multiple, we introduce non-maximum suppression, or NMS for short\n","def nms(detections, iou_thresh=0.):\n","  dets_by_class = {}\n","  final_result = []\n","  for det in detections:\n","    cls = det[1]\n","    if cls not in dets_by_class:\n","      dets_by_class[cls] = []\n","    dets_by_class[cls].append( det )\n","  for _, dets in dets_by_class.items():\n","    candidates = list(dets)\n","    candidates.sort( key=lambda x:x[0], reverse=True )\n","    while len(candidates) > 0:\n","      candidate = candidates.pop(0)\n","      _,_,cx,cy,cw,ch = candidate\n","      copy = list(candidates)\n","      for other in candidates:\n","        # Compute the IoU. If it exceeds thresh, we remove it\n","        _,_,ox,oy,ow,oh = other\n","        if iou( (cx,cy,cw,ch), (ox,oy,ow,oh) ) > iou_thresh:\n","          copy.remove(other)\n","      candidates = list(copy)\n","      final_result.append(candidate)\n","  return final_result\n","\n","# Computes the intersection-over-union (IoU) of two bounding boxes\n","def iou(bb1, bb2):\n","  x1,y1,w1,h1 = bb1\n","  xmin1 = x1 - w1/2\n","  xmax1 = x1 + w1/2\n","  ymin1 = y1 - h1/2\n","  ymax1 = y1 + h1/2\n","\n","  x2,y2,w2,h2 = bb2\n","  xmin2 = x2 - w2/2\n","  xmax2 = x2 + w2/2\n","  ymin2 = y2 - h2/2\n","  ymax2 = y2 + h2/2\n","\n","  area1 = w1*h1\n","  area2 = w2*h2\n","\n","  # Compute the boundary of the intersection\n","  xmin_int = max( xmin1, xmin2 )\n","  xmax_int = min( xmax1, xmax2 )\n","  ymin_int = max( ymin1, ymin2 )\n","  ymax_int = min( ymax1, ymax2 )\n","  intersection = max(xmax_int - xmin_int, 0) * max( ymax_int - ymin_int, 0 )\n","\n","  # Remove the double counted region\n","  union = area1+area2-intersection\n","\n","  return intersection / union\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8WK-yfyJJ_N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592999098215,"user_tz":-480,"elapsed":971,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"41fec632-e79a-41f0-ce93-ec7e7a2c632d"},"source":["submit_annotations"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/Jun_Kai_input_frcnn_36000.json'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"BGLibffKHIWp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1eqCHdojeHVeC-rlF5s4dLIbqGweC_Nue"},"executionInfo":{"status":"ok","timestamp":1592999513978,"user_tz":-480,"elapsed":11158,"user":{"displayName":"Lo Jun Kai","photoUrl":"","userId":"06658869486867465477"}},"outputId":"19caf456-7fd1-4329-ea87-d1f8f9a8126c"},"source":["# Display some of the images\n","import os\n","import PIL\n","import json\n","from collections import OrderedDict\n","from PIL import ImageEnhance, ImageFont, ImageDraw\n","\n","submit_annotations = '/content/drive/My Drive/Colab Notebooks/TIL/CV/junkai/Jun_Kai_input_frcnn_36000.json'\n","val_annotations = '/content/drive/My Drive/Colab Notebooks/TIL/CV/input/val.json'\n","eval_imgs_folder = '/content/drive/My Drive/Colab Notebooks/TIL/CV/input/val/val'\n","\n","# Double check if this is still valid\n","cat_list = ['tops', 'trousers', 'outerwear', 'dresses', 'skirts']\n","\n","with open(submit_annotations, 'r') as f:\n","  results = json.load(f)\n","\n","sorted_results = OrderedDict()\n","for i in results:\n","  img_id = i['image_id']\n","\n","  if img_id in sorted_results:\n","    sorted_results[img_id].append(i)\n","  else:\n","    sorted_results[img_id] = [i]\n","  \n","# sorted_results = OrderedDict({img_id_1: {}, img_id_2: {}, ...})\n","\n","# Run this to visualize\n","rank_colors = ['cyan', 'magenta', 'DarkOrange', 'DimGray', 'DarkTurquoise']\n","det_threshold=0.\n","top_dets=3\n","\n","start=0\n","end=20\n","for k in range(start,end):\n","  image_id = list(sorted_results.keys())[k]\n","  image = PIL.Image.open(os.path.join(eval_imgs_folder, str(image_id)+'.jpg'))\n","\n","  detection = []\n","  for image_dict in sorted_results[image_id]:\n","    score = image_dict['score']\n","    cat_id = image_dict['category_id']\n","    x,y,w,h = image_dict['bbox']  # [x,y,width,height]\n","\n","    detection.append((score, cat_id, x, y, w, h))\n","\n","  # nms takes in and returns [(score, cat, bbox,bbox1,bbox1,bbox1), (...)]\n","  preds = nms(detection, iou_thresh=0.01)  # Originally 0.5\n","\n","  for num, pred in enumerate(preds):\n","    score = pred[0]\n","    cat_id = pred[1]\n","    x0,y0,w,h = pred[2:6]\n","\n","    x1 = int(x0 + w)\n","    y1 = int(y0 + h)\n","    text = cat_list[cat_id-1]\n","    score = str(round(score, 5))\n","\n","    font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', 100)\n","    draw = ImageDraw.Draw(image)\n","    draw.rectangle([x0, y0, x1, y1], outline = rank_colors[cat_id-1], width=30)\n","    draw.text([x1, y1], text, fill = rank_colors[cat_id-1], font=font)\n","    draw.text([x1, y1+100], score, fill = rank_colors[cat_id-1], font=font)\n","  display(image.resize((255,255)))\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"ER8bYq0XHIvX","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}