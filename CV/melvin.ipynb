{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "melvin.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DKQ8KfRLXAQr",
        "hKLER8mDTG7d",
        "6SIGic-bH2eT",
        "yrI9Qdq1H2ed",
        "80E3iltBH2em",
        "K4JLrFC-H2es",
        "6TVdCBWhH2ex",
        "mXCIJHL0H2e8",
        "OH-Z-XJzH2fI",
        "w0J143jRH2fN",
        "uoLP-yqdH2fQ",
        "4DxqFyaEH2fe",
        "s5x40AXxH2fi"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGfVevGbH5cJ",
        "colab_type": "text"
      },
      "source": [
        "# README\n",
        "\n",
        "# **Be sure to change the paths for save_model_folder**\n",
        "\n",
        "Do put in comments of the parts that you are editing - which parameter, and what are the results of your testing\n",
        "\n",
        "If you have a model with a higher accuracy, do upload the pb files into 'working' dir, and then change the load_model_path to your model. Do not delete the old model in 'working' dir until no one is using it (older notebooks should still run on the old model for consistency purposes)\n",
        "\n",
        "# Current task\n",
        "\n",
        "https://keras.io/api/applications/\n",
        "- Go to this link, pick a model you want to try out,\n",
        "- Change the `backbone_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False)` line\n",
        "- Change the `for layer in model.get_layer('resnet50').layers:` line\n",
        "- Look at how to properly connect the subsequent layers (it needs to be the same size as the output layer of the backbone model - currently 7x7 and 14x14 has been done for us)\n",
        "- Change the `intermediate_layer_model = keras.Model(inputs=backbone_model.input, outputs=backbone_model.get_layer('conv4_block6_out').output)` line in the transfer model to match the correct layer\n",
        "- Write down the model you are trying out here:\n",
        "\n",
        "Model | Accuracy | Folder\n",
        "--- | --- | ---\n",
        "resnet50 | ... | working\n",
        "resnet50v2 | worse | melvin/resnet50v2\n",
        "\n",
        "(please don't pick one that has an insane amount of parameters as it will be slow to train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6fLNBoHVCi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "17f68705-5773-40a2-d6d6-94b1d17c414c"
      },
      "source": [
        "# Run this cell to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/TIL/CV'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/1_3rbcj37tYfAdfqjAM_yv4DA3DdbL0WH/TIL/CV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCKOABP8KwLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "58b57c62-746a-40be-9dca-e2f66de6034f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Amerul\t\t\t\t\t   junkai\n",
            " Amerul_testV2.ipynb\t\t\t   Jun_Kai_Master_Copy.ipynb\n",
            " copy_dir.ipynb\t\t\t\t   Jun_Kai_Roboflow_Tutorial_Eval.ipynb\n",
            "'Copy of Gerald2.ipynb'\t\t\t   Jun_Kai_Roboflow_Tutorial.ipynb\n",
            " create_coco_tf_record.ipynb\t\t   junkai_tf_utils\n",
            " DeepFashion2\t\t\t\t   melvin\n",
            "'FashionODFinal-checkpoint Master.ipynb'   melvin.ipynb\n",
            " gerald\t\t\t\t\t   siewru\n",
            " Gerald2.ipynb\t\t\t\t  'Siew Ru.ipynb'\n",
            " Gerald.ipynb\t\t\t\t   some_random_team.ipynb\n",
            " input\t\t\t\t\t   working\n",
            " interim_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RELnupU7fhJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "90899d5d-2a98-411d-a7aa-69ad9a3ab6d0"
      },
      "source": [
        "\"\"\" Check for the gpu attached to this colab.\n",
        "    Following are their repective runtimes per epoch:\n",
        "    Tesla T4 (259s) | K80 (541s) | P100-PCIE (203s)\n",
        "    If you are given a noob gpu, change runtime type to cpu and then back to gpu\n",
        "    and also go to runtime > managed sessions, and terminate your notebook session (more important)\n",
        "    Do this until you're happy (not too many consecutive times tho)\n",
        "\"\"\"\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jun 17 15:25:37 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    72W / 149W |    641MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "ceZYXYSOH2eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import PIL\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from math import log, exp\n",
        "from tqdm import tqdm\n",
        "from random import shuffle\n",
        "from PIL import ImageEnhance, ImageFont, ImageDraw\n",
        "from IPython.display import Image, display\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "\n",
        "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
        "\n",
        "cat_list = ['tops', 'trousers', 'outerwear', 'dresses', 'skirts']\n",
        "\n",
        "input_shape = (224,224,3)\n",
        "wt_decay = 5e-4\n",
        "\n",
        "dims_list = [(7,7),(14,14)]\n",
        "aspect_ratios = [(1,1), (1,2), (1,3), (1,4)]\n",
        "\n",
        "base_folder = ''\n",
        "data_folder = os.path.join( base_folder, 'input')\n",
        "train_imgs_folder = os.path.join( data_folder, 'train', 'train' )\n",
        "train_annotations = os.path.join( data_folder, 'train.json' )\n",
        "val_imgs_folder = os.path.join( data_folder, 'val', 'val' )\n",
        "val_annotations = os.path.join( data_folder, 'val.json' )\n",
        "\n",
        "train_pickle = os.path.join( data_folder, 'train.p' )\n",
        "val_pickle = os.path.join( data_folder, 'val.p' )\n",
        "save_model_folder = os.path.join( base_folder, 'melvin', 'mod-aspect' )  # Change this to your name\n",
        "load_model_folder = save_model_folder\n",
        "\n",
        "model_context = 'model-7x7-14x14-modaspect-modyoloposneg-wd{}'.format(wt_decay)  # Change this to however you want to name your model\n",
        "\n",
        "# Submission file name\n",
        "submit_annotations = os.path.join( save_model_folder, \"submission-\" + model_context + \".json\")\n",
        "\n",
        "# If you want to start a new model, use the line below:\n",
        "load_model_path = None\n",
        "\n",
        "# If you want to load your own model - sub the correct path in! (folder icon at the left toolbar > drive > ..., right click, copy path)\n",
        "# Paste the relative path\n",
        "# load_model_path = os.path.join( base_folder, \"working/model-7x7-14x14-3aspect-modyoloposneg-wd0.0005-best_val_loss.pb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKQ8KfRLXAQr",
        "colab_type": "text"
      },
      "source": [
        "## Master model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7JpbnseWxMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save_model_folder = os.path.join( base_folder, 'melvin', 'mod-aspect' )\n",
        "#load_model_folder = save_model_folder\n",
        "#model_context = 'model-7x7-14x14-modaspect-modyoloposneg-wd{}'.format(wt_decay)  # Change this to however you want to name your model\n",
        "#load_model_path = os.path.join( load_model_folder, 'ft-{}-best_val_loss.pb'.format(model_context) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKLER8mDTG7d",
        "colab_type": "text"
      },
      "source": [
        "## Verify filepaths and versions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eIq-_auTFa6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d23fa80d-74b6-4fd9-e1da-67336a310f0a"
      },
      "source": [
        "print(\"{:<20}{}\".format(\"data_folder:\", data_folder))\n",
        "print(\"{:<20}{}\".format(\"train_imgs_folder:\", train_imgs_folder))\n",
        "print(\"{:<20}{}\".format(\"train_annotations:\", train_annotations))\n",
        "print(\"{:<20}{}\".format(\"val_imgs_folder:\", val_imgs_folder))\n",
        "print(\"{:<20}{}\".format(\"val_annotations:\", val_annotations))\n",
        "print(\"{:<20}{}\".format(\"train_pickle:\", train_pickle))\n",
        "print(\"{:<20}{}\".format(\"val_pickle:\", val_pickle))\n",
        "print(\"{:<20}{}\".format(\"save_model_folder:\", save_model_folder))\n",
        "print(\"{:<20}{}\".format(\"load_model_folder:\", load_model_folder))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_folder:        ./DeepFashion2\n",
            "train_imgs_folder:  ./DeepFashion2/validation/image\n",
            "train_annotations:  ./DeepFashion2/validation/coco_annos.json\n",
            "val_imgs_folder:    ./input/val/val\n",
            "val_annotations:    ./DeepFashion2/input/val.json\n",
            "train_pickle:       ./input/train.p\n",
            "val_pickle:         ./input/val.p\n",
            "save_model_folder:  ./melvin/deepfashion2validation\n",
            "load_model_folder:  ./melvin/deepfashion2validation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "r4JnMUKwH2eM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4c9fe0d-354b-4467-e7e1-6661f8e2ddc4"
      },
      "source": [
        "tf.__version__ # Make sure tensorflow version is greater than 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SIGic-bH2eT",
        "colab_type": "text"
      },
      "source": [
        "# Data Augmentation Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Q3UPG4LQH2eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Augmentation methods. We need to implement our own augmentation because native support in keras does not change the bounding box \n",
        "labels for us as the image is altered. We need to do it ourselves.\n",
        "'''\n",
        "# Helper method: Computes the boundary of the image that includes all bboxes\n",
        "def compute_reasonable_boundary(labels):\n",
        "  bounds = [ (x-w/2, x+w/2, y-h/2, y+h/2) for _,x,y,w,h in labels]\n",
        "  xmin = min([bb[0] for bb in bounds])\n",
        "  xmax = max([bb[1] for bb in bounds])\n",
        "  ymin = min([bb[2] for bb in bounds])\n",
        "  ymax = max([bb[3] for bb in bounds])\n",
        "  return xmin, xmax, ymin, ymax\n",
        "\n",
        "def aug_horizontal_flip(img, labels):\n",
        "  flipped_labels = []\n",
        "  for c,x,y,w,h in labels:\n",
        "    flipped_labels.append( (c,1-x,y,w,h) )\n",
        "  return img.transpose(PIL.Image.FLIP_LEFT_RIGHT), np.array(flipped_labels)\n",
        "\n",
        "def aug_crop(img, labels):\n",
        "  # Compute bounds such that no boxes are cut out\n",
        "  xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n",
        "  # Choose crop_xmin from [0, xmin]\n",
        "  crop_xmin = max( np.random.uniform() * xmin, 0 )\n",
        "  # Choose crop_xmax from [xmax, 1]\n",
        "  crop_xmax = min( xmax + (np.random.uniform() * (1-xmax)), 1 )\n",
        "  # Choose crop_ymin from [0, ymin]\n",
        "  crop_ymin = max( np.random.uniform() * ymin, 0 )\n",
        "  # Choose crop_ymax from [ymax, 1]\n",
        "  crop_ymax = min( ymax + (np.random.uniform() * (1-ymax)), 1 )\n",
        "  # Compute the \"new\" width and height of the cropped image\n",
        "  crop_w = crop_xmax - crop_xmin\n",
        "  crop_h = crop_ymax - crop_ymin\n",
        "  cropped_labels = []\n",
        "  for c,x,y,w,h in labels:\n",
        "    c_x = (x - crop_xmin) / crop_w\n",
        "    c_y = (y - crop_ymin) / crop_h\n",
        "    c_w = w / crop_w\n",
        "    c_h = h / crop_h\n",
        "    cropped_labels.append( (c,c_x,c_y,c_w,c_h) )\n",
        "\n",
        "  W,H = img.size\n",
        "  # Compute the pixel coordinates and perform the crop\n",
        "  impix_xmin = int(W * crop_xmin)\n",
        "  impix_xmax = int(W * crop_xmax)\n",
        "  impix_ymin = int(H * crop_ymin)\n",
        "  impix_ymax = int(H * crop_ymax)\n",
        "  return img.crop( (impix_xmin, impix_ymin, impix_xmax, impix_ymax) ), np.array( cropped_labels )\n",
        "\n",
        "def aug_translate(img, labels):\n",
        "  # Compute bounds such that no boxes are cut out\n",
        "  xmin, xmax, ymin, ymax = compute_reasonable_boundary(labels)\n",
        "  trans_range_x = [-xmin, 1 - xmax]\n",
        "  tx = trans_range_x[0] + (np.random.uniform() * (trans_range_x[1] - trans_range_x[0]))\n",
        "  trans_range_y = [-ymin, 1 - ymax]\n",
        "  ty = trans_range_y[0] + (np.random.uniform() * (trans_range_y[1] - trans_range_y[0]))\n",
        "\n",
        "  trans_labels = []\n",
        "  for c,x,y,w,h in labels:\n",
        "    trans_labels.append( (c,x+tx,y+ty,w,h) )\n",
        "\n",
        "  W,H = img.size\n",
        "  tx_pix = int(W * tx)\n",
        "  ty_pix = int(H * ty)\n",
        "  return img.rotate(0, translate=(tx_pix, ty_pix)), np.array( trans_labels )\n",
        "\n",
        "def aug_colorbalance(img, labels, color_factors=[0.2,2.0]):\n",
        "  factor = color_factors[0] + np.random.uniform() * (color_factors[1] - color_factors[0])\n",
        "  enhancer = ImageEnhance.Color(img)\n",
        "  return enhancer.enhance(factor), labels\n",
        "\n",
        "def aug_contrast(img, labels, contrast_factors=[0.2,2.0]):\n",
        "  factor = contrast_factors[0] + np.random.uniform() * (contrast_factors[1] - contrast_factors[0])\n",
        "  enhancer = ImageEnhance.Contrast(img)\n",
        "  return enhancer.enhance(factor), labels\n",
        "\n",
        "def aug_brightness(img, labels, brightness_factors=[0.2,2.0]):\n",
        "  factor = brightness_factors[0] + np.random.uniform() * (brightness_factors[1] - brightness_factors[0])\n",
        "  enhancer = ImageEnhance.Brightness(img)\n",
        "  return enhancer.enhance(factor), labels\n",
        "\n",
        "def aug_sharpness(img, labels, sharpness_factors=[0.2,10.0]):\n",
        "  factor = sharpness_factors[0] + np.random.uniform() * (sharpness_factors[1] - sharpness_factors[0])\n",
        "  enhancer = ImageEnhance.Sharpness(img)\n",
        "  return enhancer.enhance(factor), labels\n",
        "\n",
        "# Performs no augmentations and returns the original image and bbox. Used for the validation images.\n",
        "def aug_identity(pil_img, label_arr):\n",
        "  return np.array(pil_img), label_arr\n",
        "\n",
        "# This is the default augmentation scheme that we will use for each training image.\n",
        "def aug_default(img, labels, p={'flip':0.5, 'crop':0.5, 'translate':0.5, 'color':0.2, 'contrast':0.2, 'brightness':0.2, 'sharpness':0.2}):\n",
        "  if p['color'] > np.random.uniform():\n",
        "    img, labels = aug_colorbalance(img, labels)\n",
        "  if p['contrast'] > np.random.uniform():\n",
        "    img, labels = aug_contrast(img, labels)\n",
        "  if p['brightness'] > np.random.uniform():\n",
        "    img, labels = aug_brightness(img, labels)\n",
        "  if p['sharpness'] > np.random.uniform():\n",
        "    img, labels = aug_sharpness(img, labels)\n",
        "  \n",
        "  if p['flip'] > np.random.uniform():\n",
        "    img, labels = aug_horizontal_flip(img, labels)\n",
        "  if p['crop'] > np.random.uniform():\n",
        "    img, labels = aug_crop(img, labels)\n",
        "  if p['translate'] > np.random.uniform():\n",
        "    img, labels = aug_translate(img, labels)\n",
        "  return np.array(img), labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrI9Qdq1H2ed",
        "colab_type": "text"
      },
      "source": [
        "# Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S8ugL4IOH2ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shape of ypred: ( batch, i, j, aspect_ratios, 1+4+numclasses ). For a batch,i,j, we get #aspect_ratios vectors of length 7.\n",
        "# Shape of ytrue: ( batch, i, j, aspect_ratios, 1+4+numclasses+2 ). For a batch,i,j, we get #aspect_ratios vectors of length 9 (two more for objectness and cat/loc indicators)\n",
        "def custom_loss(ytrue, ypred):\n",
        "  obj_loss_weight = 1.0\n",
        "  cat_loss_weight = 1.0\n",
        "  loc_loss_weight = 1.0\n",
        "\n",
        "  end_cat = len(cat_list) + 1\n",
        "\n",
        "  objloss_indicators = ytrue[:,:,:,:,-2:-1]\n",
        "  catlocloss_indicators = ytrue[:,:,:,:,-1:]\n",
        "\n",
        "  ytrue_obj, ypred_obj = ytrue[:,:,:,:,:1], ypred[:,:,:,:,:1]\n",
        "  ytrue_obj = tf.where( objloss_indicators != 0, ytrue_obj, 0 )\n",
        "  ypred_obj = tf.where( objloss_indicators != 0, ypred_obj, 0 )\n",
        "  objectness_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)( ytrue_obj, ypred_obj )\n",
        "\n",
        "  ytrue_cat, ypred_cat = ytrue[:,:,:,:,1:end_cat], ypred[:,:,:,:,1:end_cat]\n",
        "  ytrue_cat = tf.where( catlocloss_indicators != 0, ytrue_cat, 0 )\n",
        "  ypred_cat = tf.where( catlocloss_indicators != 0, ypred_cat, 0 )\n",
        "  categorical_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) ( ytrue_cat, ypred_cat )\n",
        "\n",
        "  # Remember that ytrue is longer than ypred, so we will need to stop at index -2, which is where the indicators are stored\n",
        "  ytrue_loc, ypred_loc = ytrue[:,:,:,:,end_cat:-2], ypred[:,:,:,:,end_cat:]\n",
        "  ytrue_loc = tf.where( catlocloss_indicators != 0, ytrue_loc, 0 )\n",
        "  ypred_loc = tf.where( catlocloss_indicators != 0, ypred_loc, 0 )\n",
        "  localisation_loss = tf.keras.losses.Huber() ( ytrue_loc, ypred_loc )\n",
        "\n",
        "  return obj_loss_weight*objectness_loss + cat_loss_weight*categorical_loss + loc_loss_weight*localisation_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c5v-Y660H2ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computes the intersection-over-union (IoU) of two bounding boxes\n",
        "def iou(bb1, bb2):\n",
        "  x1,y1,w1,h1 = bb1\n",
        "  xmin1 = x1 - w1/2\n",
        "  xmax1 = x1 + w1/2\n",
        "  ymin1 = y1 - h1/2\n",
        "  ymax1 = y1 + h1/2\n",
        "\n",
        "  x2,y2,w2,h2 = bb2\n",
        "  xmin2 = x2 - w2/2\n",
        "  xmax2 = x2 + w2/2\n",
        "  ymin2 = y2 - h2/2\n",
        "  ymax2 = y2 + h2/2\n",
        "\n",
        "  area1 = w1*h1\n",
        "  area2 = w2*h2\n",
        "\n",
        "  # Compute the boundary of the intersection\n",
        "  xmin_int = max( xmin1, xmin2 )\n",
        "  xmax_int = min( xmax1, xmax2 )\n",
        "  ymin_int = max( ymin1, ymin2 )\n",
        "  ymax_int = min( ymax1, ymax2 )\n",
        "  intersection = max(xmax_int - xmin_int, 0) * max( ymax_int - ymin_int, 0 )\n",
        "\n",
        "  # Remove the double counted region\n",
        "  union = area1+area2-intersection\n",
        "\n",
        "  return intersection / union\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80E3iltBH2em",
        "colab_type": "text"
      },
      "source": [
        "# Sampling Schemes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "whTv2XwpH2en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sampling schemes\n",
        "def modified_yolo_posneg_sampling(iou_scores_dict, label_tensor, gtclass, cat_list, iou_threshold=0.5):\n",
        "  iou_scores = []\n",
        "  zeros = [0] * len(cat_list)\n",
        "\n",
        "  for _, scores in iou_scores_dict.items():\n",
        "    iou_scores.extend(scores)\n",
        "  iou_scores.sort( key=lambda x: x[0], reverse=True )\n",
        "  \n",
        "  top_iou_score = iou_scores.pop(0)\n",
        "  _, key, i, j, k, dx, dy, dw, dh = top_iou_score\n",
        "  payload = [1, *zeros, dx,dy,dw,dh]\n",
        "  payload[gtclass + 1] = 1\n",
        "  # Train objectness, class and loc for the positive\n",
        "  label_tensor[key][i,j,k,-2:] = 1\n",
        "  label_tensor[key][i,j,k,:len(payload)] = payload\n",
        "\n",
        "  # Train objectness only for the negatives\n",
        "  low_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] < iou_threshold]\n",
        "  for _, key, i, j, k, _, _, _, _ in low_iou_scores:\n",
        "    label_tensor[key][i,j,k,-2] = 1\n",
        "\n",
        "  # Train cat/loc only for the in-betweens - those with high IoU but not positive\n",
        "  high_iou_scores = [iou_score for iou_score in iou_scores if iou_score[0] >= iou_threshold]\n",
        "  for _, key, i, j, k, dx, dy, dw, dh in high_iou_scores:\n",
        "    label_tensor[key][i,j,k,-1] = 1\n",
        "    payload = [0,*zeros,dx,dy,dw,dh]\n",
        "    payload[gtclass + 1] = 1\n",
        "    label_tensor[key][i,j,k,:len(payload)] = payload\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4JLrFC-H2es",
        "colab_type": "text"
      },
      "source": [
        "# Encoding labels / Decoding model output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-XEvsEZQH2et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Encoder: label -> tensor\n",
        "label_arr: np array like:\n",
        "[[class_idx x y w h]]: num_labels x 5\n",
        "...  \n",
        "Used to figure out for each label line, which tensor entry to shove it into.\n",
        "If the box corresponding to the tensor entry overlaps the ground truth by at least a predefined threshold, then we shove it in.\n",
        "'''\n",
        "def encode_label(label_arr, dims_list, aspect_ratios, iou_fn, sampling_fn, cat_list):\n",
        "  num_entries = 7 + len(cat_list) # objectness, ... len(cat_list) ..., dx, dy, dw, dh, obj_indicator, catloc_indicator\n",
        "  np_labels = {}\n",
        "  for dims in dims_list:\n",
        "    dimkey = '{}x{}'.format(*dims)\n",
        "    np_labels[dimkey] = np.zeros( (*dims, len(aspect_ratios), num_entries ) )\n",
        "\n",
        "  for label in label_arr:\n",
        "    gtclass, gtx, gty, gtw, gth = label\n",
        "    gtclass = int(gtclass)\n",
        "    gt_bbox = [gtx, gty, gtw, gth]\n",
        "    \n",
        "    iou_scores_dict = {}\n",
        "\n",
        "    for dims in dims_list:\n",
        "      key = '{}x{}'.format(*dims)\n",
        "    \n",
        "      kx,ky = dims\n",
        "      gapx = 1.0 / kx\n",
        "      gapy = 1.0 / ky\n",
        "      '''\n",
        "      There are kx x ky tiles. \n",
        "      For now, all have the same w,h of gapx,gapy. \n",
        "      For the (i,j)-th tile, x = 0.5*gapx + i*gapx = (0.5+i)*gapx | y = (0.5+j)*gapy\n",
        "      '''\n",
        "      for i in range(kx):\n",
        "        for j in range(ky):\n",
        "          for k in range( len(aspect_ratios) ):\n",
        "            dims_aspect_key = (*dims, k) # a 3-tuple: (dim1,dim2,ar)\n",
        "            if dims_aspect_key not in iou_scores_dict:\n",
        "              iou_scores_dict[dims_aspect_key] = []\n",
        "            x = (0.5+i)*gapx\n",
        "            y = (0.5+j)*gapy\n",
        "\n",
        "            # Different aspect ratios alter the anchor box default dimensions\n",
        "            w = gapx * aspect_ratios[k][0]\n",
        "            h = gapy * aspect_ratios[k][1]\n",
        "            cand_bbox = [x,y,w,h]\n",
        "\n",
        "            # SSD formulation\n",
        "            dx = (gtx - x) / w \n",
        "            dy = (gty - y) / h\n",
        "            dw = log( gtw / w )\n",
        "            dh = log( gth / h )\n",
        "            \n",
        "            int_over_union = iou_fn( cand_bbox, gt_bbox )\n",
        "            iou_scores_dict[dims_aspect_key].append( (int_over_union, key, i, j, k, dx, dy, dw, dh) )\n",
        "      sampling_fn( iou_scores_dict, np_labels, gtclass, cat_list )\n",
        "  return np_labels\n",
        "\n",
        "def decode_tensor(pred_dict, aspect_ratios):\n",
        "  results = []\n",
        "  for dim_str, pred_tensor in pred_dict.items():\n",
        "    pred_tensor = pred_tensor[0] # remove the batch\n",
        "    kx, ky = [int(g) for g in dim_str.split('x')]\n",
        "    gapx = 1. / kx\n",
        "    gapy = 1. / ky\n",
        "\n",
        "    # We trained without activations, so we need to process the logits into probabilities/scores\n",
        "    pred_arr = np.array(pred_tensor)\n",
        "    obj_logits = pred_arr[:,:,:,0]\n",
        "    obj_scores = 1. / (1 + np.exp(-obj_logits))\n",
        "    pred_arr[:,:,:,0] = obj_scores\n",
        "    \n",
        "    cls_logits = pred_arr[:,:,:,1:-4]\n",
        "    cls_scores = np.exp(cls_logits)\n",
        "    cls_scores = cls_scores / cls_scores.sum(axis=-1)[...,np.newaxis]\n",
        "    pred_arr[:,:,:,1:-4] = cls_scores\n",
        "\n",
        "    for k, ar in enumerate(aspect_ratios):\n",
        "      for i in range(kx):\n",
        "        for j in range(ky):\n",
        "          cx = (0.5+i)*gapx\n",
        "          cy = (0.5+j)*gapy\n",
        "          w = gapx * ar[0]\n",
        "          h = gapy * ar[1]\n",
        "\n",
        "          payload = pred_arr[i,j,k]\n",
        "          obj_score = payload[0]\n",
        "          dx, dy, dw, dh = payload[-4:]\n",
        "          cls_probs = payload[1:-4]\n",
        "\n",
        "          predx = (dx * w) + cx\n",
        "          predy = (dy * h) + cy\n",
        "          predw = w * exp( dw )\n",
        "          predh = h * exp( dh )\n",
        "          max_cls_idx = np.argmax( cls_probs )\n",
        "          max_cls_prob = cls_probs[max_cls_idx]\n",
        "          category_id = max_cls_idx + 1\n",
        "          det_score = obj_score * max_cls_prob\n",
        "          results.append( (det_score, category_id, predx, predy, predw, predh) )\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TVdCBWhH2ex",
        "colab_type": "text"
      },
      "source": [
        "# Constructing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IT0BQ_IjH2e3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transfer_model_7x7_14x14(backbone_model, input_shape, dims_list, num_aspect_ratios, wt_decay, model_name='transfer-objdet-model-7x7-14x14'):\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  intermediate_layer_model = keras.Model(inputs=backbone_model.input,\n",
        "                                         outputs=backbone_model.get_layer('conv4_block6_out').output)\n",
        "  intermediate_output = intermediate_layer_model(inputs) #14\n",
        "  backbone_output = backbone_model(inputs) #7\n",
        "\n",
        "  x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(backbone_output) #7\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(1024, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(512, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #7\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  upsample = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(2048, 3, padding='same', kernel_regularizer=l2(wt_decay))(upsample) #7\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  tens_7x7 = layers.Add()([x,backbone_output])\n",
        "\n",
        "  x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(upsample) #7\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2DTranspose(512, 5, strides=(2, 2), padding='same')(x) #14\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "\n",
        "  x = layers.Concatenate()([x,intermediate_output])\n",
        "\n",
        "  x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(256, 1, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.01)(x)\n",
        "  x = layers.Conv2D(512, 3, padding='same', kernel_regularizer=l2(wt_decay))(x) #14\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  tens_14x14 = layers.LeakyReLU(0.01)(x)\n",
        "\n",
        "  dim_tensor_map = {'7x7': tens_7x7, '14x14': tens_14x14}\n",
        "\n",
        "  # For each dimension, construct a predictions tensor. Accumulate them into a dictionary for keras to understand multiple labels.\n",
        "  preds_dict = {}\n",
        "  for dims in dims_list:\n",
        "    dimkey = '{}x{}'.format(*dims)\n",
        "    tens = dim_tensor_map[dimkey]\n",
        "    ar_preds = []\n",
        "    for _ in range(num_aspect_ratios):\n",
        "      objectness_preds = layers.Conv2D(1, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
        "      class_preds = layers.Conv2D(len(cat_list), 1, kernel_regularizer=l2(wt_decay))( tens )\n",
        "      bbox_preds = layers.Conv2D(4, 1, kernel_regularizer=l2(wt_decay))( tens )\n",
        "      ar_preds.append( layers.Concatenate()([objectness_preds, class_preds, bbox_preds]) )\n",
        "\n",
        "    if num_aspect_ratios > 1:\n",
        "      predictions = layers.Concatenate()(ar_preds)\n",
        "    elif num_aspect_ratios == 1:\n",
        "      predictions = ar_preds[0]\n",
        "\n",
        "    predictions = layers.Reshape( (*dims, num_aspect_ratios, 5+len(cat_list)), name=dimkey )(predictions)\n",
        "    preds_dict[dimkey] = predictions\n",
        "\n",
        "  model = keras.Model(inputs, preds_dict, name=model_name)\n",
        "\n",
        "  model.compile( optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                 loss=custom_loss )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXCIJHL0H2e8",
        "colab_type": "text"
      },
      "source": [
        "# Data generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vJ70xenHH2e9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TILPickle(Sequence):\n",
        "  def __init__(self, pickle_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn, testmode=False):\n",
        "\n",
        "    with open(pickle_file, 'rb') as p:\n",
        "      self.ids, self.x, self.y = pickle.load(p)\n",
        "    self.batch_size = batch_size\n",
        "    self.augment_fn = augment_fn\n",
        "    self.input_wh = (*input_size[:2][::-1],input_size[2])\n",
        "    self.label_encoder = label_encoder\n",
        "    self.preprocess_fn = preprocess_fn\n",
        "    self.testmode = testmode\n",
        "    \n",
        "  def __len__(self):\n",
        "    return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "    batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "    batch_ids = self.ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "    x_acc, y_acc = [], {}\n",
        "    \n",
        "    for x,y in zip( batch_x, batch_y ):\n",
        "      x_aug, y_aug = self.augment_fn( x, y )\n",
        "      if x_aug.size != self.input_wh[:2]:\n",
        "        x_aug.resize( self.input_wh )\n",
        "      x_acc.append( np.array(x_aug) )\n",
        "      y_dict = self.label_encoder( y_aug )\n",
        "      for dimkey, label in y_dict.items():\n",
        "        if dimkey not in y_acc:\n",
        "          y_acc[dimkey] = []\n",
        "        y_acc[dimkey].append( label )\n",
        "\n",
        "    if self.testmode:\n",
        "      return batch_ids, self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n",
        "    return self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FYoeU3zJH2fA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TILSequence(Sequence):\n",
        "  def __init__(self, img_folder, json_annotation_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn, testmode=False):\n",
        "\n",
        "    self._prepare_data(img_folder, json_annotation_file)\n",
        "    self.batch_size = batch_size\n",
        "    self.augment_fn = augment_fn\n",
        "    self.input_wh = (*input_size[:2][::-1],input_size[2])\n",
        "    self.label_encoder = label_encoder\n",
        "    self.preprocess_fn = preprocess_fn\n",
        "    self.testmode = testmode\n",
        "    \n",
        "  def _prepare_data(self, img_folder, json_annotation_file):\n",
        "    imgs_dict = {im.split('.')[0].lstrip(\"0\"):im for im in os.listdir(img_folder) if im.endswith('.jpg')}\n",
        "    data_dict = {}\n",
        "    with open(json_annotation_file, 'r') as f:\n",
        "      annotations_dict = json.load(f)\n",
        "    annotations_list = annotations_dict['annotations']\n",
        "    for annotation in annotations_list:\n",
        "      img_id = str(annotation['image_id'])\n",
        "      c = annotation['category_id'] - 1 # TODO: make sure that category ids start from 1, not 0\n",
        "      boxleft,boxtop,boxwidth,boxheight = annotation['bbox']\n",
        "      if img_id in imgs_dict:\n",
        "        img_fp = os.path.join(img_folder, imgs_dict[img_id])\n",
        "        imwidth,imheight = PIL.Image.open(img_fp).size\n",
        "        if img_id not in data_dict:\n",
        "          data_dict[img_id] = []\n",
        "        box_cenx = boxleft + boxwidth/2.\n",
        "        box_ceny = boxtop + boxheight/2.\n",
        "        x,y,w,h = box_cenx/imwidth, box_ceny/imheight, boxwidth/imwidth, boxheight/imheight\n",
        "\n",
        "        data_dict[img_id].append( [c,x,y,w,h] )\n",
        "    self.x, self.y, self.ids = [], [], []\n",
        "    for img_id, labels in data_dict.items():\n",
        "      self.x.append( os.path.join(img_folder, imgs_dict[img_id]) )\n",
        "      self.y.append( np.array(labels) )\n",
        "      self.ids.append( img_id )\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "    batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "    x_acc, y_acc = [], {}\n",
        "    original_img_dims = []\n",
        "    with Pool(self.batch_size) as p:\n",
        "      # Read in the PIL objects from filepaths\n",
        "      batch_x = p.map(load_img, batch_x)\n",
        "    \n",
        "    for x,y in zip( batch_x, batch_y ):\n",
        "      W,H = x.size\n",
        "      original_img_dims.append( (W,H) )\n",
        "\n",
        "      x = x.resize( self.input_wh[:2] )\n",
        "      x_aug, y_aug = self.augment_fn( x, y )\n",
        "      x_acc.append( np.array(x_aug) )\n",
        "      y_dict = self.label_encoder( y_aug )\n",
        "      for dimkey, label in y_dict.items():\n",
        "        if dimkey not in y_acc:\n",
        "          y_acc[dimkey] = []\n",
        "        y_acc[dimkey].append( label )\n",
        "\n",
        "    return self.get_batch_test(idx, x_acc, y_acc, original_img_dims) if self.testmode else self.get_batch(x_acc, y_acc)\n",
        "\n",
        "  def get_batch_test(self, idx, x_acc, y_acc, original_img_dims):\n",
        "    batch_ids = self.ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "    return batch_ids, original_img_dims, self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n",
        "\n",
        "  def get_batch(self, x_acc, y_acc):\n",
        "    return self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-Z-XJzH2fI",
        "colab_type": "text"
      },
      "source": [
        "# Training / Transfer Learning of Model\n",
        "\n",
        "\n",
        "- There is overfitting now that I set top 25% (of each dim-ar combination) as positives. How?\n",
        "- Larger image size - maybe 448\n",
        "- Transfer learning\n",
        "- Change weights of losses?\n",
        "\n",
        "### Also add more callbacks, such as tensorboard \n",
        "dataset, batch_size, augment_fn, input_size, label_encoder, preprocess_fn\n",
        "encode_label(label_arr, dims_list, aspect_ratios, iou_fn, sampling_fn, cat_list)\n",
        "img_folder, json_annotation_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F-7FZinPH2fE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7994815c-b0b8-4398-9832-3c1aceabe0dc"
      },
      "source": [
        "if load_model_path is None:\n",
        "  backbone_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False)\n",
        "  model = transfer_model_7x7_14x14(backbone_model, input_shape=input_shape, dims_list=dims_list, num_aspect_ratios=len(aspect_ratios), wt_decay=wt_decay, model_name=model_context+'-res50')\n",
        "  model.summary()\n",
        "else:\n",
        "  print(\"Loading model...\")\n",
        "  model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model-7x7-14x14-4aspect-modyoloposneg-wd0.0005-res50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "resnet50 (Model)                (None, 7, 7, 2048)   23587712    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 7, 7, 512)    1049088     resnet50[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 7, 7, 512)    2048        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 7, 7, 512)    0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 7, 7, 1024)   4719616     leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 7, 7, 1024)   4096        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 7, 7, 1024)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 7, 7, 512)    524800      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 7, 7, 512)    2048        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 7, 7, 512)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 7, 7, 1024)   4719616     leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 7, 7, 1024)   4096        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 7, 7, 1024)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 7, 7, 512)    524800      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 7, 7, 512)    2048        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 7, 7, 512)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 7, 7, 256)    131328      leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 7, 7, 256)    1024        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 7, 7, 256)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 14, 14, 512)  3277312     leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 14, 14, 512)  2048        conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 14, 14, 512)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "model (Model)                   (None, 14, 14, 1024) 8589184     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 14, 14, 1536) 0           leaky_re_lu_7[0][0]              \n",
            "                                                                 model[1][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 14, 14, 256)  393472      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 14, 14, 256)  1024        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 14, 14, 256)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 14, 14, 512)  1180160     leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 14, 14, 512)  2048        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 14, 14, 512)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 14, 14, 256)  131328      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 14, 14, 256)  1024        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 14, 14, 512)  1180160     leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 14, 14, 512)  2048        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 14, 14, 512)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 14, 14, 256)  131328      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 14, 14, 256)  1024        conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 14, 14, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 7, 7, 2048)   9439232     leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 14, 14, 512)  1180160     leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 7, 7, 2048)   8192        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 14, 14, 512)  2048        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 7, 7, 2048)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 14, 14, 512)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 7, 7, 2048)   0           leaky_re_lu_5[0][0]              \n",
            "                                                                 resnet50[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 14, 14, 1)    513         leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 14, 14, 5)    2565        leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 14, 14, 4)    2052        leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 7, 7, 1)      2049        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 7, 7, 5)      10245       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 7, 7, 4)      8196        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 7, 7, 1)      2049        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 7, 7, 5)      10245       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 7, 7, 4)      8196        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 7, 7, 1)      2049        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 7, 7, 5)      10245       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 7, 7, 4)      8196        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 7, 7, 1)      2049        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 7, 7, 5)      10245       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 7, 7, 4)      8196        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 14, 14, 10)   0           conv2d_25[0][0]                  \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 14, 14, 10)   0           conv2d_28[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 14, 14, 10)   0           conv2d_31[0][0]                  \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 14, 14, 10)   0           conv2d_34[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 7, 7, 10)     0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 7, 7, 10)     0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 7, 7, 10)     0           conv2d_19[0][0]                  \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 7, 7, 10)     0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 14, 14, 40)   0           concatenate_6[0][0]              \n",
            "                                                                 concatenate_7[0][0]              \n",
            "                                                                 concatenate_8[0][0]              \n",
            "                                                                 concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 7, 7, 40)     0           concatenate_1[0][0]              \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 concatenate_3[0][0]              \n",
            "                                                                 concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "14x14 (Reshape)                 (None, 14, 14, 4, 10 0           concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "7x7 (Reshape)                   (None, 7, 7, 4, 10)  0           concatenate_5[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 52,307,408\n",
            "Trainable params: 52,236,880\n",
            "Non-trainable params: 70,528\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyt3dc4XccjO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5c2daab1-539b-4b64-f3ef-4babff8c632b"
      },
      "source": [
        "# Setup the parameters and sequence\n",
        "\n",
        "bs=16\n",
        "n_epochs_warmup = 100  #300\n",
        "n_epochs_after = 100  #300\n",
        "\n",
        "label_encoder = lambda y: encode_label(y, dims_list, aspect_ratios, iou, modified_yolo_posneg_sampling, cat_list)\n",
        "preproc_fn = lambda x: x / 255.\n",
        "\n",
        "print('Creating training sequence...')\n",
        "train_sequence = TILSequence(train_imgs_folder, train_annotations, bs, aug_default, input_shape, label_encoder, preproc_fn)\n",
        "# train_sequence = TILPickle(train_pickle, bs, aug_default, input_shape, label_encoder, preproc_fn)\n",
        "print('Creating validation sequence...')\n",
        "# val_sequence = TILSequence(val_imgs_folder, val_annotations, bs, aug_identity, input_shape, label_encoder, preproc_fn)\n",
        "val_sequence = TILPickle(val_pickle, bs, aug_identity, input_shape, label_encoder, preproc_fn)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating training sequence...\n",
            "Creating validation sequence...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AMzo8qP3-k1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Warm up the model\n",
        "\n",
        "save_model_path = os.path.join( save_model_folder, '{}-best_val_loss.pb'.format(model_context) )\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                                                filepath=save_model_path,\n",
        "                                                                save_weights_only=False,\n",
        "                                                                monitor='val_loss',\n",
        "                                                                mode='auto',\n",
        "                                                                save_best_only=True)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=12)  # 30 is unnecessary (too long)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-8)  # 5 previously\n",
        "\n",
        "for layer in model.get_layer('resnet50').layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "print('Warming up the model...')\n",
        "model_log = model.fit(x=train_sequence, \n",
        "          epochs=n_epochs_warmup, \n",
        "          validation_data=val_sequence, \n",
        "          callbacks=[model_checkpoint_callback, earlystopping, reduce_lr],\n",
        "          verbose=2)  # Reduce printed output but speeds up time\n",
        "\n",
        "print('Model warmed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p4MJAUd9iUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print out loss epoch graph\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(model_log.history['loss'])\n",
        "plt.plot(model_log.history['val_loss'])\n",
        "plt.title('Loss (Lower Better)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fzsHHlzdH2fJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fine tuning (ft)\n",
        "print('Loading best val version of model...')\n",
        "\n",
        "# Reload model\n",
        "try:\n",
        "  del model\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "# if there is no ft-<model_context>-best_val_loss.pb, then use the one without ft prefix (start from scratch)\n",
        "# (ft prefix is used when the model is saved from this block of code)\n",
        "try:\n",
        "  load_model_path = os.path.join( load_model_folder, 'ft-{}-best_val_loss.pb'.format(model_context) )\n",
        "  model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
        "except OSError:\n",
        "  load_model_path = os.path.join( load_model_folder, '{}-best_val_loss.pb'.format(model_context) )\n",
        "  model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})\n",
        "\n",
        "print(\"From:\", load_model_path)\n",
        "\n",
        "for layer in model.get_layer('resnet50').layers:\n",
        "  layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss=custom_loss)\n",
        "model_context = 'ft-' + model_context\n",
        "save_model_path = os.path.join( save_model_folder, '{}-best_val_loss.pb'.format(model_context) )\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                                                filepath=save_model_path,\n",
        "                                                                save_weights_only=False,\n",
        "                                                                monitor='val_loss',\n",
        "                                                                mode='auto',\n",
        "                                                                save_best_only=True)\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=12)  # 30 is unnecessary (too long)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-8)  # 5 previously\n",
        "\n",
        "model_log = model.fit(x=train_sequence, \n",
        "          epochs=n_epochs_after, \n",
        "          validation_data=val_sequence, \n",
        "          callbacks=[model_checkpoint_callback, earlystopping, reduce_lr],\n",
        "          verbose=2)  # Reduce printed output but speeds up time\n",
        "\n",
        "# Final save\n",
        "model.save(os.path.join(save_model_folder, '{}-final.pb'.format(model_context)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxtZ_w7bbuDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(model_log.history['loss'])\n",
        "plt.plot(model_log.history['val_loss'])\n",
        "plt.title('Loss (Lower Better)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0J143jRH2fN",
        "colab_type": "text"
      },
      "source": [
        "# Non-maximum suppression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UBSB8cneH2fN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To fix multiple, we introduce non-maximum suppression, or NMS for short\n",
        "def nms(detections, iou_thresh=0.):\n",
        "  print(detections)\n",
        "  dets_by_class = {}\n",
        "  final_result = []\n",
        "  for det in detections:\n",
        "    cls = det[1]\n",
        "    if cls not in dets_by_class:\n",
        "      dets_by_class[cls] = []\n",
        "    dets_by_class[cls].append( det )\n",
        "  for _, dets in dets_by_class.items():\n",
        "    candidates = list(dets)\n",
        "    candidates.sort( key=lambda x:x[0], reverse=True )\n",
        "    while len(candidates) > 0:\n",
        "      candidate = candidates.pop(0)\n",
        "      _,_,cx,cy,cw,ch = candidate\n",
        "      copy = list(candidates)\n",
        "      for other in candidates:\n",
        "        # Compute the IoU. If it exceeds thresh, we remove it\n",
        "        _,_,ox,oy,ow,oh = other\n",
        "        if iou( (cx,cy,cw,ch), (ox,oy,ow,oh) ) > iou_thresh:\n",
        "          copy.remove(other)\n",
        "      candidates = list(copy)\n",
        "      final_result.append(candidate)\n",
        "  return final_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoLP-yqdH2fQ",
        "colab_type": "text"
      },
      "source": [
        "# Load a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZMah6U4UH2fR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the model\n",
        "# IMPORTANT: load in the correct path for your model\n",
        "load_model_path = os.path.join( load_model_folder, 'ft-{}-best_val_loss.pb'.format(model_context) )\n",
        "print(\"Loading model from:\", load_model_path)\n",
        "model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rn_4pndrH2fU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the test data\n",
        "label_encoder = lambda y: encode_label(y, dims_list, aspect_ratios, iou, modified_yolo_posneg_sampling, cat_list)\n",
        "preproc_fn = lambda x: x / 255.\n",
        "\n",
        "print('Creating test validation pickle sequence...')\n",
        "test_sequence_pickle = TILPickle(val_pickle, 1, aug_identity, input_shape, label_encoder, preproc_fn, testmode=True)\n",
        "print('Creating test validation sequence...')\n",
        "test_sequence = TILSequence(val_imgs_folder, val_annotations, 1, aug_identity, input_shape, label_encoder, preproc_fn, testmode=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3TI67OZsH2fZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test to make sure that both dispensers dispense the same data\n",
        "img_idx = 42\n",
        "ids_pickle, x_pickle, y_pickle = test_sequence_pickle[img_idx]\n",
        "ids_seq, dims_seq, x_seq, y_seq = test_sequence[img_idx]\n",
        "\n",
        "print('Image ids of pickle and seq:', ids_pickle[0], ',', ids_seq[0])\n",
        "print('Are input arrays same?:', np.allclose( x_pickle, x_seq ))\n",
        "for dimkey, ylabel_pickle in y_pickle.items():\n",
        "  ylabel_seq = y_seq[dimkey]\n",
        "  print('Are labels same for key=(', dimkey, ')?:', np.allclose( ylabel_pickle, ylabel_seq ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DxqFyaEH2fe",
        "colab_type": "text"
      },
      "source": [
        "# Visualize Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zGnAnEA2H2fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this to visualize\n",
        "rank_colors = ['cyan', 'magenta', 'pink']\n",
        "det_threshold=0.\n",
        "top_dets=3\n",
        "\n",
        "start=0\n",
        "end=20\n",
        "for k in range(start,end):\n",
        "  _, _, img_arr, label_cxywh, = test_sequence[k]\n",
        "  img_arr = img_arr[0]\n",
        "  pil_img = PIL.Image.fromarray( (img_arr * 255.).astype(np.uint8) )\n",
        "  W,H = pil_img.size\n",
        "  pred_dict = model(np.array([img_arr]))\n",
        "  preds = decode_tensor( pred_dict, aspect_ratios )\n",
        "    \n",
        "  # Post-processing\n",
        "  preds.sort( key=lambda x:x[0], reverse=True )\n",
        "  preds = [pred for pred in preds if pred[0] >= det_threshold]\n",
        "  preds = preds[:top_dets]\n",
        "  preds = nms(preds, iou_thresh=0.5)\n",
        "\n",
        "  draw_img = pil_img.copy()\n",
        "  draw = ImageDraw.Draw(draw_img)\n",
        "  for i, pred in enumerate(preds):\n",
        "    conf,cls,x,y,w,h = pred\n",
        "    bb_x = int(x * W)\n",
        "    bb_y = int(y * H)\n",
        "    bb_w = int(w * W)\n",
        "    bb_h = int(h * H)\n",
        "    left = int(bb_x - bb_w / 2)\n",
        "    top = int(bb_y - bb_h / 2)\n",
        "    right = int(bb_x + bb_w / 2)\n",
        "    bot = int(bb_y + bb_h / 2)\n",
        "    cls_str = cat_list[cls-1]\n",
        "\n",
        "    draw.rectangle(((left, top), (right, bot)), outline=rank_colors[i])\n",
        "    draw.text((bb_x, bb_y), cls_str, fill=rank_colors[i])\n",
        "    draw.text( ( int(left + bb_w*.1), int(top + bb_h*.1) ), '{:.2f}'.format(conf), fill=rank_colors[i] )\n",
        "\n",
        "  display(draw_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5x40AXxH2fi",
        "colab_type": "text"
      },
      "source": [
        "# Generating detections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fhsMZxx1H2fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generating detections on the folder of validation images\n",
        "detections = []\n",
        "det_threshold=0.\n",
        "for i in tqdm(range(len(test_sequence))):\n",
        "  ids_seq, dims, input_arr, _ = test_sequence[i]\n",
        "  img_id = int(ids_seq[0])\n",
        "  W,H = dims[0]\n",
        "\n",
        "  # Here, I'm inferencing one-by-one, but you can batch it if you want it faster\n",
        "  pred_dict = model(input_arr)\n",
        "  preds = decode_tensor( pred_dict, aspect_ratios )\n",
        "\n",
        "  # Post-processing\n",
        "  preds = [pred for pred in preds if pred[0] >= det_threshold]\n",
        "  preds.sort( key=lambda x:x[0], reverse=True )\n",
        "  preds = preds[:100] # we only evaluate you on 100 detections per image\n",
        "\n",
        "  for i, pred in enumerate(preds):\n",
        "    conf,cat_id,x,y,w,h = pred\n",
        "    left = W * (x - w/2.)\n",
        "    left = round(left,1)\n",
        "    top = H * (y - h/2.)\n",
        "    top = round(top,1)\n",
        "    width = W*w\n",
        "    width = round(width,1)\n",
        "    height = H*h\n",
        "    height = round(height,1)\n",
        "    conf = float(conf)\n",
        "    cat_id = int(cat_id)\n",
        "    detections.append( {'image_id':img_id, 'category_id':cat_id, 'bbox':[left, top, width, height], 'score':conf} )\n",
        "\n",
        "with open(submit_annotations, 'w') as f:\n",
        "  json.dump(detections, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej6INWbeH2fl",
        "colab_type": "text"
      },
      "source": [
        "# COCO mAP scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuDN2rx-VwLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Auto reload after installing new modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "s2AHwI7iH2fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, we need to install cocoapi to evaluate our detections\n",
        "# This installation is a modified version of the original to suit this competition\n",
        "! pip install git+https://github.com/jinmingteo/cocoapi.git#subdirectory=PythonAPI --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2zKBCbxjH2fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "20PA3Vi7H2ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get evaluation score against validation set\n",
        "coco_gt = COCO(val_annotations)\n",
        "coco_dt = coco_gt.loadRes(submit_annotations)\n",
        "cocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQsjiB6iEfah",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Just run everything in this section.\n",
        "\n",
        "- Ensure your path is correct\n",
        "- Ensure the functions used have not been changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICTzs8BLcEL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "115a6459-14bb-440a-8e46-755ba1743bb1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/TIL/CV'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1_3rbcj37tYfAdfqjAM_yv4DA3DdbL0WH/TIL/CV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ZQJ0WkGE_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import PIL\n",
        "from math import exp\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from multiprocessing import Pool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2kD3jJEF_Yw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "788913ed-5ebc-4a5a-99e5-88fa1891dbe3"
      },
      "source": [
        "eval_folder = \"final_evaluation\"\n",
        "model_name = \"mel\"\n",
        "\n",
        "# Load model path\n",
        "load_model_path = os.path.join( \"melvin\", 'mod-aspect', 'ft-model-7x7-14x14-modaspect-modyoloposneg-wd0.0005-final.pb' )\n",
        "\n",
        "# Load dataset path\n",
        "eval_annotations = os.path.join(eval_folder, \"CV_final_evaluation.json\")  # Change\n",
        "eval_imgs_folder = os.path.join(eval_folder, \"CV_final_images\")  # Change\n",
        "\n",
        "# Submission file name\n",
        "submit_annotations = os.path.join( eval_folder, \"daim_submission_\" + model_name + \".json\")\n",
        "\n",
        "print(\"{:<20}{}\".format(\"eval_folder:\", eval_folder))\n",
        "print(\"{:<20}{}\".format(\"model_name:\", model_name))\n",
        "print(\"{:<20}{}\".format(\"load_model_path:\", load_model_path))\n",
        "print(\"{:<20}{}\".format(\"eval_annotations:\", eval_annotations))\n",
        "print(\"{:<20}{}\".format(\"eval_imgs_folder:\", eval_imgs_folder))\n",
        "print(\"{:<20}{}\".format(\"submit_annotations:\", submit_annotations))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eval_folder:        final_evaluation\n",
            "model_name:         mel\n",
            "load_model_path:    melvin/mod-aspect/ft-model-7x7-14x14-modaspect-modyoloposneg-wd0.0005-final.pb\n",
            "eval_annotations:   final_evaluation/CV_final_evaluation.json\n",
            "eval_imgs_folder:   final_evaluation/CV_final_images\n",
            "submit_annotations: final_evaluation/daim_submission_mel.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xndZXkoIMW_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom sequence to run the evaluation annotation file\n",
        "\n",
        "class TILSequence(Sequence):\n",
        "  def __init__(self, img_folder, json_annotation_file, batch_size, augment_fn, input_size, label_encoder, preprocess_fn, testmode=True):\n",
        "\n",
        "    self._prepare_data(img_folder, json_annotation_file)\n",
        "    self.batch_size = batch_size\n",
        "    self.augment_fn = augment_fn\n",
        "    self.input_wh = (*input_size[:2][::-1],input_size[2])\n",
        "    self.label_encoder = label_encoder\n",
        "    self.preprocess_fn = preprocess_fn\n",
        "    self.testmode = testmode\n",
        "\n",
        "  def _prepare_data(self, img_folder, json_annotation_file):\n",
        "    imgs_dict = {im.split('.')[0]:im for im in os.listdir(img_folder) if im.endswith('.jpg')}\n",
        "    data_dict = {}\n",
        "    with open(json_annotation_file, 'r') as f:\n",
        "      annotations_dict = json.load(f)\n",
        "    annotations_list = annotations_dict['images']\n",
        "    for annotation in annotations_list:\n",
        "      img_id = str(annotation['id'])\n",
        "      if img_id in imgs_dict:\n",
        "        img_fp = os.path.join(img_folder, imgs_dict[img_id])\n",
        "        imwidth,imheight = PIL.Image.open(img_fp).size\n",
        "        if img_id not in data_dict:\n",
        "          data_dict[img_id] = []\n",
        "\n",
        "    self.x, self.ids = [], []\n",
        "    for img_id in data_dict.keys():\n",
        "      self.x.append( os.path.join(img_folder, imgs_dict[img_id]) )\n",
        "      self.ids.append( img_id )\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "    x_acc = []\n",
        "    original_img_dims = []\n",
        "    with Pool(self.batch_size) as p:\n",
        "      # Read in the PIL objects from filepaths\n",
        "      batch_x = p.map(load_img, batch_x)\n",
        "    \n",
        "    for x in batch_x:\n",
        "      W,H = x.size\n",
        "      original_img_dims.append( (W,H) )\n",
        "\n",
        "      x = x.resize( self.input_wh[:2] )\n",
        "      x_aug = x\n",
        "      x_acc.append( np.array(x_aug) )\n",
        "\n",
        "    return self.get_batch_test(idx, x_acc, original_img_dims) if self.testmode else self.get_batch(x_acc, y_acc)\n",
        "\n",
        "  def get_batch_test(self, idx, x_acc, original_img_dims):\n",
        "    batch_ids = self.ids[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "    return batch_ids, original_img_dims, self.preprocess_fn( np.array( x_acc ) ), 1\n",
        "\n",
        "  def get_batch(self, x_acc, y_acc):\n",
        "    return self.preprocess_fn( np.array( x_acc ) ), { dimkey: np.array( gt_tensor ) for dimkey, gt_tensor in y_acc.items() }\n",
        "  \n",
        "# Shape of ypred: ( batch, i, j, aspect_ratios, 1+4+numclasses ). For a batch,i,j, we get #aspect_ratios vectors of length 7.\n",
        "# Shape of ytrue: ( batch, i, j, aspect_ratios, 1+4+numclasses+2 ). For a batch,i,j, we get #aspect_ratios vectors of length 9 (two more for objectness and cat/loc indicators)\n",
        "def custom_loss(ytrue, ypred):\n",
        "  obj_loss_weight = 1.0\n",
        "  cat_loss_weight = 1.0\n",
        "  loc_loss_weight = 1.0\n",
        "\n",
        "  end_cat = len(cat_list) + 1\n",
        "\n",
        "  objloss_indicators = ytrue[:,:,:,:,-2:-1]\n",
        "  catlocloss_indicators = ytrue[:,:,:,:,-1:]\n",
        "\n",
        "  ytrue_obj, ypred_obj = ytrue[:,:,:,:,:1], ypred[:,:,:,:,:1]\n",
        "  ytrue_obj = tf.where( objloss_indicators != 0, ytrue_obj, 0 )\n",
        "  ypred_obj = tf.where( objloss_indicators != 0, ypred_obj, 0 )\n",
        "  objectness_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)( ytrue_obj, ypred_obj )\n",
        "\n",
        "  ytrue_cat, ypred_cat = ytrue[:,:,:,:,1:end_cat], ypred[:,:,:,:,1:end_cat]\n",
        "  ytrue_cat = tf.where( catlocloss_indicators != 0, ytrue_cat, 0 )\n",
        "  ypred_cat = tf.where( catlocloss_indicators != 0, ypred_cat, 0 )\n",
        "  categorical_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) ( ytrue_cat, ypred_cat )\n",
        "\n",
        "  # Remember that ytrue is longer than ypred, so we will need to stop at index -2, which is where the indicators are stored\n",
        "  ytrue_loc, ypred_loc = ytrue[:,:,:,:,end_cat:-2], ypred[:,:,:,:,end_cat:]\n",
        "  ytrue_loc = tf.where( catlocloss_indicators != 0, ytrue_loc, 0 )\n",
        "  ypred_loc = tf.where( catlocloss_indicators != 0, ypred_loc, 0 )\n",
        "  localisation_loss = tf.keras.losses.Huber() ( ytrue_loc, ypred_loc )\n",
        "\n",
        "  return obj_loss_weight*objectness_loss + cat_loss_weight*categorical_loss + loc_loss_weight*localisation_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNNqt3Irby8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_tensor(pred_dict, aspect_ratios):\n",
        "  results = []\n",
        "  for dim_str, pred_tensor in pred_dict.items():\n",
        "    pred_tensor = pred_tensor[0] # remove the batch\n",
        "    kx, ky = [int(g) for g in dim_str.split('x')]\n",
        "    gapx = 1. / kx\n",
        "    gapy = 1. / ky\n",
        "\n",
        "    # We trained without activations, so we need to process the logits into probabilities/scores\n",
        "    pred_arr = np.array(pred_tensor)\n",
        "    obj_logits = pred_arr[:,:,:,0]\n",
        "    obj_scores = 1. / (1 + np.exp(-obj_logits))\n",
        "    pred_arr[:,:,:,0] = obj_scores\n",
        "    \n",
        "    cls_logits = pred_arr[:,:,:,1:-4]\n",
        "    cls_scores = np.exp(cls_logits)\n",
        "    cls_scores = cls_scores / cls_scores.sum(axis=-1)[...,np.newaxis]\n",
        "    pred_arr[:,:,:,1:-4] = cls_scores\n",
        "\n",
        "    for k, ar in enumerate(aspect_ratios):\n",
        "      for i in range(kx):\n",
        "        for j in range(ky):\n",
        "          cx = (0.5+i)*gapx\n",
        "          cy = (0.5+j)*gapy\n",
        "          w = gapx * ar[0]\n",
        "          h = gapy * ar[1]\n",
        "\n",
        "          payload = pred_arr[i,j,k]\n",
        "          obj_score = payload[0]\n",
        "          dx, dy, dw, dh = payload[-4:]\n",
        "          cls_probs = payload[1:-4]\n",
        "\n",
        "          predx = (dx * w) + cx\n",
        "          predy = (dy * h) + cy\n",
        "          predw = w * exp( dw )\n",
        "          predh = h * exp( dh )\n",
        "          max_cls_idx = np.argmax( cls_probs )\n",
        "          max_cls_prob = cls_probs[max_cls_idx]\n",
        "          category_id = max_cls_idx + 1\n",
        "          det_score = obj_score * max_cls_prob\n",
        "          results.append( (det_score, category_id, predx, predy, predw, predh) )\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzfY8LrYEyLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef8d0061-7ede-4d3d-b65d-464ceea576cb"
      },
      "source": [
        "# Load the model\n",
        "print(\"Loading model from:\", load_model_path)\n",
        "model = tf.keras.models.load_model(load_model_path, custom_objects={'custom_loss':custom_loss})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from: melvin/mod-aspect/ft-model-7x7-14x14-modaspect-modyoloposneg-wd0.0005-final.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zYWAEO7AI_m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b519dec3-695a-4457-9bfa-058d24499529"
      },
      "source": [
        "# Load the eval data\n",
        "# Performs no augmentations and returns the original image and bbox. Used for the validation images.\n",
        "def aug_identity(pil_img, label_arr):\n",
        "  return np.array(pil_img), label_arr\n",
        "\n",
        "input_shape = (224,224,3)\n",
        "label_encoder = lambda y: encode_label(y, dims_list, aspect_ratios, iou, modified_yolo_posneg_sampling, cat_list)\n",
        "preproc_fn = lambda x: x / 255.\n",
        "\n",
        "print(\"Loading eval images from:     \", eval_imgs_folder)\n",
        "print(\"Loading eval annotations from:\", eval_annotations)\n",
        "test_sequence = TILSequence(eval_imgs_folder, eval_annotations, 1, aug_identity, input_shape, label_encoder, preproc_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading eval images from:      final_evaluation/CV_final_images\n",
            "Loading eval annotations from: final_evaluation/CV_final_evaluation.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:2766: DecompressionBombWarning: Image size (99272481 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  DecompressionBombWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITwSSiIxEzAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "906c0283-3c51-4d18-e137-a44ec360beb9"
      },
      "source": [
        "# Generating detections on the folder of validation images\n",
        "detections = []\n",
        "det_threshold=0.\n",
        "for i in tqdm(range(len(test_sequence))):\n",
        "  ids_seq, dims, input_arr, _ = test_sequence[i]\n",
        "  img_id = int(ids_seq[0])\n",
        "  W,H = dims[0]\n",
        "\n",
        "  # Here, I'm inferencing one-by-one, but you can batch it if you want it faster\n",
        "  pred_dict = model(input_arr)\n",
        "  preds = decode_tensor( pred_dict, aspect_ratios )\n",
        "\n",
        "  # Post-processing\n",
        "  preds = [pred for pred in preds if pred[0] >= det_threshold]\n",
        "  preds.sort( key=lambda x:x[0], reverse=True )\n",
        "  preds = preds[:100] # we only evaluate you on 100 detections per image\n",
        "\n",
        "  for i, pred in enumerate(preds):\n",
        "    conf,cat_id,x,y,w,h = pred\n",
        "    left = W * (x - w/2.)\n",
        "    left = round(left,1)\n",
        "    top = H * (y - h/2.)\n",
        "    top = round(top,1)\n",
        "    width = W*w\n",
        "    width = round(width,1)\n",
        "    height = H*h\n",
        "    height = round(height,1)\n",
        "    conf = float(conf)\n",
        "    cat_id = int(cat_id)\n",
        "    detections.append( {'image_id':img_id, 'category_id':cat_id, 'bbox':[left, top, width, height], 'score':conf} )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|         | 86/1972 [01:09<17:51,  1.76it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: overflow encountered in exp\n",
            "  if sys.path[0] == '':\n",
            "  5%|         | 102/1972 [01:17<14:09,  2.20it/s]/usr/local/lib/python3.6/dist-packages/PIL/Image.py:2766: DecompressionBombWarning: Image size (99272481 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  DecompressionBombWarning,\n",
            " 32%|      | 638/1972 [08:51<16:32,  1.34it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n",
            "100%|| 1972/1972 [27:21<00:00,  1.20it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCaXJQwBJ914",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1768e51-d778-45ed-bb6f-023aded8cf4f"
      },
      "source": [
        "# Save to file\n",
        "print(\"Saving json file at:\", submit_annotations)\n",
        "with open(submit_annotations, 'w') as f:\n",
        "  json.dump(detections, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving json file at: final_evaluation/daim_submission_mel.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQRl4VbGoBvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To fix multiple, we introduce non-maximum suppression, or NMS for short\n",
        "def nms(detections, iou_thresh=0.):\n",
        "  dets_by_class = {}\n",
        "  final_result = []\n",
        "  for det in detections:\n",
        "    cls = det[1]\n",
        "    if cls not in dets_by_class:\n",
        "      dets_by_class[cls] = []\n",
        "    dets_by_class[cls].append( det )\n",
        "  for _, dets in dets_by_class.items():\n",
        "    candidates = list(dets)\n",
        "    candidates.sort( key=lambda x:x[0], reverse=True )\n",
        "    while len(candidates) > 0:\n",
        "      candidate = candidates.pop(0)\n",
        "      _,_,cx,cy,cw,ch = candidate\n",
        "      copy = list(candidates)\n",
        "      for other in candidates:\n",
        "        # Compute the IoU. If it exceeds thresh, we remove it\n",
        "        _,_,ox,oy,ow,oh = other\n",
        "        if iou( (cx,cy,cw,ch), (ox,oy,ow,oh) ) > iou_thresh:\n",
        "          copy.remove(other)\n",
        "      candidates = list(copy)\n",
        "      final_result.append(candidate)\n",
        "  return final_result\n",
        "\n",
        "# Computes the intersection-over-union (IoU) of two bounding boxes\n",
        "def iou(bb1, bb2):\n",
        "  x1,y1,w1,h1 = bb1\n",
        "  xmin1 = x1 - w1/2\n",
        "  xmax1 = x1 + w1/2\n",
        "  ymin1 = y1 - h1/2\n",
        "  ymax1 = y1 + h1/2\n",
        "\n",
        "  x2,y2,w2,h2 = bb2\n",
        "  xmin2 = x2 - w2/2\n",
        "  xmax2 = x2 + w2/2\n",
        "  ymin2 = y2 - h2/2\n",
        "  ymax2 = y2 + h2/2\n",
        "\n",
        "  area1 = w1*h1\n",
        "  area2 = w2*h2\n",
        "\n",
        "  # Compute the boundary of the intersection\n",
        "  xmin_int = max( xmin1, xmin2 )\n",
        "  xmax_int = min( xmax1, xmax2 )\n",
        "  ymin_int = max( ymin1, ymin2 )\n",
        "  ymax_int = min( ymax1, ymax2 )\n",
        "  intersection = max(xmax_int - xmin_int, 0) * max( ymax_int - ymin_int, 0 )\n",
        "\n",
        "  # Remove the double counted region\n",
        "  union = area1+area2-intersection\n",
        "\n",
        "  return intersection / union\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxH2btuic9Ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display some of the images\n",
        "from collections import OrderedDict\n",
        "from PIL import ImageEnhance, ImageFont, ImageDraw\n",
        "\n",
        "# Double check if this is still valid\n",
        "cat_list = ['tops', 'trousers', 'outerwear', 'dresses', 'skirts']\n",
        "\n",
        "with open(submit_annotations, 'r') as f:\n",
        "  results = json.load(f)\n",
        "\n",
        "sorted_results = OrderedDict()\n",
        "for i in results:\n",
        "  img_id = i['image_id']\n",
        "\n",
        "  if img_id in sorted_results:\n",
        "    sorted_results[img_id].append(i)\n",
        "  else:\n",
        "    sorted_results[img_id] = [i]\n",
        "  \n",
        "# sorted_results = OrderedDict({img_id_1: {}, img_id_2: {}, ...})\n",
        "\n",
        "# Run this to visualize\n",
        "rank_colors = ['cyan', 'magenta', 'DarkOrange', 'DimGray', 'DarkTurquoise']\n",
        "det_threshold=0.\n",
        "top_dets=3\n",
        "\n",
        "start=0\n",
        "end=20\n",
        "for k in range(start,end):\n",
        "  image_id = list(sorted_results.keys())[k]\n",
        "  image = PIL.Image.open(os.path.join(eval_imgs_folder, str(image_id)+'.jpg'))\n",
        "\n",
        "  detection = []\n",
        "  for image_dict in sorted_results[image_id]:\n",
        "    score = image_dict['score']\n",
        "    cat_id = image_dict['category_id']\n",
        "    x,y,w,h = image_dict['bbox']  # [x,y,width,height]\n",
        "\n",
        "    detection.append((score, cat_id, x, y, w, h))\n",
        "\n",
        "  # nms takes in and returns [(score, cat, bbox,bbox1,bbox1,bbox1), (...)]\n",
        "  preds = nms(detection, iou_thresh=0.01)  # Originally 0.5\n",
        "\n",
        "  for num, pred in enumerate(preds):\n",
        "    score = pred[0]\n",
        "    cat_id = pred[1]\n",
        "    x,y,w,h = pred[2:6]\n",
        "\n",
        "    x0 = int(x - w/2)\n",
        "    x1 = int(x + w/2)\n",
        "    y0 = int(y - h/2)\n",
        "    y1 = int(y + h/2)\n",
        "    text = cat_list[cat_id-1]\n",
        "    score = str(round(score, 5))\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    draw.rectangle([x0, y0, x1, y1], outline = rank_colors[cat_id-1])\n",
        "    draw.text([x1, y1], text, fill = rank_colors[cat_id-1])\n",
        "    draw.text([x1, y1+15], score, fill = rank_colors[cat_id-1])\n",
        "  display(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlDzCohBNkoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}